{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 导入数学和随机模块，分别用于数值运算与随机数生成\nimport math\nimport random\n# 从functools导入partial，用于函数偏应用（预填参数）\nfrom functools import partial\n# 引入类型提示相关工具\nfrom typing import Callable, Optional, Tuple\nimport flax.linen as nn # 引入Flax的模块定义接口\n# 导入JAX核心模块及其NumPy接口\nimport jax\nimport jax.numpy as jnp\n# 用于处理Flax中的冻结字典（不可变参数容器）\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\n# 导入Flax中用于合并attention mask的工具函数\nfrom flax.linen import combine_masks, make_causal_mask\n# 导入Flax中attention模块的核心函数：点积注意力权重计算\nfrom flax.linen.attention import dot_product_attention_weights\n# 导入用于嵌套字典展开/恢复的工具（常用于权重转换或配置结构化）\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax import lax # JAX中用于跨设备同步的控制流工具\nfrom jax.random import PRNGKey # JAX随机种子生成接口\n# 从HuggingFace Transformers库中导入多种模型输出类型（用于兼容模型返回结构）\nfrom transformers.modeling_flax_outputs import (\n    FlaxBaseModelOutput,\n    FlaxBaseModelOutputWithPastAndCrossAttentions,\n    FlaxCausalLMOutputWithCrossAttentions,\n    FlaxSeq2SeqLMOutput,\n    FlaxSeq2SeqModelOutput,\n    FlaxSeq2SeqQuestionAnsweringModelOutput,\n    FlaxSeq2SeqSequenceClassifierOutput,\n)\n# 引入Flax模型相关的辅助工具（主要用于封装模型行为和生成文档字符串）\nfrom transformers.modeling_flax_utils import (\n    ACT2FN,  # 激活函数映射表\n    FlaxPreTrainedModel,  # 预训练模型基类\n    append_call_sample_docstring,  # 添加样例调用文档\n    append_replace_return_docstrings,  # 添加或替换模型返回值文档\n    overwrite_call_docstring,  # 覆盖模型 __call__ 的文档\n)\n# 引入用于文档生成与日志的工具函数\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n# 导入BART模型的配置类（用于构建与加载模型结构）\nfrom transformers.models.bart.configuration_bart import BartConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:14:33.696613Z","iopub.execute_input":"2025-05-27T03:14:33.697037Z","iopub.status.idle":"2025-05-27T03:14:33.702864Z","shell.execute_reply.started":"2025-05-27T03:14:33.697011Z","shell.execute_reply":"2025-05-27T03:14:33.702156Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"logger = logging.get_logger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:14:33.703709Z","iopub.execute_input":"2025-05-27T03:14:33.703992Z","iopub.status.idle":"2025-05-27T03:14:33.727306Z","shell.execute_reply.started":"2025-05-27T03:14:33.703963Z","shell.execute_reply":"2025-05-27T03:14:33.726664Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n_CONFIG_FOR_DOC = \"BartConfig\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:14:42.593250Z","iopub.execute_input":"2025-05-27T03:14:42.594010Z","iopub.status.idle":"2025-05-27T03:14:42.597145Z","shell.execute_reply.started":"2025-05-27T03:14:42.593982Z","shell.execute_reply":"2025-05-27T03:14:42.596504Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"BART_START_DOCSTRING = r\"\"\"\n    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a Flax Linen\n    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a\n    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.\n\n    Finally, this model supports inherent JAX features such as:\n\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        config ([`BartConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n            `jax.numpy.bfloat16` (on TPUs).\n\n            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n            specified all the computation will be performed with the given `dtype`.\n\n            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n            parameters.**\n\n            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n            [`~FlaxPreTrainedModel.to_bf16`].\n\"\"\"\n\nBART_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            For translation and summarization training, `decoder_input_ids` should be provided. If no\n            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n            for denoising pre-training following the paper.\n        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the\n            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n            range `[0, config.max_position_embeddings - 1]`.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\nBART_ENCODE_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\nBART_DECODE_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            For translation and summarization training, `decoder_input_ids` should be provided. If no\n            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n            for denoising pre-training following the paper.\n        encoder_outputs (`tuple(tuple(jnp.ndarray)`):\n            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the\n            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n            range `[0, config.max_position_embeddings - 1]`.\n        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:15:32.747897Z","iopub.execute_input":"2025-05-27T03:15:32.748175Z","iopub.status.idle":"2025-05-27T03:15:32.756178Z","shell.execute_reply.started":"2025-05-27T03:15:32.748154Z","shell.execute_reply":"2025-05-27T03:15:32.755450Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 该函数用于训练阶段解码器的输入预处理，将标签序列右移一位，插入起始 token，同时处理 -100 标记为 pad_token_id，\ndef shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:\n    # 创建与 input_ids 相同形状的零张量\n    shifted_input_ids = jnp.zeros_like(input_ids)\n    # 将原 input_ids 除最后一个 token 外，填入新张量中，从第1列开始\n    shifted_input_ids = shifted_input_ids.at[:, 1:].set(input_ids[:, :-1])\n    # 第0列设置为解码器起始 token\n    shifted_input_ids = shifted_input_ids.at[:, 0].set(decoder_start_token_id)\n    # 将所有位置中原本是 -100 的标记替换为 pad_token_id\n    shifted_input_ids = jnp.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n    return shifted_input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:22:10.705337Z","iopub.execute_input":"2025-05-27T03:22:10.705627Z","iopub.status.idle":"2025-05-27T03:22:10.710401Z","shell.execute_reply.started":"2025-05-27T03:22:10.705607Z","shell.execute_reply":"2025-05-27T03:22:10.709794Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"config=BartConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:40:23.622023Z","iopub.execute_input":"2025-05-27T03:40:23.622396Z","iopub.status.idle":"2025-05-27T03:40:23.625841Z","shell.execute_reply.started":"2025-05-27T03:40:23.622373Z","shell.execute_reply":"2025-05-27T03:40:23.625138Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":" make_causal_mask(\n                jnp.ones((1,8), dtype=\"bool\"), dtype=\"bool\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T03:40:57.929192Z","iopub.execute_input":"2025-05-27T03:40:57.929891Z","iopub.status.idle":"2025-05-27T03:40:58.120308Z","shell.execute_reply.started":"2025-05-27T03:40:57.929868Z","shell.execute_reply":"2025-05-27T03:40:58.119563Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Array([[[[ True, False, False, False, False, False, False, False],\n         [ True,  True, False, False, False, False, False, False],\n         [ True,  True,  True, False, False, False, False, False],\n         [ True,  True,  True,  True, False, False, False, False],\n         [ True,  True,  True,  True,  True, False, False, False],\n         [ True,  True,  True,  True,  True,  True, False, False],\n         [ True,  True,  True,  True,  True,  True,  True, False],\n         [ True,  True,  True,  True,  True,  True,  True,  True]]]],      dtype=bool)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":" *a,b,c,d=(2, 128, 8, 64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T04:41:15.215573Z","iopub.execute_input":"2025-05-27T04:41:15.215834Z","iopub.status.idle":"2025-05-27T04:41:15.219381Z","shell.execute_reply.started":"2025-05-27T04:41:15.215816Z","shell.execute_reply":"2025-05-27T04:41:15.218491Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"type(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T04:41:25.953646Z","iopub.execute_input":"2025-05-27T04:41:25.954335Z","iopub.status.idle":"2025-05-27T04:41:25.958590Z","shell.execute_reply.started":"2025-05-27T04:41:25.954312Z","shell.execute_reply":"2025-05-27T04:41:25.957990Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"len(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T04:41:34.035260Z","iopub.execute_input":"2025-05-27T04:41:34.035512Z","iopub.status.idle":"2025-05-27T04:41:34.040092Z","shell.execute_reply.started":"2025-05-27T04:41:34.035495Z","shell.execute_reply":"2025-05-27T04:41:34.039346Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"tuple([2]) + (1, 1,2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T04:44:07.053173Z","iopub.execute_input":"2025-05-27T04:44:07.053433Z","iopub.status.idle":"2025-05-27T04:44:07.058336Z","shell.execute_reply.started":"2025-05-27T04:44:07.053416Z","shell.execute_reply":"2025-05-27T04:44:07.057716Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(2, 1, 1, 2)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"jnp.broadcast_to(\n                jnp.arange(2) < 1 + 1,\n                tuple([2]) + (1, 1,2),\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T04:43:46.648646Z","iopub.execute_input":"2025-05-27T04:43:46.649230Z","iopub.status.idle":"2025-05-27T04:43:46.777007Z","shell.execute_reply.started":"2025-05-27T04:43:46.649185Z","shell.execute_reply":"2025-05-27T04:43:46.776448Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Array([[[[ True,  True]]],\n\n\n       [[[ True,  True]]]], dtype=bool)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# @nn.compact 并不只能用于 __call__ 方法，也可以用于其他方法，但前提是：\n# ✅ 该方法内部需要注册子模块或变量（如 self.param, self.variable, Module() 等）\n# Flax 中子模块和参数的定义需要在 setup() 或 @nn.compact 装饰的方法中进行：\n# 在 setup() 中定义子模块或变量：适合静态模块结构。\n# 在被 @nn.compact 装饰的方法中定义子模块或变量：适合结构依赖输入、需要动态构建的情况。\n\nclass FlaxBartAttention(nn.Module):\n    config: BartConfig # 配置\n    embed_dim: int # 嵌入维度\n    num_heads: int # 头数\n    dropout: float = 0.0 # dropout比率\n    causal: bool = False # 是否是因果掩码,如果是,就是自回归\n    bias: bool = True\n    dtype: jnp.dtype = jnp.float32  # 矩阵计算用的数据类型\n    # flax linen的模块一般设置于此\n    def setup(self) -> None:\n        self.head_dim = self.embed_dim // self.num_heads # 每个头的表示大小 hd\n        if self.head_dim * self.num_heads != self.embed_dim: # 头数必须被嵌入维度整除\n            raise ValueError( # 如果不能整除,抛出错误\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        # 线性层\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=self.bias, # 是否使用截距\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n        # q,k,v投影层\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense() # 最后的线性层\n        # dropout层\n        self.dropout_layer = nn.Dropout(rate=self.dropout)\n\n        if self.causal: # 如果是自回归\n            # 构造因果掩码\n            self.causal_mask = make_causal_mask(\n                jnp.ones((1, self.config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\"\n            )\n    # 拆分嵌入维度 (b,s,d)-->(b,s,h,hd)\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n    # 合并头 (b,s,h,hd)-->(b,s,d)\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n    # 此函数用于将当前时间步的 key 和 value 拼接到缓存中（cached_key/value），\n    # 实现自回归推理中的缓存更新，加快生成效率\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        # 判断是否已初始化缓存（首次调用时无缓存)\n        # 判断当前模块实例中，变量集合 \"cache\" 下是否已经存在名为 \"cached_key\" 的变量。\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        # 初始化缓存变量（key, value, index），默认填充为零，仅在首次调用时生效\n        # self.variable(...) 是 惰性注册机制：只有变量不存在时才会执行初始化表达式。\n        # 若变量已存在（is_initialized == True），这几句不会覆盖变量内容，只返回句柄。\n        # self.variable(...) 执行逻辑\n        # 如果 \"cache\" 变量集合中不存在 \"cached_key\"，则：\n        # 注册该变量，初始化为 jnp.zeros(key.shape, key.dtype)\n        # 如果已经存在 \"cached_key\"，则：\n        # 不再执行初始化表达式，只返回这个变量的引用（一个 flax.core.scope.Variable 对象）\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n        # Flax 的变量机制：\n        # self.has_variable(...) 是判断变量是否已经存在\n        # self.variable(...) 是注册变量（如果不存在）并返回句柄\n        # 所以这三句执行完后，下次再调用这个函数时，has_variable(...) 才会返回 True\n        if is_initialized:\n            # 获取缓存张量的维度信息：batch 维 + max_length+ num_heads + head_dim\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # 获取当前缓存的写入位置\n            cur_index = cache_index.value\n            # 生成写入位置索引（用于 dynamic_update_slice） (0,cur_index, 0, 0)\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            # 将当前 key 和 value 插入缓存中对应位置\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            # 更新缓存变量的值\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]  # 记录更新了多少个 token（通常为 1，但支持并行）\n            # 更新插入位置,每次如果只是一个token,这次之后就更新为原位置+num_updated_cache_vectors\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # jnp.arange(max_length) < cur_index + num_updated_cache_vectors\n            # 生成 [0, 1, ..., max_length-1]，判断每个位置是否 在当前生成范围内。\n            # 返回形如 [True, True, ..., False, False] 的一维 bool 数组，前面是可看的 key，后面是不可看的。\n            # 将上述 1D bool 数组广播成 full attention mask：shape = (*batch_dims, 1, tgt_len, max_length)\n            # tgt_len = num_updated_cache_vectors，表示当前解码了多少个新 token；\n            # max_length 是 key 的长度；最终形成 [batch, 1, tgt_len, max_length] 的因果 mask。\n            # 这个 mask 控制的是：\n            # 当前 query 只能 attend 到历史生成的 key（缓存过的部分），不能看到未来的填充值（如全 0 的未写入部分）。\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            # 将因果 mask 与原 attention_mask 结合\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        # 第一次 is_initialized为False,这时直接返回key,value,attention_mask\n        # 只有之后第二次,第三次...才会执行if is_initialized:之内的代码\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states: jnp.ndarray,\n        key_value_states: Optional[jnp.ndarray] = None,\n        attention_mask: Optional[jnp.ndarray] = None,\n        init_cache: bool = False,\n        deterministic: bool = True,\n    ) -> Tuple[jnp.ndarray]:\n        # 判断是否是 cross-attention（用于 decoder 的 encoder-decoder attention）\n        is_cross_attention = key_value_states is not None\n        batch_size = hidden_states.shape[0]\n\n        # 计算 query 向量（投影后 shape 为 [b, t, h * d]）\n        query_states = self.q_proj(hidden_states)\n        # 根据 attention 类型决定 key/value 来源\n        # key_value_states应该是编码器输出\n        if is_cross_attention:\n            # cross_attentions\n            key_states = self.k_proj(key_value_states)\n            value_states = self.v_proj(key_value_states)\n        else:\n            # self_attention\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n        # 拆分多头（shape: [b, t, h, d_head]）\n        query_states = self._split_heads(query_states)\n        key_states = self._split_heads(key_states)\n        value_states = self._split_heads(value_states)\n\n        # ========== 处理因果 Mask ========== #\n        if self.causal:\n            query_length, key_length = query_states.shape[1], key_states.shape[1]\n            if self.has_variable(\"cache\", \"cached_key\"):\n                # 缓存中已有 key，说明是增量推理模式（step-by-step）\n                mask_shift = self.variables[\"cache\"][\"cache_index\"] # 当前缓存中插值位置\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1] # 缓存中的key长度\n                # 从预定义的因果 mask 中动态截取有效部分\n                causal_mask = lax.dynamic_slice(\n                    self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n                )\n            else: # 正常情况下,切出(当前q长度,当前k长度)\n                causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n            # 广播\n            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        # ========== 合并 Padding 和 Causal Mask ========== #\n        if attention_mask is not None and self.causal:\n            # broadcast 到 [b, 1, tgt_len, src_len] 形状，并与因果 mask 做与运算\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n            attention_mask = combine_masks(attention_mask, causal_mask)\n        elif self.causal: # 没有填充掩码,这时就只用因果掩码\n            attention_mask = causal_mask\n        elif attention_mask is not None: # 非因果注意力，只扩维用于 broadcast\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        # 在快速自回归解码过程中，我们一次只输入一个位置的内容，并逐步缓存 key 和 value。\n        # ========== 缓存处理（推理阶段单步拼接）========== #\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            # 拼接 key/value 到缓存，并更新 mask,这时的attention_mask一般已经是缓存的因果mask\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states, value_states, query_states, attention_mask\n            )\n\n        # # 将布尔注意力掩码转换为注意力偏差\n        # ========== 将 mask 转为 attention bias ========== #\n        if attention_mask is not None:\n            # True → 0.0；False → -inf（禁止 attend）\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n        else: \n            attention_bias = None\n        # ========== Dropout 随机数生成器 ========== #\n        dropout_rng = None\n        if not deterministic and self.dropout > 0.0:\n            dropout_rng = self.make_rng(\"dropout\") # 构造随机rng key\n        # ========== 计算注意力权重（含 dropout 和 bias）========== #\n        # broadcast_dropout=True每个 head 在相同位置的 attention 都被丢弃或保留；\n        # 相比 broadcast_dropout=False（每个 head 各自丢弃不同位置），它更节省资源，也更稳定。\n        attn_weights = dot_product_attention_weights(\n            query_states,  # 查询向量，形状 (..., q_len, num_heads, head_dim)\n            key_states,  # 键向量，形状 (..., kv_len, num_heads, head_dim)\n            bias=attention_bias,  # 注意力偏置（通常由 mask 转换而来，用于屏蔽非法位置）\n            dropout_rng=dropout_rng,   # 用于注意力 dropout 的随机数生成器\n            dropout_rate=self.dropout,   # dropout 比率，仅在训练时启用\n            broadcast_dropout=True,  # 是否对所有 heads 应用相同的 dropout mask（节省计算）\n            deterministic=deterministic,  # 是否为确定性模式（True 表示推理，禁用 dropout）\n            dtype=self.dtype,  # 数据类型，如 float32、bfloat16 等\n            precision=None,   # 点积计算的精度选项，通常为默认精度即可\n        )\n        # ========== 使用注意力权重加权 value，得到输出 ========== #\n        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states)\n        attn_output = self._merge_heads(attn_output) # 多头合并为原始维度\n        attn_output = self.out_proj(attn_output)  # 最终线性投影输出\n        # 返回注意力输出和权重矩阵\n        return attn_output, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:12:18.854947Z","iopub.execute_input":"2025-05-27T05:12:18.855597Z","iopub.status.idle":"2025-05-27T05:12:19.069813Z","shell.execute_reply.started":"2025-05-27T05:12:18.855575Z","shell.execute_reply":"2025-05-27T05:12:19.068992Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# 编码器层\nclass FlaxBartEncoderLayer(nn.Module):\n    config: BartConfig  # 配置\n    dtype: jnp.dtype = jnp.float32 # 计算的数据类型\n\n    def setup(self) -> None:\n        self.embed_dim = self.config.d_model # 嵌入维度\n        self.self_attn = FlaxBartAttention( # 自注意力\n            config=self.config,\n            embed_dim=self.embed_dim,\n            num_heads=self.config.encoder_attention_heads,\n            dropout=self.config.attention_dropout,\n            dtype=self.dtype,\n        )\n        self.self_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05) # 标准化\n        self.dropout_layer = nn.Dropout(rate=self.config.dropout) # dropout\n        self.activation_fn = ACT2FN[self.config.activation_function] # 激活函数\n        self.activation_dropout_layer = nn.Dropout(rate=self.config.activation_dropout) \n        self.fc1 = nn.Dense( # 前馈第一个线性层\n            self.config.encoder_ffn_dim,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n        self.fc2 = nn.Dense( # 前馈降维线性层\n            self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n        )\n        self.final_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05) # 最后的标准化层\n\n    def __call__(\n        self,\n        hidden_states: jnp.ndarray,\n        attention_mask: jnp.ndarray,\n        output_attentions: bool = True,\n        deterministic: bool = True,\n    ) -> Tuple[jnp.ndarray]:\n        residual = hidden_states # 残差\n        # 获取注意力输出\n        hidden_states, attn_weights = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask)\n        # dropout\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n        hidden_states = residual + hidden_states # 注意力前后残差\n        hidden_states = self.self_attn_layer_norm(hidden_states) # 标准化\n        residual = hidden_states # 残差\n        hidden_states = self.activation_fn(self.fc1(hidden_states)) # 前馈升维\n        # 升维后的dropout \n        hidden_states = self.activation_dropout_layer(hidden_states, deterministic=deterministic)\n        hidden_states = self.fc2(hidden_states) # 前馈降维层\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic) # dropout\n        hidden_states = residual + hidden_states # 前馈前后残差\n        hidden_states = self.final_layer_norm(hidden_states) # 标准化\n        outputs = (hidden_states,) # 获取编码器层的输出\n        if output_attentions:\n            outputs += (attn_weights,)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:19:38.838433Z","iopub.execute_input":"2025-05-27T05:19:38.838913Z","iopub.status.idle":"2025-05-27T05:19:38.848042Z","shell.execute_reply.started":"2025-05-27T05:19:38.838891Z","shell.execute_reply":"2025-05-27T05:19:38.847357Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"config.encoder_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:20:43.416608Z","iopub.execute_input":"2025-05-27T05:20:43.417324Z","iopub.status.idle":"2025-05-27T05:20:43.421841Z","shell.execute_reply.started":"2025-05-27T05:20:43.417301Z","shell.execute_reply":"2025-05-27T05:20:43.421252Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"config.encoder_layerdrop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:21:27.522254Z","iopub.execute_input":"2025-05-27T05:21:27.522808Z","iopub.status.idle":"2025-05-27T05:21:27.527259Z","shell.execute_reply.started":"2025-05-27T05:21:27.522788Z","shell.execute_reply":"2025-05-27T05:21:27.526571Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"random.uniform(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:23:32.823775Z","iopub.execute_input":"2025-05-27T05:23:32.824505Z","iopub.status.idle":"2025-05-27T05:23:32.828923Z","shell.execute_reply.started":"2025-05-27T05:23:32.824470Z","shell.execute_reply":"2025-05-27T05:23:32.828123Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0.294972321158809"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"class FlaxBartEncoderLayerCollection(nn.Module): # 编码器层集合\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        # 编码器\n        self.layers = [\n            FlaxBartEncoderLayer(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.encoder_layers)\n        ]\n        self.layerdrop = self.config.encoder_layerdrop # 这个会丢弃整个编码器层\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        deterministic: bool = True,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        all_attentions = () if output_attentions else None # 存储每层的注意力权重矩阵\n        all_hidden_states = () if output_hidden_states else None # 存储每个编码器层的输出\n        for encoder_layer in self.layers: # 遍历每个编码器层\n            if output_hidden_states: # 存储每层的输入\n                all_hidden_states = all_hidden_states + (hidden_states,)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1) # 一个随机数\n            # 如果是训练模式,并且获取的随机数<配置中设置的layerdrop 跳过当前编码器层\n            if not deterministic and (dropout_probability < self.layerdrop):  # skip the layer\n                layer_outputs = (None, None)\n            else: # 否则,正常做注意力\n                layer_outputs = encoder_layer( # 注意力\n                    hidden_states,\n                    attention_mask,\n                    output_attentions,\n                    deterministic,\n                )\n            hidden_states = layer_outputs[0] # 编码器层的输出\n            if output_attentions: # 用来存储每层的注意力权重矩阵\n                all_attentions = all_attentions + (layer_outputs[1],)\n        # 用来存储最后一层的隐藏状态\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        # 输出:(经过所有层之后的编码器输出,...)\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n        # 返回元组\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n        # 返回结构化输出\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:27:57.424738Z","iopub.execute_input":"2025-05-27T05:27:57.425420Z","iopub.status.idle":"2025-05-27T05:27:57.433851Z","shell.execute_reply.started":"2025-05-27T05:27:57.425397Z","shell.execute_reply":"2025-05-27T05:27:57.433095Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class FlaxBartDecoderLayer(nn.Module):\n    config: BartConfig \n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self) -> None:\n        self.embed_dim = self.config.d_model # 嵌入维度\n        self.self_attn = FlaxBartAttention( # 解码器自注意力\n            config=self.config,\n            embed_dim=self.embed_dim,\n            num_heads=self.config.decoder_attention_heads,\n            dropout=self.config.attention_dropout,\n            causal=True, # 使用因果掩码\n            dtype=self.dtype,\n        )\n        self.dropout_layer = nn.Dropout(rate=self.config.dropout) # dropout\n        self.activation_fn = ACT2FN[self.config.activation_function] # 激活函数\n        self.activation_dropout_layer = nn.Dropout(rate=self.config.activation_dropout) # 前馈中间层的dropout\n        # 自注意力之后的norm\n        self.self_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n        self.encoder_attn = FlaxBartAttention( # 跨注意力\n            config=self.config,\n            embed_dim=self.embed_dim,\n            num_heads=self.config.decoder_attention_heads,\n            dropout=self.config.attention_dropout,\n            dtype=self.dtype,\n        )\n        self.encoder_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n        self.fc1 = nn.Dense( # 前馈升维\n            self.config.decoder_ffn_dim,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n        self.fc2 = nn.Dense( # 前馈降维\n            self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n        )\n        self.final_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n\n    def __call__(\n        self,\n        hidden_states: jnp.ndarray,\n        attention_mask: jnp.ndarray,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        init_cache: bool = False,\n        output_attentions: bool = True,\n        deterministic: bool = True,\n    ) -> Tuple[jnp.ndarray]:\n        residual = hidden_states # 残差\n\n        # 解码器自注意力\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states=hidden_states, attention_mask=attention_mask, init_cache=init_cache\n        )\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic) # dropout\n        hidden_states = residual + hidden_states # 自注意力前后残差\n        hidden_states = self.self_attn_layer_norm(hidden_states) # norm\n\n        # Cross-Attention Block\n        cross_attn_weights = None # 交叉注意力权重\n        if encoder_hidden_states is not None: # 如果编码器输出存在的话\n            residual = hidden_states # 残差\n            hidden_states, cross_attn_weights = self.encoder_attn(\n                hidden_states=hidden_states, # 上一步自注意力的输出\n                key_value_states=encoder_hidden_states, # 编码器输出\n                attention_mask=encoder_attention_mask, # 编码器填充掩码\n            )\n            hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic) # dropout\n            hidden_states = residual + hidden_states # 跨注意力前后残差+norm\n            hidden_states = self.encoder_attn_layer_norm(hidden_states) \n\n        # 前馈全连接\n        residual = hidden_states # 残差\n        hidden_states = self.activation_fn(self.fc1(hidden_states)) # 升维+激活\n        hidden_states = self.activation_dropout_layer(hidden_states, deterministic=deterministic) # dropout\n        hidden_states = self.fc2(hidden_states) # 降维\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic) # dropout\n        hidden_states = residual + hidden_states # 前馈前后残差+norm\n        hidden_states = self.final_layer_norm(hidden_states)\n        outputs = (hidden_states,) # 解码器层的输出\n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config.decoder_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:36:06.118370Z","iopub.execute_input":"2025-05-27T05:36:06.119196Z","iopub.status.idle":"2025-05-27T05:36:06.123589Z","shell.execute_reply.started":"2025-05-27T05:36:06.119173Z","shell.execute_reply":"2025-05-27T05:36:06.122816Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"class FlaxBartDecoderLayerCollection(nn.Module): # 解码器\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        self.layers = [ # 解码器层堆叠\n            FlaxBartDecoderLayer(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.decoder_layers)\n        ]\n        self.layerdrop = self.config.decoder_layerdrop # 这个可以整层丢弃\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None # 每个解码器层的输出的集合\n        all_self_attns = () if output_attentions else None # 所有的自注意力权重矩阵\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None # 交叉注意力权重矩阵\n        # 遍历每个decoder层\n        for decoder_layer in self.layers:\n            if output_hidden_states: # 添加每个decoder layer的输入\n                all_hidden_states += (hidden_states,)\n                # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1) \n            # 如果是训练模式,并且dropout_probability < self.layerdrop满足条件\n            if not deterministic and (dropout_probability < self.layerdrop): # 整层丢弃\n                layer_outputs = (None, None, None)\n            else: # 正常情况\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    init_cache=init_cache,\n                    output_attentions=output_attentions,\n                    deterministic=deterministic,\n                )\n            \n            hidden_states = layer_outputs[0] # 当前解码器层的输出\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        # 添加最后一个解码器层的输出\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        \n        outputs = [hidden_states, all_hidden_states, all_self_attns, all_cross_attentions]\n        # 返回元组\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n        # 返回结构化输出\n        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states, # 最后一层解码器的输出\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:41:25.459829Z","iopub.execute_input":"2025-05-27T05:41:25.460098Z","iopub.status.idle":"2025-05-27T05:41:25.469703Z","shell.execute_reply.started":"2025-05-27T05:41:25.460079Z","shell.execute_reply":"2025-05-27T05:41:25.469010Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class FlaxBartClassificationHead(nn.Module): # 分类头\n    \"\"\"句子级分类任务的头部.\"\"\"\n    config: BartConfig\n    inner_dim: int\n    num_classes: int\n    pooler_dropout: float\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self): \n        self.dense = nn.Dense( # 线性层\n            self.inner_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n        )\n        self.dropout = nn.Dropout(rate=self.pooler_dropout)\n        self.out_proj = nn.Dense( # 最后的输出线性层\n            self.num_classes,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n\n    def __call__(self, hidden_states: jnp.ndarray, deterministic: bool):\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic) # dropout\n        hidden_states = self.dense(hidden_states) # 线性投影\n        hidden_states = jnp.tanh(hidden_states) # 用tanh激活函数增加非线性\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic) # dropout\n        hidden_states = self.out_proj(hidden_states) # 最后输出几分类\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:44:06.269804Z","iopub.execute_input":"2025-05-27T05:44:06.270447Z","iopub.status.idle":"2025-05-27T05:44:06.276855Z","shell.execute_reply.started":"2025-05-27T05:44:06.270424Z","shell.execute_reply":"2025-05-27T05:44:06.276133Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"config.max_position_embeddings ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:45:08.386714Z","iopub.execute_input":"2025-05-27T05:45:08.387315Z","iopub.status.idle":"2025-05-27T05:45:08.391986Z","shell.execute_reply.started":"2025-05-27T05:45:08.387293Z","shell.execute_reply":"2025-05-27T05:45:08.391254Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"1024"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"class FlaxBartEncoder(nn.Module):\n    config: BartConfig\n    embed_tokens: nn.Embed\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        self.dropout_layer = nn.Dropout(rate=self.config.dropout) # dropout层\n        embed_dim = self.config.d_model # 嵌入维度\n        self.padding_idx = self.config.pad_token_id # 填充id\n        self.max_source_positions = self.config.max_position_embeddings  # 最大位置\n        self.embed_scale = math.sqrt(embed_dim) if self.config.scale_embedding else 1.0 # 缩放嵌入\n\n        # Bart 的设置是，如果指定了 padding_idx，则将嵌入 ID 偏移 2\n        # 并适当调整num_embeddings。其他型号没有这个技巧\n        self.offset = 2\n        self.embed_positions = nn.Embed(\n            self.config.max_position_embeddings + self.offset,\n            embed_dim,\n            embedding_init=jax.nn.initializers.normal(self.config.init_std), # 正态分布初始化\n            dtype=self.dtype,\n        )\n        self.layers = FlaxBartEncoderLayerCollection(self.config, self.dtype) # 子模块是编码器\n        self.layernorm_embedding = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05) # norm\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        position_ids,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        input_shape = input_ids.shape # 输入的形状\n        input_ids = input_ids.reshape(-1, input_shape[-1]) # 变形成(b,s)\n        # 词嵌入\n        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n        # 位置嵌入\n        embed_pos = self.embed_positions(position_ids + self.offset)\n\n        hidden_states = inputs_embeds + embed_pos # 带位置的嵌入表示\n        hidden_states = self.layernorm_embedding(hidden_states) # norm\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic) # dropout\n        # 经过编码器后的输出\n        outputs = self.layers(\n            hidden_states,\n            attention_mask,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return outputs\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=outputs.last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:49:50.251920Z","iopub.execute_input":"2025-05-27T05:49:50.252507Z","iopub.status.idle":"2025-05-27T05:49:50.260980Z","shell.execute_reply.started":"2025-05-27T05:49:50.252484Z","shell.execute_reply":"2025-05-27T05:49:50.260443Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class FlaxBartDecoder(nn.Module): # 解码器\n    config: BartConfig\n    embed_tokens: nn.Embed\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        self.dropout_layer = nn.Dropout(rate=self.config.dropout) # dropout 层\n        embed_dim = self.config.d_model # 嵌入维度\n        self.padding_idx = self.config.pad_token_id # 填充id\n        self.max_target_positions = self.config.max_position_embeddings\n        self.embed_scale = math.sqrt(self.config.d_model) if self.config.scale_embedding else 1.0\n        \n        self.offset = 2\n        self.embed_positions = nn.Embed(\n            self.config.max_position_embeddings + self.offset,\n            embed_dim,\n            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n            dtype=self.dtype,\n        )\n\n        self.layers = FlaxBartDecoderLayerCollection(self.config, self.dtype) # 解码器集合子模块\n        self.layernorm_embedding = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        position_ids,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        input_shape = input_ids.shape # 输入形状\n        input_ids = input_ids.reshape(-1, input_shape[-1]) # (b,s)\n        # 目标序列词嵌入\n        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n        # embed positions\n        positions = self.embed_positions(position_ids + self.offset)\n        hidden_states = inputs_embeds + positions # 合并后的嵌入表示\n        hidden_states = self.layernorm_embedding(hidden_states) # norm\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic) # dropout\n        # 解码器输出\n        outputs = self.layers(\n            hidden_states,\n            attention_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return outputs\n\n        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=outputs.last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:55:26.474330Z","iopub.execute_input":"2025-05-27T05:55:26.474848Z","iopub.status.idle":"2025-05-27T05:55:26.484219Z","shell.execute_reply.started":"2025-05-27T05:55:26.474827Z","shell.execute_reply":"2025-05-27T05:55:26.483403Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class FlaxBartModule(nn.Module):\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        self.shared = nn.Embed(  # 共享词嵌入模块\n            self.config.vocab_size, # 词表大小\n            self.config.d_model, # 嵌入维度\n            embedding_init=jax.nn.initializers.normal(self.config.init_std), # 嵌入初始化\n            dtype=self.dtype,\n        )\n        # encoder和decoder 使用相同的词嵌入\n        self.encoder = FlaxBartEncoder(self.config, dtype=self.dtype, embed_tokens=self.shared)\n        self.decoder = FlaxBartDecoder(self.config, dtype=self.dtype, embed_tokens=self.shared)\n\n    def _get_encoder_module(self): # 获取encoder\n        return self.encoder\n\n    def _get_decoder_module(self): # 获取decoder\n        return self.decoder\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        decoder_input_ids,\n        decoder_attention_mask,\n        position_ids,\n        decoder_position_ids,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        encoder_outputs = self.encoder( # 编码器输出\n            input_ids=input_ids, # 传入编码器输入\n            attention_mask=attention_mask, # 编码器填充mask\n            position_ids=position_ids, # 位置ids\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic, # 训练或推理模式\n        )\n\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids, # 目标序列输入\n            attention_mask=decoder_attention_mask, # 解码器因果掩码\n            position_ids=decoder_position_ids, # 目标序列位置ids\n            encoder_hidden_states=encoder_outputs[0],# 编码器输出用来做跨交叉的k,v\n            encoder_attention_mask=attention_mask, # 编码器填充mask\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n        # 返回最后的输出\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return FlaxSeq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state, # 经过encoder-decoder后的输出\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,# 编码器输出\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:01:33.510606Z","iopub.execute_input":"2025-05-27T06:01:33.511421Z","iopub.status.idle":"2025-05-27T06:01:33.524146Z","shell.execute_reply.started":"2025-05-27T06:01:33.511390Z","shell.execute_reply":"2025-05-27T06:01:33.523485Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# 1. 模型保存/加载时的参数嵌套结构\n# 当你用 .save_pretrained() 或 .from_pretrained() 保存/加载模型时，base_model_prefix 会决定保存文件中的结构。例如：\n# from transformers import FlaxBartForConditionalGeneration\n# model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n# print(model.params.keys())\n# 你会看到参数是嵌套在 'model' 这个 key 下的：\n# dict_keys(['model'])  # 因为 base_model_prefix = \"model\"\n# 这意味着：权重文件里是：\n# {\n#   \"model\": { ... 所有 Bart 模型参数 ... },\n#   \"lm_head\": ...\n# }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlaxBartPreTrainedModel(FlaxPreTrainedModel):\n    config_class = BartConfig # 配置\n    base_model_prefix: str = \"model\" # 用于自动匹配权重时的子模块前缀\n    module_class: nn.Module = None # 要实例化的核心模型模块（由子类指定）\n\n    def __init__(\n        self,\n        config: BartConfig,\n        input_shape: Tuple[int] = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        # 实例化子模块（如编码器-解码器模型），传入配置与其他参数\n        module = self.module_class(config=config, dtype=dtype, **kwargs) \n        # 调用基类构造方法，完成权重初始化等工作\n        # 当前类的init_weights可能被间接调用（由 FlaxPreTrainedModel.__init__ 内部触发）\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # 构造初始化输入张量（伪数据），用于执行一次 forward，触发参数初始化\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        # make sure initialization pass will work for FlaxBartForSequenceClassificationModule\n        input_ids = input_ids.at[(..., -1)].set(self.config.eos_token_id)  # 设置结尾 token 保证合法性\n        attention_mask = jnp.ones_like(input_ids) # 编码器填充mask\n        decoder_input_ids = input_ids # 解码器输入\n        decoder_attention_mask = jnp.ones_like(input_ids) # 解码器因果mask\n        batch_size, sequence_length = input_ids.shape\n        # 构造位置编码张量\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n        decoder_position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n        # 拆分随机数 rng key\n        # 拆分随机数生成器，分别用于参数和 dropout 初始化\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng} # 设置rngs 参数流和dropout流\n        # 调用模块的 init 方法，返回包含参数的 PyTree\n        random_params = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            decoder_input_ids,\n            decoder_attention_mask,\n            position_ids,\n            decoder_position_ids,\n        )[\"params\"]\n        # 如果提供了已有参数（例如加载某些层的预训练参数），则用已有参数覆盖初始化参数\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]  # 填充缺失项\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params)) # 重新 freeze 为不可变字典\n        else: # 如果没提供传入参数,返回随机初始化的参数\n            return random_params\n    # 初始化用于解码阶段的缓存（如 KV 缓存），用于加速自回归生成。\n    def init_cache(self, batch_size, max_length, encoder_outputs):\n        r\"\"\"\n        Args:\n            batch_size (`int`):\n                batch_size used for fast auto-regressive decoding. Defines the batch size of the initialized cache.\n            max_length (`int`):\n                maximum possible length for auto-regressive decoding. Defines the sequence length of the initialized\n                cache.\n            encoder_outputs (`Union[FlaxBaseModelOutput, tuple(tuple(jnp.ndarray)]`):\n                `encoder_outputs` consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*:\n                `attentions`). `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)\n                is a sequence of hidden-states at the output of the last layer of the encoder. Used in the\n                cross-attention of the decoder.\n        \"\"\"\n        # 构造伪输入，仅用于触发 decoder 中的缓存初始化逻辑\n        decoder_input_ids = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        decoder_attention_mask = jnp.ones_like(decoder_input_ids)\n        decoder_position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(decoder_input_ids).shape[-1]), decoder_input_ids.shape\n        )\n         # 定义只调用 decoder 的 forward 方法\n        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, decoder_position_ids, **kwargs):\n            decoder_module = module._get_decoder_module()\n            return decoder_module(\n                decoder_input_ids,\n                decoder_attention_mask,\n                decoder_position_ids,\n                **kwargs,\n            )\n        # 调用 init 仅执行 decoder 的初始化，设置 init_cache=True 以初始化 KV 缓存\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0),\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            decoder_position_ids=decoder_position_ids,\n            encoder_hidden_states=encoder_outputs[0],\n            init_cache=True,\n            method=_decoder_forward,   # 只初始化 decoder 子模块\n        )\n        return unfreeze(init_variables[\"cache\"])  # 返回初始化后的缓存（KV缓存等），用于生成任务\n\n    @add_start_docstrings(BART_ENCODE_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=FlaxBaseModelOutput, config_class=BartConfig)\n    def encode(\n        self,\n        input_ids: jnp.ndarray,\n        attention_mask: Optional[jnp.ndarray] = None,\n        position_ids: Optional[jnp.ndarray] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train: bool = False,\n        params: dict = None,\n        dropout_rng: PRNGKey = None,\n    ):\n        r\"\"\"\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n        >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n        >>> text = \"My friends are cool but they eat too many carbs.\"\n        >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n        >>> encoder_outputs = model.encode(**inputs)\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        if attention_mask is None: # 这种默认的填充mask\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None: #设置默认的位置ids\n            batch_size, sequence_length = input_ids.shape\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None: # 设置rng dropout流\n            rngs[\"dropout\"] = dropout_rng\n\n        def _encoder_forward(module, input_ids, attention_mask, position_ids, **kwargs):\n            encode_module = module._get_encoder_module() # encoder的前向模式\n            return encode_module(input_ids, attention_mask, position_ids, **kwargs)\n\n        return self.module.apply(\n            {\"params\": params or self.params}, # 参数\n            input_ids=jnp.array(input_ids, dtype=\"i4\"), # input_ids\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"), # \n            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train, # 确定性, False表示使用dropout\n            rngs=rngs,\n            method=_encoder_forward, # 调用的方法 调用的是编码器\n        )\n\n    @add_start_docstrings(BART_DECODE_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=FlaxBaseModelOutputWithPastAndCrossAttentions, config_class=BartConfig)\n    def decode(\n        self,\n        decoder_input_ids,\n        encoder_outputs,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        decoder_attention_mask: Optional[jnp.ndarray] = None,\n        decoder_position_ids: Optional[jnp.ndarray] = None,\n        past_key_values: dict = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train: bool = False,\n        params: dict = None,\n        dropout_rng: PRNGKey = None,\n    ):\n        r\"\"\"\n        Returns:\n\n        Example:\n\n        ```python\n        >>> import jax.numpy as jnp\n        >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n        >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n        >>> text = \"My friends are cool but they eat too many carbs.\"\n        >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n        >>> encoder_outputs = model.encode(**inputs)\n\n        >>> decoder_start_token_id = model.config.decoder_start_token_id\n        >>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n        >>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n        >>> last_decoder_hidden_states = outputs.last_hidden_state\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        encoder_hidden_states = encoder_outputs[0] # 编码器输出\n        if encoder_attention_mask is None: # 如果不存在编码器填充mask\n            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length)) # 默认的编码器填充掩码\n\n        batch_size, sequence_length = decoder_input_ids.shape\n        if decoder_attention_mask is None: # 设置默认的解码器因果掩码\n            decoder_attention_mask = jnp.ones((batch_size, sequence_length))\n        # 设置默认的解码器位置ids\n        if decoder_position_ids is None: \n            if past_key_values is not None: \n                raise ValueError(\"Make sure to provide `decoder_position_ids` when passing `past_key_values`.\")\n\n            decoder_position_ids = jnp.broadcast_to(\n                jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)\n            )\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None: # 设置默认的rng dropout流\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} # 参数\n\n        # 如果传入了 past_key_values，那么缓存已经被初始化，此时需要传递一个私有标志 init_cache，以确保缓存被使用。\n        # 同时还必须确保缓存被标记为 mutable（可变），这样 FlaxBartAttention 模块才能修改缓存内容。\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values \n            mutable = [\"cache\"]\n        else:\n            mutable = False\n        # 解码器前向call\n        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, decoder_position_ids, **kwargs):\n            decoder_module = module._get_decoder_module()\n            return decoder_module(\n                decoder_input_ids,\n                decoder_attention_mask,\n                decoder_position_ids,\n                **kwargs,\n            )\n\n        outputs = self.module.apply(\n            inputs, # inputs是一个字典结构 有参数,以及缓存等\n            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),# 目标序列输入\n            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n            decoder_position_ids=jnp.array(decoder_position_ids, dtype=\"i4\"),\n            encoder_hidden_states=encoder_hidden_states, # 编码器输入\n            encoder_attention_mask=jnp.array(encoder_attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs, # 随机数生成器\n            mutable=mutable, # 可以修改的变量 \"cache\"\n            method=_decoder_forward,\n        )\n\n        # 将更新的缓存添加到模型输出\n        if past_key_values is not None and return_dict:\n            outputs, past = outputs # 解码器输出+缓存\n            outputs[\"past_key_values\"] = unfreeze(past[\"cache\"]) # 设置缓存\n            return outputs \n        elif past_key_values is not None and not return_dict:\n            outputs, past = outputs\n            outputs = outputs[:1] + (unfreeze(past[\"cache\"]),) + outputs[1:] # 返回元组\n\n        return outputs\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    def __call__(\n        self,\n        input_ids: jnp.ndarray,\n        attention_mask: Optional[jnp.ndarray] = None,\n        decoder_input_ids: Optional[jnp.ndarray] = None,\n        decoder_attention_mask: Optional[jnp.ndarray] = None,\n        position_ids: Optional[jnp.ndarray] = None,\n        decoder_position_ids: Optional[jnp.ndarray] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train: bool = False,\n        params: dict = None,\n        dropout_rng: PRNGKey = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        # 准备编码器输入\n        if attention_mask is None: # 设置默认的编码器填充mask\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None: # 设置默认的原序列位置ids\n            batch_size, sequence_length = input_ids.shape\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        # 这时decoder_input_ids没有传入,只给了 input_ids（目标序列），我就自动 shift 它，生成 decoder_input_ids。\n        # 这段逻辑默认 input_ids 同时作为目标序列用在 decoder 上，是为了支持预训练或某些推理场景的兼容行为，不是通用的翻译推理\n        # 逻辑。在翻译时，应显式传入 decoder_input_ids。\n        if decoder_input_ids is None:\n            decoder_input_ids = shift_tokens_right( # 目标序列输入input_ids\n                input_ids, self.config.pad_token_id, decoder_start_token_id=self.config.decoder_start_token_id\n            )\n        if decoder_attention_mask is None: # 设置默认的解码器自注意力因果掩码\n            decoder_attention_mask = jnp.ones_like(decoder_input_ids)\n        if decoder_position_ids is None: # 设置默认的解码器目标序列位置ids\n            batch_size, sequence_length = decoder_input_ids.shape\n            decoder_position_ids = jnp.broadcast_to(\n                jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)\n            )\n\n        # Handle any PRNG if needed 处理随机数 rng流\n        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n        # self.__call__() 是模型封装类的前向调用；\n        # self.module.__call__() 才是网络结构的具体实现。\n        # 这并不是递归调用 self.__call__，而是调用 子模块.__call__()，也就是你定义的 transformer 编码器-解码器网络结构。\n        return self.module.apply(\n            {\"params\": params or self.params}, # 参数\n            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n            decoder_position_ids=jnp.array(decoder_position_ids, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:53:00.939168Z","iopub.execute_input":"2025-05-27T06:53:00.939425Z","iopub.status.idle":"2025-05-27T06:53:00.967859Z","shell.execute_reply.started":"2025-05-27T06:53:00.939408Z","shell.execute_reply":"2025-05-27T06:53:00.967326Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"help(FlaxBartPreTrainedModel.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n# >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n# >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n# >>> text = \"My friends are cool but they eat too many carbs.\"\n# >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n# >>> encoder_outputs = model.encode(**inputs)\n\n# >>> decoder_start_token_id = model.config.decoder_start_token_id\n# >>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n# >>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n# >>> last_decoder_hidden_states = outputs.last_hidden_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"The bare Bart Model transformer outputting raw hidden-states without any specific head on top.\",\n    BART_START_DOCSTRING,\n)\nclass FlaxBartModel(FlaxBartPreTrainedModel): \n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n    module_class = FlaxBartModule # 设定的子模块 返回解码器最后一层的输出\n\nappend_call_sample_docstring(FlaxBartModel, _CHECKPOINT_FOR_DOC, FlaxSeq2SeqModelOutput, _CONFIG_FOR_DOC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:59:05.802125Z","iopub.execute_input":"2025-05-27T06:59:05.802696Z","iopub.status.idle":"2025-05-27T06:59:05.807038Z","shell.execute_reply.started":"2025-05-27T06:59:05.802675Z","shell.execute_reply":"2025-05-27T06:59:05.806336Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"help(FlaxBartModel.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxBartModel\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = FlaxBartModel.from_pretrained(\"facebook/bart-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:00:06.887680Z","iopub.execute_input":"2025-05-27T07:00:06.887942Z","iopub.status.idle":"2025-05-27T07:00:22.400591Z","shell.execute_reply.started":"2025-05-27T07:00:06.887923Z","shell.execute_reply":"2025-05-27T07:00:22.400006Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a43f71f00bc480fb5881a6f2427c957"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56bd03799be4bc5a0821a708826df18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539c45dd3604477dab79d42a61a88a79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073cf3750cf34f65aefbd14bc8dc5b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"flax_model.msgpack:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60560c2f20c246b1a60b2d9f97e5f39a"}},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:00:22.401857Z","iopub.execute_input":"2025-05-27T07:00:22.402125Z","iopub.status.idle":"2025-05-27T07:00:22.441904Z","shell.execute_reply.started":"2025-05-27T07:00:22.402101Z","shell.execute_reply":"2025-05-27T07:00:22.441401Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:00:22.599467Z","iopub.execute_input":"2025-05-27T07:00:22.599698Z","iopub.status.idle":"2025-05-27T07:00:30.106984Z","shell.execute_reply.started":"2025-05-27T07:00:22.599680Z","shell.execute_reply":"2025-05-27T07:00:30.106458Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"last_hidden_states = outputs.last_hidden_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:00:30.107917Z","iopub.execute_input":"2025-05-27T07:00:30.108157Z","iopub.status.idle":"2025-05-27T07:00:30.111738Z","shell.execute_reply.started":"2025-05-27T07:00:30.108131Z","shell.execute_reply":"2025-05-27T07:00:30.111247Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"last_hidden_states.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:00:44.492512Z","iopub.execute_input":"2025-05-27T07:00:44.492778Z","iopub.status.idle":"2025-05-27T07:00:44.497407Z","shell.execute_reply.started":"2025-05-27T07:00:44.492758Z","shell.execute_reply":"2025-05-27T07:00:44.496818Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"(1, 8, 768)"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"class FlaxBartForConditionalGenerationModule(nn.Module): # 条件生成模块\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32\n    bias_init: Callable[..., jnp.ndarray] = jax.nn.initializers.zeros # 偏距初始化方法\n\n    def setup(self):\n        self.model = FlaxBartModule(config=self.config, dtype=self.dtype)  # 设定的子模块 返回解码器最后一层的输出\n        self.lm_head = nn.Dense(  # 掩码语言头\n            self.model.shared.num_embeddings,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n        # bias参数\n        # 这是一个占位的偏置项，方便微调场景中手动设置（如 adapter tuning、LoRA 之类），但默认主干训练时保持不变，类似一个可注册但冻结的 bias\n        self.final_logits_bias = self.param(\"final_logits_bias\", self.bias_init, (1, self.model.shared.num_embeddings))\n\n    def _get_encoder_module(self):\n        return self.model.encoder\n\n    def _get_decoder_module(self):\n        return self.model.decoder\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        decoder_input_ids,\n        decoder_attention_mask,\n        position_ids,\n        decoder_position_ids,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        outputs = self.model( # 获取encoder-decoder的输出\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            position_ids=position_ids,\n            decoder_position_ids=decoder_position_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        hidden_states = outputs[0] # 最后一层的隐藏状态 (b,s,d)\n        # 如果设定要共享词嵌入\n        if self.config.tie_word_embeddings:\n            shared_embedding = self.model.variables[\"params\"][\"shared\"][\"embedding\"] # 获取嵌入\n            # 调用lm_head获取输出lm_logits hidden_states传入的参数\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n        else: # 不共享时,直接前向\n            lm_logits = self.lm_head(hidden_states) \n        # 加上截距\n        # 训练时禁用梯度（stop_gradient）的做法，主要是：\n        # 保留这个 bias，不让它被损失函数优化过程干扰。\n        lm_logits += jax.lax.stop_gradient(self.final_logits_bias.astype(self.dtype))\n        # 返回输出 元组或结构字典\n        if not return_dict:\n            output = (lm_logits,) + outputs[1:]\n            return output\n\n        return FlaxSeq2SeqLMOutput(\n            logits=lm_logits,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:14:54.722834Z","iopub.execute_input":"2025-05-27T07:14:54.723124Z","iopub.status.idle":"2025-05-27T07:14:54.733718Z","shell.execute_reply.started":"2025-05-27T07:14:54.723103Z","shell.execute_reply":"2025-05-27T07:14:54.733118Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# 用于微调阶段、加载 checkpoint 时的结构对齐\n# self.param(\"final_logits_bias\", ...) 注册了这个参数结构，哪怕不参与训练；\n# 这允许你在微调或加载权重时，自动从 .ckpt 或 .safetensors 中加载已有的偏置值；\n# 不需要写自定义加载逻辑，也不需要自定义 forward 接口。\n# 换句话说：\n# 即使你不更新这个 bias，但你依然可能加载到了非零值，并在推理中使用。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n)\nclass FlaxBartForConditionalGeneration(FlaxBartPreTrainedModel): # 条件生成模型\n    module_class = FlaxBartForConditionalGenerationModule # 子模块 返回(b,s,v)\n    dtype: jnp.dtype = jnp.float32\n\n    @add_start_docstrings(BART_DECODE_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=FlaxCausalLMOutputWithCrossAttentions, config_class=BartConfig)\n    def decode(\n        self,\n        decoder_input_ids,\n        encoder_outputs,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        decoder_attention_mask: Optional[jnp.ndarray] = None,\n        decoder_position_ids: Optional[jnp.ndarray] = None,\n        past_key_values: dict = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train: bool = False,\n        params: dict = None,\n        dropout_rng: PRNGKey = None,\n    ):\n        r\"\"\"\n        Returns:\n\n        Example:\n\n        ```python\n        >>> import jax.numpy as jnp\n        >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n        >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n        >>> text = \"My friends are cool but they eat too many carbs.\"\n        >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n        >>> encoder_outputs = model.encode(**inputs)\n\n        >>> decoder_start_token_id = model.config.decoder_start_token_id\n        >>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n        >>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n        >>> logits = outputs.logits\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n        \n        encoder_hidden_states = encoder_outputs[0] # 获取编码器输出\n        if encoder_attention_mask is None: # 编码器填充mask\n            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        batch_size, sequence_length = decoder_input_ids.shape\n        if decoder_attention_mask is None: # 设置默认的解码器因果mask\n            decoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        if decoder_position_ids is None: # 设置默认的解码器位置ids\n            # 如果有缓存,就必须传入decoder_position_ids\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `decoder_position_ids` when passing `past_key_values`.\")\n\n            decoder_position_ids = jnp.broadcast_to(\n                jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)\n            )\n\n        # 设置rngs 这里设置dropout随机流\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} # 设置输入 参数 \n\n        # 如果传递了 past_key_values，则缓存已初始化，必须有一个私有标志 init_cache\n        # 传递以确保使用缓存。必须确保缓存被标记为可变的，以便\n        # 可以通过 FlaxBartAttention 模块进行更改\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values # 这个作为参数的一部分 名称:cache\n            mutable = [\"cache\"] # 设置缓存参数可变\n        else: \n            mutable = False\n        \n        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, decoder_position_ids, **kwargs):\n            decoder_module = module._get_decoder_module()\n            outputs = decoder_module(\n                decoder_input_ids,\n                decoder_attention_mask,\n                decoder_position_ids,\n                **kwargs,\n            )\n            hidden_states = outputs[0] # 解码器输出\n            # 设置lm_logits,共享嵌入和不共享嵌入的两种情况\n            if self.config.tie_word_embeddings: \n                shared_embedding = module.model.variables[\"params\"][\"shared\"][\"embedding\"]\n                lm_logits = module.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n            else:\n                lm_logits = module.lm_head(hidden_states)\n            # 返回掩码语言头输出\n            lm_logits += module.final_logits_bias.astype(self.dtype)\n            return lm_logits, outputs\n        # 只使用解码器的情况\n        outputs = self.module.apply(\n            inputs,\n            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n            decoder_position_ids=jnp.array(decoder_position_ids, dtype=\"i4\"),\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=jnp.array(encoder_attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n            mutable=mutable,\n            method=_decoder_forward,\n        )\n\n        if past_key_values is None: # 如果没有缓存\n            lm_logits, decoder_outputs = outputs\n        else: # 有缓存的情况 解包情况\n            (lm_logits, decoder_outputs), past = outputs\n        # 返回结果\n        if return_dict:\n            outputs = FlaxCausalLMOutputWithCrossAttentions(\n                logits=lm_logits,\n                hidden_states=decoder_outputs.hidden_states,\n                attentions=decoder_outputs.attentions,\n                cross_attentions=decoder_outputs.cross_attentions,\n            )\n        else:\n            outputs = (lm_logits,) + decoder_outputs[1:]\n\n        # 如果有缓存,并且设定返回字典\n        if past_key_values is not None and return_dict:\n            outputs[\"past_key_values\"] = unfreeze(past[\"cache\"]) # 把缓存加入输出的字典结构\n            return outputs\n        elif past_key_values is not None and not return_dict: # 返回元组的情况\n            outputs = outputs[:1] + (unfreeze(past[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        max_length,\n        attention_mask: Optional[jax.Array] = None,\n        decoder_attention_mask: Optional[jax.Array] = None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # 获取当前解码序列的形状\n        batch_size, seq_length = decoder_input_ids.shape\n        # 初始化缓存结构（用于加速解码），包含自注意力和交叉注意力的 key/value 状态\n        past_key_values = self.init_cache(batch_size, max_length, encoder_outputs)\n        # 注意，通常情况下，对于位置 x > input_ids.shape[-1] 和 x < cache_length，我们需要在 attention_mask 中填充 0。\n        # 但由于 decoder 使用的是因果掩码（causal mask），这些位置反正也会被屏蔽。\n        # 因此我们可以在这里构造一个静态的 attention_mask，这样在编译时更高效 扩展注意力掩码\n        # 构造扩展的 decoder attention mask（形状为 [batch_size, max_length]）\n        # 由于 decoder 使用 causal mask，理论上不需要手动屏蔽未来的位置，因此这里用静态的 mask 即可编译时优化\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if decoder_attention_mask is not None:\n            # 生成 position_ids：即每个 token 的位置，基于已有 mask 的 cumsum\n            position_ids = decoder_attention_mask.cumsum(axis=-1) - 1\n            # 将实际的 decoder_attention_mask 更新到静态模板中，以保留有效位置\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, decoder_attention_mask, (0, 0))\n        else: # 如果未提供 decoder_attention_mask，则直接生成标准的连续位置编码\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n        # 返回 decoder 所需的全部输入，供生成过程中的单步调用使用\n        return {\n            \"past_key_values\": past_key_values, # 用于缓存历史的 attention 信息\n            \"encoder_outputs\": encoder_outputs,  # 编码器的输出，供 cross-attention 使用\n            \"encoder_attention_mask\": attention_mask,  # 编码器侧的 attention mask\n            \"decoder_attention_mask\": extended_attention_mask,  # 解码器侧的 attention mask（已扩展）\n            \"decoder_position_ids\": position_ids,  # 解码器的位置编码\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n         # 更新缓存值为当前输出中的 past_key_values（即更新后的解码历史）\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        # 更新 position_ids，令其自增一（只保留最新位置，用于下一步生成）\n        model_kwargs[\"decoder_position_ids\"] = model_kwargs[\"decoder_position_ids\"][:, -1:] + 1\n        return model_kwargs # 返回更新后的 kwargs，供下一步生成使用","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:36:58.036878Z","iopub.execute_input":"2025-05-27T07:36:58.037679Z","iopub.status.idle":"2025-05-27T07:36:58.054035Z","shell.execute_reply.started":"2025-05-27T07:36:58.037655Z","shell.execute_reply":"2025-05-27T07:36:58.053314Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"help(FlaxBartForConditionalGeneration.decode)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n    \nmodel = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:38:39.637587Z","iopub.execute_input":"2025-05-27T07:38:39.637827Z","iopub.status.idle":"2025-05-27T07:38:59.063044Z","shell.execute_reply.started":"2025-05-27T07:38:39.637811Z","shell.execute_reply":"2025-05-27T07:38:59.062477Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9159b9e95124f04abfee7884080c33e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"flax_model.msgpack:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a6592f76bc64b60aca4eae7f3f1ee23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c244bac263545d88790580d730f39fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bed379cc0ca4aa697e08033c72721d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a29d39971574a398e4a768388cda8a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c6ce5135dd44799a3bedfbfbb10860"}},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"text = \"My friends are cool but they eat too many carbs.\"\ninputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:38:59.064387Z","iopub.execute_input":"2025-05-27T07:38:59.064709Z","iopub.status.idle":"2025-05-27T07:38:59.086031Z","shell.execute_reply.started":"2025-05-27T07:38:59.064692Z","shell.execute_reply":"2025-05-27T07:38:59.085556Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"encoder_outputs = model.encode(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:39:02.489552Z","iopub.execute_input":"2025-05-27T07:39:02.490004Z","iopub.status.idle":"2025-05-27T07:39:04.946472Z","shell.execute_reply.started":"2025-05-27T07:39:02.489983Z","shell.execute_reply":"2025-05-27T07:39:04.945692Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"decoder_start_token_id = model.config.decoder_start_token_id\ndecoder_start_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:39:39.136837Z","iopub.execute_input":"2025-05-27T07:39:39.137357Z","iopub.status.idle":"2025-05-27T07:39:39.142162Z","shell.execute_reply.started":"2025-05-27T07:39:39.137333Z","shell.execute_reply":"2025-05-27T07:39:39.141462Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:40:33.637601Z","iopub.execute_input":"2025-05-27T07:40:33.638047Z","iopub.status.idle":"2025-05-27T07:40:33.644191Z","shell.execute_reply.started":"2025-05-27T07:40:33.638025Z","shell.execute_reply":"2025-05-27T07:40:33.643624Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"Array([[1]], dtype=int32)"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:39:59.436397Z","iopub.execute_input":"2025-05-27T07:39:59.436631Z","iopub.status.idle":"2025-05-27T07:39:59.485392Z","shell.execute_reply.started":"2025-05-27T07:39:59.436615Z","shell.execute_reply":"2025-05-27T07:39:59.484808Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"decoder_input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:40:53.310465Z","iopub.execute_input":"2025-05-27T07:40:53.311124Z","iopub.status.idle":"2025-05-27T07:40:53.315708Z","shell.execute_reply.started":"2025-05-27T07:40:53.311101Z","shell.execute_reply":"2025-05-27T07:40:53.315074Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"Array([[2]], dtype=int32)"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"outputs = model.decode(decoder_input_ids, encoder_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:41:16.563082Z","iopub.execute_input":"2025-05-27T07:41:16.563816Z","iopub.status.idle":"2025-05-27T07:41:17.798176Z","shell.execute_reply.started":"2025-05-27T07:41:16.563793Z","shell.execute_reply":"2025-05-27T07:41:17.797653Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"logits = outputs.logits\nlogits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:41:36.149489Z","iopub.execute_input":"2025-05-27T07:41:36.150042Z","iopub.status.idle":"2025-05-27T07:41:36.154290Z","shell.execute_reply.started":"2025-05-27T07:41:36.150021Z","shell.execute_reply":"2025-05-27T07:41:36.153660Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"(1, 1, 50264)"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"FLAX_BART_CONDITIONAL_GENERATION_DOCSTRING = \"\"\"\n    Returns:\n\n    Summarization example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n    >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n    >>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"np\")\n\n    >>> # Generate Summary\n    >>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n    >>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n    ```\n\n    Mask filling example:\n\n    ```python\n    >>> import jax\n    >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n    >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n\n    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n    >>> input_ids = tokenizer([TXT], return_tensors=\"jax\")[\"input_ids\"]\n\n    >>> logits = model(input_ids).logits\n    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero()[0].item()\n    >>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)\n    >>> values, predictions = jax.lax.top_k(probs, k=1)\n\n    >>> tokenizer.decode(predictions).split()\n    ```\n\"\"\"\noverwrite_call_docstring(\n    FlaxBartForConditionalGeneration, \n    BART_INPUTS_DOCSTRING + FLAX_BART_CONDITIONAL_GENERATION_DOCSTRING\n)\nappend_replace_return_docstrings(\n    FlaxBartForConditionalGeneration,\n    output_type=FlaxSeq2SeqLMOutput, config_class=_CONFIG_FOR_DOC\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:42:24.881516Z","iopub.execute_input":"2025-05-27T07:42:24.881805Z","iopub.status.idle":"2025-05-27T07:42:24.888333Z","shell.execute_reply.started":"2025-05-27T07:42:24.881784Z","shell.execute_reply":"2025-05-27T07:42:24.887590Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"jnp.where(jnp.array([[1,2,3]]) == 2, 1, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:45:16.004357Z","iopub.execute_input":"2025-05-27T07:45:16.004622Z","iopub.status.idle":"2025-05-27T07:45:16.107611Z","shell.execute_reply.started":"2025-05-27T07:45:16.004601Z","shell.execute_reply":"2025-05-27T07:45:16.106877Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"Array([[0, 1, 0]], dtype=int32, weak_type=True)"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"class FlaxBartForSequenceClassificationModule(nn.Module): # 序列分类模块\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32\n    num_labels: Optional[int] = None\n\n    def setup(self):\n        self.model = FlaxBartModule(config=self.config, dtype=self.dtype) # 返回解码器输出(b,s,d)\n        self.classification_head = FlaxBartClassificationHead( # 分类头\n            config=self.config,\n            inner_dim=self.config.d_model,\n            num_classes=self.num_labels if self.num_labels is not None else self.config.num_labels,\n            pooler_dropout=self.config.classifier_dropout,\n        )\n\n    def _get_encoder_module(self):\n        return self.model.encoder\n\n    def _get_decoder_module(self):\n        return self.model.decoder\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        decoder_input_ids,\n        decoder_attention_mask,\n        position_ids,\n        decoder_position_ids,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        outputs = self.model( \n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            position_ids=position_ids,\n            decoder_position_ids=decoder_position_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        hidden_states = outputs[0]  # last hidden state\n        # eos_token_id的位置是1,其他位置是0\n        eos_mask = jnp.where(input_ids == self.config.eos_token_id, 1, 0)\n\n        # 检查是否在 JAX JIT 编译过程中，如果是 tracer 则跳过后续值检查（避免 concretization 错误）\n        if not isinstance(eos_mask, jax.interpreters.partial_eval.DynamicJaxprTracer):\n            # 确保每个样本都有相同数量的 <eos>，否则 raise 异常\n            if len(jnp.unique(eos_mask.sum(1))) > 1:\n                raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n             # 检查是否存在缺失 <eos> 的情况\n            if any(eos_mask.sum(1) == 0):\n                raise ValueError(\"There are missing <eos> tokens in input_ids\")\n\n            # 处理多个 <eos> 的情况：通过加小噪声找出最后一个 <eos> 的位置\n            # 例如两个位置为 <eos>，会保留最后一个为1，其余位置为0\n            eos_mask_noised = eos_mask + jnp.arange(eos_mask.shape[1]) * 1e-6\n            eos_mask = jnp.where(eos_mask_noised == eos_mask_noised.max(1).reshape(-1, 1), 1, 0)\n        # 使用 einsum 提取最后一个 <eos> 位置对应的 hidden_state 作为句子表示\n        # 实际计算等价于：hidden_states * eos_mask[:, :, None] 再在 seq_len 维度上求和\n        sentence_representation = jnp.einsum(\"ijk, ij -> ijk\", hidden_states, eos_mask).sum(1)\n        # 将句子表示输入到分类头，得到最终 logits（如情感分类等任务）\n        logits = self.classification_head(sentence_representation, deterministic=deterministic)\n        # 返回元组或字典结构\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return output\n        # \n        return FlaxSeq2SeqSequenceClassifierOutput(\n            logits=logits, # 分类分数\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:50:04.693839Z","iopub.execute_input":"2025-05-27T07:50:04.694603Z","iopub.status.idle":"2025-05-27T07:50:04.705606Z","shell.execute_reply.started":"2025-05-27T07:50:04.694577Z","shell.execute_reply":"2025-05-27T07:50:04.704846Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n    tasks.\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass FlaxBartForSequenceClassification(FlaxBartPreTrainedModel): # 序列分类模型\n    module_class = FlaxBartForSequenceClassificationModule # 使用具体的模块做事\n    dtype = jnp.float32\n\n\nappend_call_sample_docstring(\n    FlaxBartForSequenceClassification,\n    _CHECKPOINT_FOR_DOC,\n    FlaxSeq2SeqSequenceClassifierOutput,\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:50:49.111589Z","iopub.execute_input":"2025-05-27T07:50:49.112276Z","iopub.status.idle":"2025-05-27T07:50:49.116478Z","shell.execute_reply.started":"2025-05-27T07:50:49.112252Z","shell.execute_reply":"2025-05-27T07:50:49.115750Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"help(FlaxBartForSequenceClassification.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxBartForSequenceClassification\n    \ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = FlaxBartForSequenceClassification.from_pretrained(\"facebook/bart-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:51:43.798176Z","iopub.execute_input":"2025-05-27T07:51:43.798467Z","iopub.status.idle":"2025-05-27T07:51:47.045589Z","shell.execute_reply.started":"2025-05-27T07:51:43.798446Z","shell.execute_reply":"2025-05-27T07:51:47.044859Z"}},"outputs":[{"name":"stderr","text":"Some weights of FlaxBartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: {('classification_head', 'dense', 'bias'), ('classification_head', 'out_proj', 'bias'), ('classification_head', 'dense', 'kernel'), ('classification_head', 'out_proj', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:51:50.621696Z","iopub.execute_input":"2025-05-27T07:51:50.622332Z","iopub.status.idle":"2025-05-27T07:51:50.627137Z","shell.execute_reply.started":"2025-05-27T07:51:50.622281Z","shell.execute_reply":"2025-05-27T07:51:50.626557Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"outputs = model(**inputs)\nlogits = outputs.logits\nlogits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:52:06.283267Z","iopub.execute_input":"2025-05-27T07:52:06.283552Z","iopub.status.idle":"2025-05-27T07:52:07.208397Z","shell.execute_reply.started":"2025-05-27T07:52:06.283532Z","shell.execute_reply":"2025-05-27T07:52:07.207701Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"(1, 3)"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"key = jax.random.PRNGKey(0)\nb, s = 4, 5  # 举例\nx = jax.random.normal(key, shape=(b, s, 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:56:04.580124Z","iopub.execute_input":"2025-05-27T07:56:04.580439Z","iopub.status.idle":"2025-05-27T07:56:04.890023Z","shell.execute_reply.started":"2025-05-27T07:56:04.580416Z","shell.execute_reply":"2025-05-27T07:56:04.889255Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"x.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:56:13.908341Z","iopub.execute_input":"2025-05-27T07:56:13.908882Z","iopub.status.idle":"2025-05-27T07:56:13.913103Z","shell.execute_reply.started":"2025-05-27T07:56:13.908861Z","shell.execute_reply":"2025-05-27T07:56:13.912570Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(4, 5, 2)"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"a,b = jnp.split(x,2, axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:56:20.598691Z","iopub.execute_input":"2025-05-27T07:56:20.599481Z","iopub.status.idle":"2025-05-27T07:56:20.651030Z","shell.execute_reply.started":"2025-05-27T07:56:20.599458Z","shell.execute_reply":"2025-05-27T07:56:20.650485Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"print(a.shape,b.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:56:31.838183Z","iopub.execute_input":"2025-05-27T07:56:31.838456Z","iopub.status.idle":"2025-05-27T07:56:31.842371Z","shell.execute_reply.started":"2025-05-27T07:56:31.838438Z","shell.execute_reply":"2025-05-27T07:56:31.841628Z"}},"outputs":[{"name":"stdout","text":"(4, 5, 1) (4, 5, 1)\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"class FlaxBartForQuestionAnsweringModule(nn.Module):\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32\n    num_labels = 2\n\n    def setup(self):\n        self.model = FlaxBartModule(config=self.config, dtype=self.dtype) # 返回(b,s,d)\n        self.qa_outputs = nn.Dense( # 返回(b,s,2)\n            self.num_labels, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n        )\n\n    def _get_encoder_module(self):\n        return self.model.encoder\n\n    def _get_decoder_module(self):\n        return self.model.decoder\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        decoder_input_ids,\n        decoder_attention_mask,\n        position_ids,\n        decoder_position_ids,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            position_ids=position_ids,\n            decoder_position_ids=decoder_position_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        sequence_output = outputs[0] # 返回最后一个decoder的输出 (b,s,d)\n\n        logits = self.qa_outputs(sequence_output) # (b,s,2)\n        start_logits, end_logits = jnp.split(logits, logits.shape[-1], axis=-1) # 拆分\n        start_logits = start_logits.squeeze(-1) # 模型对答案起始的预测\n        end_logits = end_logits.squeeze(-1) # 模型对答案结束的预测\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[1:]\n            return output\n\n        return FlaxSeq2SeqQuestionAnsweringModelOutput(\n            start_logits=start_logits,\n            end_logits=end_logits,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:57:43.086891Z","iopub.execute_input":"2025-05-27T07:57:43.087397Z","iopub.status.idle":"2025-05-27T07:57:43.096314Z","shell.execute_reply.started":"2025-05-27T07:57:43.087373Z","shell.execute_reply":"2025-05-27T07:57:43.095582Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass FlaxBartForQuestionAnswering(FlaxBartPreTrainedModel):\n    module_class = FlaxBartForQuestionAnsweringModule\n    dtype = jnp.float32\n\n\nappend_call_sample_docstring(\n    FlaxBartForQuestionAnswering,\n    _CHECKPOINT_FOR_DOC,\n    FlaxSeq2SeqQuestionAnsweringModelOutput,\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:57:57.752384Z","iopub.execute_input":"2025-05-27T07:57:57.752654Z","iopub.status.idle":"2025-05-27T07:57:57.757155Z","shell.execute_reply.started":"2025-05-27T07:57:57.752634Z","shell.execute_reply":"2025-05-27T07:57:57.756471Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"help(FlaxBartForQuestionAnswering.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxBartForQuestionAnswering\n    \ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = FlaxBartForQuestionAnswering.from_pretrained(\"facebook/bart-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:58:46.034227Z","iopub.execute_input":"2025-05-27T07:58:46.034500Z","iopub.status.idle":"2025-05-27T07:58:48.594600Z","shell.execute_reply.started":"2025-05-27T07:58:46.034480Z","shell.execute_reply":"2025-05-27T07:58:48.594066Z"}},"outputs":[{"name":"stderr","text":"Some weights of FlaxBartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: {('qa_outputs', 'bias'), ('qa_outputs', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\ninputs = tokenizer(question, text, return_tensors=\"jax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:58:58.578168Z","iopub.execute_input":"2025-05-27T07:58:58.578712Z","iopub.status.idle":"2025-05-27T07:58:58.600831Z","shell.execute_reply.started":"2025-05-27T07:58:58.578691Z","shell.execute_reply":"2025-05-27T07:58:58.599940Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:59:05.184293Z","iopub.execute_input":"2025-05-27T07:59:05.184873Z","iopub.status.idle":"2025-05-27T07:59:08.815010Z","shell.execute_reply.started":"2025-05-27T07:59:05.184853Z","shell.execute_reply":"2025-05-27T07:59:08.814435Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"start_scores = outputs.start_logits\nend_scores = outputs.end_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:59:16.238964Z","iopub.execute_input":"2025-05-27T07:59:16.239476Z","iopub.status.idle":"2025-05-27T07:59:16.243162Z","shell.execute_reply.started":"2025-05-27T07:59:16.239452Z","shell.execute_reply":"2025-05-27T07:59:16.242362Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"print(start_scores.shape,end_scores.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:59:42.048031Z","iopub.execute_input":"2025-05-27T07:59:42.048733Z","iopub.status.idle":"2025-05-27T07:59:42.052300Z","shell.execute_reply.started":"2025-05-27T07:59:42.048709Z","shell.execute_reply":"2025-05-27T07:59:42.051706Z"}},"outputs":[{"name":"stdout","text":"(1, 17) (1, 17)\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"class FlaxBartDecoderPreTrainedModel(FlaxPreTrainedModel): # 解码器预训练模型\n    config_class = BartConfig\n    base_model_prefix: str = \"model\"\n    module_class: nn.Module = None  # 子模块\n\n    def __init__(\n        self,\n        config: BartConfig,\n        input_shape: Tuple[int] = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        config.is_decoder = True # 单解码器架构\n        config.is_encoder_decoder = False # 非编码器-解码器双架构\n        module = self.module_class(config=config, dtype=dtype, **kwargs) # 子模块\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n    # 初始化权重\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensors\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n\n        batch_size, sequence_length = input_ids.shape\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n        # 拆分出参数流和dropout流\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng} # rngs\n        encoder_hidden_states = jnp.zeros(input_shape + (self.config.d_model,)) # (b,s,d)\n        encoder_attention_mask = attention_mask \n        module_init_outputs = self.module.init( \n            rngs, # rngs \n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False, # 返回元组\n        )\n        return module_init_outputs[\"params\"] # 返回初始化后的参数\n\n    def init_cache(self, batch_size, max_length):\n        r\"\"\"\n        Args:\n            batch_size (`int`):\n                batch_size used for fast auto-regressive decoding. Defines the batch size of the initialized cache.\n            max_length (`int`):\n                maximum possible length for auto-regressive decoding. Defines the sequence length of the initialized\n                cache.\n        \"\"\"\n        # init input variables to retrieve cache\n        input_ids = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n        \n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return unfreeze(init_variables[\"cache\"])\n\n    @add_start_docstrings_to_model_forward(BART_DECODE_INPUTS_DOCSTRING)\n    def __call__(\n        self,\n        input_ids: jnp.ndarray,\n        attention_mask: Optional[jnp.ndarray] = None,\n        position_ids: Optional[jnp.ndarray] = None,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train: bool = False,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: PRNGKey = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n        # 如果有编码器输出 没有编码器填充掩码 设置默认\n        if encoder_hidden_states is not None and encoder_attention_mask is None:\n            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # prepare decoder inputs\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            batch_size, sequence_length = input_ids.shape\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n\n        inputs = {\"params\": params or self.params}\n\n        # if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed\n        # down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be\n        # changed by FlaxBartAttention module\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:07:59.649710Z","iopub.execute_input":"2025-05-27T08:07:59.650339Z","iopub.status.idle":"2025-05-27T08:07:59.666528Z","shell.execute_reply.started":"2025-05-27T08:07:59.650316Z","shell.execute_reply":"2025-05-27T08:07:59.665773Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"class FlaxBartDecoderWrapper(nn.Module):\n    \"\"\"\n    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n    used in combination with the [`EncoderDecoderModel`] framework.\n    \"\"\"\n\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        embed_dim = self.config.d_model\n        embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            embed_dim,\n            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n            dtype=self.dtype,\n        )\n        self.decoder = FlaxBartDecoder(config=self.config, embed_tokens=embed_tokens, dtype=self.dtype)\n\n    def __call__(self, *args, **kwargs):\n        return self.decoder(*args, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:08:24.466492Z","iopub.execute_input":"2025-05-27T08:08:24.467038Z","iopub.status.idle":"2025-05-27T08:08:24.473022Z","shell.execute_reply.started":"2025-05-27T08:08:24.467016Z","shell.execute_reply":"2025-05-27T08:08:24.472318Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"class FlaxBartForCausalLMModule(nn.Module):\n    config: BartConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.model = FlaxBartDecoderWrapper(config=self.config, dtype=self.dtype)\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        position_ids,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        deterministic: bool = True,\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_embedding = self.model.variables[\"params\"][\"decoder\"][\"embed_tokens\"][\"embedding\"]\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutputWithCrossAttentions(\n            logits=lm_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:08:53.887531Z","iopub.execute_input":"2025-05-27T08:08:53.888071Z","iopub.status.idle":"2025-05-27T08:08:53.896158Z","shell.execute_reply.started":"2025-05-27T08:08:53.888047Z","shell.execute_reply":"2025-05-27T08:08:53.895569Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Bart Decoder Model with a language modeling head on top (linear layer with weights tied to the input embeddings)\n    e.g for autoregressive tasks.\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass FlaxBartForCausalLM(FlaxBartDecoderPreTrainedModel):\n    module_class = FlaxBartForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jax.Array] = None):\n        # initializing the cache\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n        # But since the decoder uses a causal mask, those positions are masked anyway.\n        # Thus, we can create a single static attention_mask here, which is more efficient for compilation\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n\n\nappend_call_sample_docstring(\n    FlaxBartForCausalLM,\n    _CHECKPOINT_FOR_DOC,\n    FlaxCausalLMOutputWithCrossAttentions,\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:09:09.282530Z","iopub.execute_input":"2025-05-27T08:09:09.282806Z","iopub.status.idle":"2025-05-27T08:09:09.289900Z","shell.execute_reply.started":"2025-05-27T08:09:09.282786Z","shell.execute_reply":"2025-05-27T08:09:09.289132Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"help(FlaxBartForCausalLM.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxBartForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = FlaxBartForCausalLM.from_pretrained(\"facebook/bart-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\noutputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:10:21.979183Z","iopub.execute_input":"2025-05-27T08:10:21.979655Z","iopub.status.idle":"2025-05-27T08:10:22.373283Z","shell.execute_reply.started":"2025-05-27T08:10:21.979635Z","shell.execute_reply":"2025-05-27T08:10:22.372647Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"outputs.logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:10:56.895640Z","iopub.execute_input":"2025-05-27T08:10:56.896076Z","iopub.status.idle":"2025-05-27T08:10:56.900542Z","shell.execute_reply.started":"2025-05-27T08:10:56.896054Z","shell.execute_reply":"2025-05-27T08:10:56.899966Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"(1, 8, 50265)"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"next_token_logits = outputs.logits[:, -1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:10:32.296999Z","iopub.execute_input":"2025-05-27T08:10:32.297692Z","iopub.status.idle":"2025-05-27T08:10:32.495164Z","shell.execute_reply.started":"2025-05-27T08:10:32.297668Z","shell.execute_reply":"2025-05-27T08:10:32.494374Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"next_token_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:10:47.055709Z","iopub.execute_input":"2025-05-27T08:10:47.056167Z","iopub.status.idle":"2025-05-27T08:10:47.060633Z","shell.execute_reply.started":"2025-05-27T08:10:47.056143Z","shell.execute_reply":"2025-05-27T08:10:47.059857Z"}},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"(1, 50265)"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"__all__ = [\n    \"FlaxBartDecoderPreTrainedModel\",\n    \"FlaxBartForCausalLM\",\n    \"FlaxBartForConditionalGeneration\",\n    \"FlaxBartForQuestionAnswering\",\n    \"FlaxBartForSequenceClassification\",\n    \"FlaxBartModel\",\n    \"FlaxBartPreTrainedModel\",\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}