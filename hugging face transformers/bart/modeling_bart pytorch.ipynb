{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Python 标准库\nimport copy  # 用于复制对象（深拷贝）\nimport math  # 提供数学计算函数（如sqrt、log等）\nimport warnings  # 用于显示/管理警告信息\n# 类型提示相关\nfrom typing import List, Optional, Tuple, Union # 类型注解支持\n# PyTorch 库\nimport torch\nimport torch.utils.checkpoint # 实现梯度检查点，节省显存\nfrom torch import nn   # 包含神经网络层定义\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss # 常用损失函数\n# Hugging Face Transformers 库\n# 激活函数映射（字符串转函数）\nfrom transformers.activations import ACT2FN\n# 文本生成相关功能的 Mixin 基类\nfrom transformers.generation import GenerationMixin\n# 构建 attention mask 的工具函数（标准/Flash Attention）\nfrom transformers.modeling_attn_mask_utils import (\n    _prepare_4d_attention_mask,  # 一般 attention mask\n    _prepare_4d_attention_mask_for_sdpa,  # SDPA 支持的 mask\n    _prepare_4d_causal_attention_mask,  # causal attention mask\n    _prepare_4d_causal_attention_mask_for_sdpa,  # causal mask for SDPA\n)\n# Flash Attention 支持检查及工具 判断是否支持左上角掩码（Flash Attention 限制）  判断 Flash Attention 是否可用\nfrom transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n# 模型输出结构定义\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n    Seq2SeqQuestionAnsweringModelOutput,\n    Seq2SeqSequenceClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel # 模型基类\n# 一些实用工具（文档注释、日志等）\nfrom transformers.utils import (\n    add_code_sample_docstrings,  # 添加代码示例文档\n    add_end_docstrings,    # 添加文档结尾注释\n    add_start_docstrings,  # 添加文档开头注释\n    add_start_docstrings_to_model_forward,  # 给forward函数添加文档注释\n    logging,   # 提供日志功能\n    replace_return_docstrings, # 替换函数返回的文档描述\n)\n# BART 配置类（定义模型结构超参）\nfrom transformers.models.bart.configuration_bart import BartConfig\n# 若 Flash Attention 可用，则引入其 forward 实现\nif is_flash_attn_available():\n    from transformers.modeling_flash_attention_utils import _flash_attention_forward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:00:57.184032Z","iopub.execute_input":"2025-05-26T17:00:57.184289Z","iopub.status.idle":"2025-05-26T17:01:23.650125Z","shell.execute_reply.started":"2025-05-26T17:00:57.184266Z","shell.execute_reply":"2025-05-26T17:01:23.649486Z"}},"outputs":[{"name":"stderr","text":"2025-05-26 17:01:10.153717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748278870.341187      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748278870.397396      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"is_flash_attn_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:01:23.651300Z","iopub.execute_input":"2025-05-26T17:01:23.651860Z","iopub.status.idle":"2025-05-26T17:01:23.658104Z","shell.execute_reply.started":"2025-05-26T17:01:23.651840Z","shell.execute_reply":"2025-05-26T17:01:23.657326Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from functools import lru_cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:01:23.659086Z","iopub.execute_input":"2025-05-26T17:01:23.659378Z","iopub.status.idle":"2025-05-26T17:01:23.678281Z","shell.execute_reply.started":"2025-05-26T17:01:23.659353Z","shell.execute_reply":"2025-05-26T17:01:23.677567Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 该函数 is_torch_npu_available 用于检测是否支持华为 NPU（Neural Processing Unit）\n# 使用 functools.lru_cache 缓存结果，避免重复检测带来的性能开销；\n# check_device 参数表示是否进一步检查设备可用性。\n@lru_cache()\ndef is_torch_npu_available(check_device=False):\n    # 若 PyTorch 本身不可用或系统未安装 torch_npu，直接返回 False。\n    if not _torch_available or importlib.util.find_spec(\"torch_npu\") is None:\n        return False\n    import torch\n    import torch_npu  # 导入模块，确保其实际可用。\n    # 若启用设备检查，则尝试访问 NPU 设备数量；\n    # 如果设备不可用，会抛出异常，返回 False；\n    # 否则返回 torch.npu.is_available()。\n    if check_device:\n        try:\n            # 如果未找到 NPU，则会引发 RuntimeError\n            _ = torch.npu.device_count()\n            return torch.npu.is_available()\n        except RuntimeError:\n            return False\n    # 在不检查设备的情况下，仅判断 torch.npu 属性存在且可用。\n    return hasattr(torch, \"npu\") and torch.npu.is_available()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers.utils import is_torch_npu_available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:01:31.056281Z","iopub.execute_input":"2025-05-26T17:01:31.056577Z","iopub.status.idle":"2025-05-26T17:01:31.060844Z","shell.execute_reply.started":"2025-05-26T17:01:31.056549Z","shell.execute_reply":"2025-05-26T17:01:31.060016Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"is_torch_npu_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:01:31.269230Z","iopub.execute_input":"2025-05-26T17:01:31.269982Z","iopub.status.idle":"2025-05-26T17:01:31.277919Z","shell.execute_reply.started":"2025-05-26T17:01:31.269946Z","shell.execute_reply":"2025-05-26T17:01:31.277088Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def is_flash_attn_available():\n   # 如果包“flash-attn”可用，则可以本地使用flash-attention。\n    if is_flash_attn_2_available():\n        return True\n    # Flash-Attention 可以在 Ascend NPU 上使用，无需安装 `flash-attn` 包\n    if is_torch_npu_available():\n        return True\n    return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers.utils import is_torch_available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:01:34.193436Z","iopub.execute_input":"2025-05-26T17:01:34.194348Z","iopub.status.idle":"2025-05-26T17:01:34.199161Z","shell.execute_reply.started":"2025-05-26T17:01:34.194314Z","shell.execute_reply":"2025-05-26T17:01:34.198044Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"is_torch_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:01:34.391590Z","iopub.execute_input":"2025-05-26T17:01:34.392360Z","iopub.status.idle":"2025-05-26T17:01:34.399090Z","shell.execute_reply.started":"2025-05-26T17:01:34.392330Z","shell.execute_reply":"2025-05-26T17:01:34.398323Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Flash Attention 必须在 CUDA（NVIDIA GPU）环境下才有效，这是因为它：\n# 依赖 CUDA 内核：Flash Attention 是一种专门为高效 GPU 计算设计的算法，它使用自定义的 CUDA kernel 实现高性能的 attention 运算。\n# 不能在 CPU 上运行：即使你安装了支持 Flash Attention 的版本，若没有 CUDA 设备，运行时也会自动 fallback 到普通 attention 实现。\n# 对 GPU 架构有要求：目前 Flash Attention v1 和 v2 对硬件有不同要求，一般推荐使用 NVIDIA A100、L4、3090、4090 等较新显卡。\n#  Flash Attention 无法在 TPU 上使用的原因：\n# Flash Attention 是为 CUDA/GPU 优化的算法\n# 它底层是用 CUDA kernel（NVIDIA专用） 写的，针对 NVIDIA GPU 的 warp/thread/block 架构深度优化。\n# TPU 使用 XLA 编译器，不支持 CUDA\n# TPU 上运行的是 XLA 编译的算子图，与 GPU 的 CUDA 编程模型完全不同，无法执行 CUDA kernel。\n# Flash Attention 不具备通用实现\n# 当前 Flash Attention（v1/v2）没有公开支持 XLA 或 TPU 专用版本，且其性能优势来自于精细的 CUDA 手工优化。\n# ✅ TPU 上的替代方案：\n# XLA 编译优化的原生 Attention：JAX/Flax 在 TPU 上的注意力计算可自动融合 + 编译优化；\n# Memory-efficient Attention 实现（non-Flash）：如：\n# flax.linen.attention.dot_product_attention（可控制是否使用 causal mask）\n# xformers（虽然也是为 GPU 设计，但部分思路可迁移）","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_flash_attn_2_available():\n    if not is_torch_available(): \n        return False\n    if not _is_package_available(\"flash_attn\"):\n        return False\n    # Let's add an extra check to see if cuda is available\n    import torch\n    if not (torch.cuda.is_available() or is_torch_mlu_available()):\n        return False\n    if torch.version.cuda:\n        return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")\n    elif torch.version.hip:\n        # TODO: Bump the requirement to 2.1.0 once released in https://github.com/ROCmSoftwarePlatform/flash-attention\n        return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.0.4\")\n    elif is_torch_mlu_available():\n        return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.3.3\")\n    else:\n        return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash_attn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from packaging import version\nimport importlib\nversion.parse(importlib.metadata.version(\"flash_attn\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:28.373399Z","iopub.execute_input":"2025-05-26T17:03:28.374262Z","iopub.status.idle":"2025-05-26T17:03:28.390429Z","shell.execute_reply.started":"2025-05-26T17:03:28.374231Z","shell.execute_reply":"2025-05-26T17:03:28.389653Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<Version('2.7.4.post1')>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers.utils import is_flash_attn_2_available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:30.610001Z","iopub.execute_input":"2025-05-26T17:03:30.610275Z","iopub.status.idle":"2025-05-26T17:03:30.614052Z","shell.execute_reply.started":"2025-05-26T17:03:30.610256Z","shell.execute_reply":"2025-05-26T17:03:30.613276Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"is_flash_attn_2_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:30.798072Z","iopub.execute_input":"2025-05-26T17:03:30.798298Z","iopub.status.idle":"2025-05-26T17:03:30.808872Z","shell.execute_reply.started":"2025-05-26T17:03:30.798282Z","shell.execute_reply":"2025-05-26T17:03:30.807923Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"logger = logging.get_logger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:30.979303Z","iopub.execute_input":"2025-05-26T17:03:30.979604Z","iopub.status.idle":"2025-05-26T17:03:30.983735Z","shell.execute_reply.started":"2025-05-26T17:03:30.979573Z","shell.execute_reply":"2025-05-26T17:03:30.982934Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# 在官方文档、教程或示例代码中调用时，默认加载的BART基础预训练模型（\"facebook/bart-base\"）；\n# 用于文档示例的预训练模型检查点名称，指向 Facebook 提供的基础版 BART 模型\n_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n # 用于文档示例的配置类名称，对应 BART 模型的配置接口\n_CONFIG_FOR_DOC = \"BartConfig\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:33.981514Z","iopub.execute_input":"2025-05-26T17:03:33.982234Z","iopub.status.idle":"2025-05-26T17:03:33.985728Z","shell.execute_reply.started":"2025-05-26T17:03:33.982213Z","shell.execute_reply":"2025-05-26T17:03:33.984891Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ✅ 用于说明 Base 模型（如 BartModel）在给定输入下的预期输出形状\n# 用于 docstring 示例中模型 forward() 的预期输出形状验证\n_EXPECTED_OUTPUT_SHAPE = [1, 8, 768]\n# ✅ 用于文档中指定 Sequence Classification 模型的示例检查点（fine-tuned on SST-2）\n_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"valhalla/bart-large-sst2\"\n# ✅ 示例中期望输出的损失值（如测试代码设定 label=gold，loss 应为 0）\n_SEQ_CLASS_EXPECTED_LOSS = 0.0\n# ✅ 说明当输入特定情感文本时，模型的输出标签应为 POSITIVE\n_SEQ_CLASS_EXPECTED_OUTPUT = \"'POSITIVE'\"\n# ✅ 用于文档中示例 QA 模型的检查点（fine-tuned on SQuAD v1）\n_CHECKPOINT_FOR_QA = \"valhalla/bart-large-finetuned-squadv1\"\n# ✅ 给定示例上下文/问题，计算损失的期望值，用于说明训练或推理效果\n_QA_EXPECTED_LOSS = 0.59\n# ✅ 对给定问题和上下文，期望的预测答案字符串，用于 doc 示例验证输出合理性\n_QA_EXPECTED_OUTPUT = \"' nice puppet'\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:34.185353Z","iopub.execute_input":"2025-05-26T17:03:34.185784Z","iopub.status.idle":"2025-05-26T17:03:35.351151Z","shell.execute_reply.started":"2025-05-26T17:03:34.185767Z","shell.execute_reply":"2025-05-26T17:03:35.350268Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 该函数用于 训练阶段解码器的输入预处理，将标签序列右移一位，插入起始 token，同时处理 -100 标记为 pad_token_id，\n# 是 seq2seq 框架中训练时标准做法。\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    # 创建与 input_ids 相同形状的零张量（保持 dtype 和 device），用于存储右移后的结果\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    # 将原 input_ids 除最后一个 token 外，右移一位，填入新张量中，从第1列开始\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    # 第0列设置为解码器起始 token（通常是 decoder_start_token_id，如 BART 中的 <s>）\n    shifted_input_ids[:, 0] = decoder_start_token_id\n    # 检查是否提供了 pad_token_id（因为后续要替换 -100）\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # 将所有位置中原本是 -100 的标记（如 label ignore 标记）替换为 pad_token_id，保证模型输入合法\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n    return shifted_input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:38.486773Z","iopub.execute_input":"2025-05-26T17:03:38.487477Z","iopub.status.idle":"2025-05-26T17:03:38.492556Z","shell.execute_reply.started":"2025-05-26T17:03:38.487452Z","shell.execute_reply":"2025-05-26T17:03:38.491754Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 该模块实现了可学习的位置嵌入（positional embeddings），用于为输入序列中的每个位置编码唯一的位置信息。\n# 这种方式区别于固定的正弦位置编码，能通过训练自动优化位置向量，提升模型效果。\n# 该类实现了Bart 模型中的可学习位置编码，并且针对 padding token 做了特殊偏移处理，支持训练和增量推理时的正确位置索引计算。\nclass BartLearnedPositionalEmbedding(nn.Embedding):\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        # Bart 特殊设计，若存在 padding_idx，则将所有位置编号 +2，且扩展嵌入表大小，防止冲突\n        self.offset = 2\n        super().__init__(num_embeddings + self.offset, embedding_dim)\n    # input_ids: [batch_size, seq_len]，输入token的ID序列（不直接用作嵌入索引，仅用来推断形状）\n    # past_key_values_length: int，解码器中缓存的过去序列长度，默认0，用于支持增量推理时的位置调整\n    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n        bsz, seq_len = input_ids.shape[:2] # batch_size 和序列长度\n        # 生成位置索引张量，范围连续支持缓存长度\n        positions = torch.arange(\n            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n        ).expand(bsz, -1)\n        # 返回对应位置的可学习嵌入，索引加上 offset 避免和 padding 冲突\n        return super().forward(positions + self.offset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:38.936290Z","iopub.execute_input":"2025-05-26T17:03:38.936869Z","iopub.status.idle":"2025-05-26T17:03:38.942810Z","shell.execute_reply.started":"2025-05-26T17:03:38.936848Z","shell.execute_reply":"2025-05-26T17:03:38.942044Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"torch.arange(0,8, dtype=torch.long).expand(2, -1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:42.023811Z","iopub.execute_input":"2025-05-26T17:03:42.024460Z","iopub.status.idle":"2025-05-26T17:03:42.041843Z","shell.execute_reply.started":"2025-05-26T17:03:42.024436Z","shell.execute_reply":"2025-05-26T17:03:42.041044Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n        [0, 1, 2, 3, 4, 5, 6, 7]])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"torch.arange(0,8, dtype=torch.long).expand(2, -1)+2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:42.227133Z","iopub.execute_input":"2025-05-26T17:03:42.227345Z","iopub.status.idle":"2025-05-26T17:03:42.233285Z","shell.execute_reply.started":"2025-05-26T17:03:42.227330Z","shell.execute_reply":"2025-05-26T17:03:42.232555Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[2, 3, 4, 5, 6, 7, 8, 9],\n        [2, 3, 4, 5, 6, 7, 8, 9]])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Bart中加2的原因主要是为了避免位置索引与特殊token的ID冲突，具体原因如下：\n# 特殊token预留\n# Bart的词表中，padding_token_id通常是0，decoder_start_token_id等特殊token也占用较低的ID（比如1）。\n# 为了不让位置编码的索引与这些特殊token的ID混淆，位置编码索引整体往后偏移2。\n# 这是Bart实现中的一个“小技巧”，保证padding token的embedding不受位置编码影响，同时简化模型处理逻辑。\n# “+2” 是为了保证位置索引与 padding 和特殊token ID 不冲突，避免混淆，保持编码唯一性。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 继承自 nn.Embedding，重写了 forward 函数，在获取词向量后乘以一个缩放因子 embed_scale。\n# 这个缩放操作通常用于平衡词嵌入的数值范围，有助于模型训练稳定性。\nclass BartScaledWordEmbedding(nn.Embedding):\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n        super().__init__(num_embeddings, embedding_dim, padding_idx)  # 调用父类初始化，设置词表大小、维度和padding索引\n        self.embed_scale = embed_scale  # 保存缩放因子，默认1.0，即不缩放\n\n    def forward(self, input_ids: torch.Tensor):\n        # 调用父类forward获取对应input_ids的词嵌入\n        # 之后乘以缩放因子 embed_scale，实现嵌入向量的缩放\n        return super().forward(input_ids) * self.embed_scale","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:46.167871Z","iopub.execute_input":"2025-05-26T17:03:46.168187Z","iopub.status.idle":"2025-05-26T17:03:46.173405Z","shell.execute_reply.started":"2025-05-26T17:03:46.168166Z","shell.execute_reply":"2025-05-26T17:03:46.172794Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"aa=torch.randn((2*3,5,6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:48.835955Z","iopub.execute_input":"2025-05-26T17:03:48.836634Z","iopub.status.idle":"2025-05-26T17:03:48.843379Z","shell.execute_reply.started":"2025-05-26T17:03:48.836613Z","shell.execute_reply":"2025-05-26T17:03:48.842787Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"aa.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:49.029400Z","iopub.execute_input":"2025-05-26T17:03:49.030119Z","iopub.status.idle":"2025-05-26T17:03:49.034668Z","shell.execute_reply.started":"2025-05-26T17:03:49.030098Z","shell.execute_reply":"2025-05-26T17:03:49.033891Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([6, 5, 6])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"bb=aa.view(2,3,5,6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:51.171706Z","iopub.execute_input":"2025-05-26T17:03:51.171991Z","iopub.status.idle":"2025-05-26T17:03:51.175917Z","shell.execute_reply.started":"2025-05-26T17:03:51.171971Z","shell.execute_reply":"2025-05-26T17:03:51.175206Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"bb.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:51.364326Z","iopub.execute_input":"2025-05-26T17:03:51.364991Z","iopub.status.idle":"2025-05-26T17:03:51.369964Z","shell.execute_reply.started":"2025-05-26T17:03:51.364970Z","shell.execute_reply":"2025-05-26T17:03:51.369152Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 3, 5, 6])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"aa.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:51.532675Z","iopub.execute_input":"2025-05-26T17:03:51.533217Z","iopub.status.idle":"2025-05-26T17:03:51.537228Z","shell.execute_reply.started":"2025-05-26T17:03:51.533199Z","shell.execute_reply":"2025-05-26T17:03:51.536711Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"torch.Size([6, 5, 6])"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# 来自“Attention Is All You Need”论文的多头注意力\nclass BartAttention(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        is_causal: bool = False,\n        config: Optional[BartConfig] = None,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim  # 嵌入维度\n        self.num_heads = num_heads # 注意力头数量\n        self.dropout = dropout # dropout 概率\n        self.head_dim = embed_dim // num_heads # 每个头的维度\n        self.config = config  # 配置参数\n        # 检查 embed_dim 是否能被 num_heads 整除\n        if (self.head_dim * num_heads) != self.embed_dim:\n            raise ValueError(\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {num_heads}).\"\n            )\n        self.scaling = self.head_dim**-0.5  # 缩放因子，防止点积值过大\n        self.is_decoder = is_decoder # 是否解码器模式\n        self.is_causal = is_causal # 是否因果遮蔽（单向）\n         # 定义线性层，分别用于生成 key, value, query 和最终输出\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n    # 重塑张量形状为 (batch_size, num_heads, seq_len, head_dim)，方便多头并行计算\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # 是否是交叉注意力（decoder中用encoder的key/value）key_value_states应该是编码器的输出\n        is_cross_attention = key_value_states is not None\n        # batch_size, 目标序列长度\n        bsz, tgt_len, _ = hidden_states.size()\n        # 计算 query 并乘缩放因子\n        query_states = self.q_proj(hidden_states) * self.scaling\n         # 根据不同情况计算 key 和 value：\n        # 1. 交叉注意力且已有缓存，复用缓存key/value\n        if (\n            is_cross_attention\n            and past_key_value is not None\n            and past_key_value[0].shape[2] == key_value_states.shape[1]\n        ):\n            # 重用k，v，cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention: # 2. 交叉注意力，无缓存，计算 key/value\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        # 3. 非交叉注意力，且有缓存，复用并拼接缓存key/value（用于decoder的自注意力缓存）\n        elif past_key_value is not None:\n            # 会先计算当前传入的k,v之后和缓存的k,v在序列维度合并\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n        else: # 4. 普通自注意力，无缓存，直接计算 key/value\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n         # 解码器情况下，保存当前 key/value 用于下一步缓存\n        # 解码器情况有两种,一种是交叉注意力,一种是有自注意力\n        if self.is_decoder: \n            # 如果是交叉注意力，则保存一个包含所有交叉注意力的键（key）和值（value）状态的元组（Tuple(torch.Tensor, torch.Tensor)）。\n            # 后续调用交叉注意力层时，可以重用所有已保存的交叉注意力键/值状态（对应第一个 if 分支）。\n            # 如果是单向自注意力（解码器），则保存所有之前解码器的键/值状态的元组。后续调用单向自注意力时，可以将之前保存的键/值状态与当前投影的键/值状态拼接（对应第三个 elif 分支）。\n            # 如果是编码器的双向自注意力，past_key_value 始终为 None。\n            past_key_value = (key_states, value_states)\n        # 将 query/key/value reshape成 (batch_size * num_heads, seq_len, head_dim) 方便并行计算\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states = key_states.reshape(*proj_shape)\n        value_states = value_states.reshape(*proj_shape)\n        src_len = key_states.size(1)  # 源序列长度\n        # torch.bmm 是批量矩阵乘法（batch matrix multiplication）。\n        # query_states 和 key_states.transpose(1, 2) 都是三维张量，形状一般是 (batch_size * num_heads, seq_len, \n        # head_dim) 和 (batch_size * num_heads, head_dim, seq_len)，bmm 会对每个 batch（这里包含多头的合并）做矩\n        # 阵乘法，得到形状 (batch_size * num_heads, seq_len, seq_len) 的注意力权重矩阵。\n        # torch.matmul：支持广播，可以对高维张量按最后两维做矩阵乘法。\n        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))  # 计算注意力权重 Q*K^T\n        # 检查权重形状是否正确\n        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n        # 如果有注意力mask，加上mask（形状为 (batch, 1, tgt_len, src_len)）\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n                )\n            # 加上mask\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n        # 表示query中每个token对key中token的关注分数\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1) # 归一化权重为概率分布\n        # 如果有头掩码，乘以对应的掩码值，屏蔽某些头 先对权重矩阵归一化,之后乘以头掩码\n        if layer_head_mask is not None:\n            if layer_head_mask.size() != (self.num_heads,):\n                raise ValueError(\n                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n                    f\" {layer_head_mask.size()}\"\n                )\n            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n        # 是否返回注意力权重  attn_weights_reshaped用于输出权重矩阵,attn_weights用于之后的计算\n        if output_attentions:\n            # 这句 不会改变原来的 attn_weights 对象，而是：返回一个新的张量对象（新变量 attn_weights_reshaped 指向它）。\n            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            # 这个没必要,因为attn_weights指向的那个对象压根没变化\n            # attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n        else: # 这个是output_attentions=False的情况\n            attn_weights_reshaped = None\n        # Dropout 正则化\n        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n        # 计算加权和，得到注意力输出\n        attn_output = torch.bmm(attn_probs, value_states)\n        # 检查输出形状\n        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        # 恢复成 (batch, num_heads, tgt_len, head_dim)\n        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1, 2) # 转换成 (batch, tgt_len, num_heads, head_dim)\n\n        # 合并多头维度，恢复到 embed_dim\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n        # 通过输出线性层，得到最终输出\n        attn_output = self.out_proj(attn_output)\n        # 返回：输出张量，注意力权重（可选），缓存的key/value（用于解码器）\n        return attn_output, attn_weights_reshaped, past_key_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:03:55.607665Z","iopub.execute_input":"2025-05-26T17:03:55.607945Z","iopub.status.idle":"2025-05-26T17:03:55.628335Z","shell.execute_reply.started":"2025-05-26T17:03:55.607925Z","shell.execute_reply":"2025-05-26T17:03:55.627699Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# 因为 torch.bmm（用于乘 value_states）只能接受形状为 (bsz * num_heads, tgt_len, src_len)，所以必须在乘法前变成\n# 这种扁平化的 shape。中间这一段只是为了 输出需要 reshape 一次保存副本，但仍需要保持原 shape 做后续计算。\n# 所以虽然看似重复，实则是为了分离：\n# 计算用的 attn_weights\n# 输出用的 attn_weights_reshaped","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@lru_cache()\ndef is_flash_attn_greater_or_equal_2_10():\n    if not _is_package_available(\"flash_attn\"):\n        return False\n    return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers.utils import is_flash_attn_greater_or_equal_2_10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:03.237659Z","iopub.execute_input":"2025-05-26T17:04:03.237940Z","iopub.status.idle":"2025-05-26T17:04:03.241715Z","shell.execute_reply.started":"2025-05-26T17:04:03.237921Z","shell.execute_reply":"2025-05-26T17:04:03.240854Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"is_flash_attn_greater_or_equal_2_10()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:03.446584Z","iopub.execute_input":"2025-05-26T17:04:03.446784Z","iopub.status.idle":"2025-05-26T17:04:03.453764Z","shell.execute_reply.started":"2025-05-26T17:04:03.446769Z","shell.execute_reply":"2025-05-26T17:04:03.453106Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"def flash_attn_supports_top_left_mask():\n    if is_flash_attn_2_available():\n        # top-left mask is used in package `flash-attn` with version lower than 2.1.0\n        return not is_flash_attn_greater_or_equal_2_10()\n\n    if is_torch_npu_available():\n        # down-right mask is used on Ascend NPU by default, set env `NPU_FA2_SPARSE_MODE=2` to activate top-left mask.\n        from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n\n        return is_npu_fa2_top_left_aligned_causal_mask()\n\n    return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"flash_attn_supports_top_left_mask()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:07.826336Z","iopub.execute_input":"2025-05-26T17:04:07.827103Z","iopub.status.idle":"2025-05-26T17:04:07.834592Z","shell.execute_reply.started":"2025-05-26T17:04:07.827073Z","shell.execute_reply":"2025-05-26T17:04:07.833938Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"bb.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:08.028776Z","iopub.execute_input":"2025-05-26T17:04:08.028997Z","iopub.status.idle":"2025-05-26T17:04:08.033875Z","shell.execute_reply.started":"2025-05-26T17:04:08.028979Z","shell.execute_reply":"2025-05-26T17:04:08.033221Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 3, 5, 6])"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"cc2 = bb.transpose(1, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:10.626491Z","iopub.execute_input":"2025-05-26T17:04:10.627165Z","iopub.status.idle":"2025-05-26T17:04:10.630912Z","shell.execute_reply.started":"2025-05-26T17:04:10.627141Z","shell.execute_reply":"2025-05-26T17:04:10.630120Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(bb.shape,cc2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:10.805582Z","iopub.execute_input":"2025-05-26T17:04:10.805796Z","iopub.status.idle":"2025-05-26T17:04:10.809765Z","shell.execute_reply.started":"2025-05-26T17:04:10.805779Z","shell.execute_reply":"2025-05-26T17:04:10.809040Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 3, 5, 6]) torch.Size([2, 5, 3, 6])\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"torch.get_autocast_gpu_dtype()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:13.259710Z","iopub.execute_input":"2025-05-26T17:04:13.259956Z","iopub.status.idle":"2025-05-26T17:04:13.265478Z","shell.execute_reply.started":"2025-05-26T17:04:13.259938Z","shell.execute_reply":"2025-05-26T17:04:13.264885Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3134095016.py:1: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /pytorch/torch/csrc/autograd/init.cpp:787.)\n  torch.get_autocast_gpu_dtype()\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"torch.float16"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"torch.is_autocast_enabled() # 是否支持混合精度","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:13.446571Z","iopub.execute_input":"2025-05-26T17:04:13.446794Z","iopub.status.idle":"2025-05-26T17:04:13.451701Z","shell.execute_reply.started":"2025-05-26T17:04:13.446777Z","shell.execute_reply":"2025-05-26T17:04:13.451057Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# 用于 BART 的 Flash Attention 模块。\n# - 继承自 BartAttention，因此保留了权重结构（如 q_proj/k_proj/v_proj/out_proj）不变；\n# - 仅重写 forward 方法以调用 FlashAttention 的公开 API；\n# - 同时处理 FlashAttention 对不同版本（如 2.0 与 2.1）掩码方式差异的问题；\n# - 支持自注意力与交叉注意力，同时能复用 past_key_value 以支持增量推理。\nclass BartFlashAttention2(BartAttention):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # 处理 FlashAttention <2.1 所使用的“左上角对齐”的 causal mask（早期实现方式）\n        # 从 FlashAttention 2.1 开始，掩码行为改为默认支持“右下角对齐”，更符合常规因果 Mask。\n        # 该标志用于控制 forward 中是否需要兼容老版本行为。\n        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n    # 重写了父类方法 将投影后的 hidden_states reshape 成 (bsz, seq_len, num_heads, head_dim)\n    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # flash-attn 不支持输出注意力权重（因为它不显式计算注意力矩阵），因此禁止该模式\n        if output_attentions:\n            raise ValueError(\"BartFlashAttention2 attention does not support output_attentions\")\n\n        # 是否是 cross-attention key_value_states应该是编码器的输出\n        is_cross_attention = key_value_states is not None\n        bsz, q_len, _ = hidden_states.size() # 目标序列批次和序列长度\n        # 计算 query 投影并 reshape 成 (bsz, q_len, num_heads, head_dim)\n        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n        # 根据是否 cross-attention 与是否提供 past_key_value 分支处理 key/value 的获取\n        # 如果是交叉注意力并且past_key_value存在\n        if (\n            is_cross_attention\n            and past_key_value is not None\n            and past_key_value[0].shape[2] == key_value_states.shape[1]\n        ):\n            # 直接用past_key_value,换轴为(b,s,h,hd)\n            key_states = past_key_value[0].transpose(1, 2)\n            value_states = past_key_value[1].transpose(1, 2)\n        elif is_cross_attention: # 是交叉注意力,但是没缓存\n            # 根据编码器的输出重新计算key,vlue\n            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None: # 是自注意力并且存在缓存\n            # 先计算当前传入的单个token的key和value\n            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n            # 之后在序列维度合并 \n            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n        else: # 是自注意力,并且没有缓存\n            # 根据hidden_states重新计算\n            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n        # 如果是 decoder，保存 k/v 以供后续时间步或交叉注意力使用\n        if self.is_decoder:\n            # 如果是交叉注意力，则保存一个包含所有交叉注意力的键（key）和值（value）状态的元组（Tuple(torch.Tensor, torch.Tensor)）。\n            # 后续调用交叉注意力层时，可以重用所有已保存的交叉注意力键/值状态（对应第一个 if 分支）。\n            # 如果是单向自注意力（解码器），则保存所有之前解码器的键/值状态的元组。后续调用单向自注意力时，可以将之前保存的键/值状态\n            # 与当前投影的键/值状态拼接（对应第三个 elif 分支）。\n            # 如果是编码器的双向自注意力，past_key_value 始终为 None。\n            # key_states.transpose(1, 2) 本身不修改原 tensor 的内存布局（它是 创建一个新的 view）\n            # past_key_value 是一个新变量，指向新的 transpose 后的 tensor 对象\n            # 原始的 key_states 并没有被修改，因为它本身没有被就地 (in-place) 改动\n            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n        # key_states.shape[-2] 是 num_heads，不应该作为 kv_seq_len\n        # 应该写成 key_states.shape[1] 才对（因为 shape 是 (bsz, seq_len, num_heads, head_dim)）\n        # kv_seq_len = key_states.shape[-2] \n        # 这段代码用意可能是想获得完整的 kv 序列长度，\n        # 但由于代码中 key_states 已经包含了缓存的拼接，直接加上缓存长度会造成重复累加，\n        # 因此，这两行代码要么需要重写（改成直接用 kv_seq_len = key_states.shape[-2]），\n        # 要么就是冗余、无意义的，可以去掉。\n        # kv_seq_len = key_states.shape[1]\n        # if past_key_value is not None:\n        #     kv_seq_len += past_key_value[0].shape[-2]\n\n        # 在 PEFT（参数高效微调）中，通常会将 LayerNorm 层转换为 float32 类型，以提高训练的稳定性。\n        # 因此，输入的 hidden_states 会被悄悄地转换为 float32 类型。\n        # 所以我们需要将它们重新转换回正确的 dtype，以确保一切按预期工作。\n        # 但这可能会降低训练和推理的速度，因此不建议将 LayerNorm 转换为 float32。\n        # （LlamaRMSNorm 处理得当，不会有这个问题）\n        # 检查输入数据类型，如果是 float32，但推理处于 autocast 模式或权重是 float16，则回退 cast\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            # torch.is_autocast_enabled() 用来判断当前上下文环境中 自动混合精度（autocast） 是否处于启用状态。\n            # 自动混合精度是 PyTorch 用于加速训练和推理的一种机制，它会自动在适当的操作中使用半精度（float16）计算，从而提升效率并降低显存占用。\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype() # 设置目标类型\n            # 处理模型量化的情况\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n            # 转换q,k,v的类型为target_dtype\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n        # 调用_flash_attention_forward函数计算注意力输出\n        attn_output = _flash_attention_forward(\n            query_states,  # 查询张量 (q)\n            key_states,  # 键张量 (k)\n            value_states,  # 值张量 (v)\n            attention_mask,  # 注意力掩码，屏蔽不应关注的部分\n            q_len,  # 查询序列长度\n            dropout=self.dropout if self.training else 0.0, # 训练时应用dropout，推理时不使用\n            is_causal=self.is_causal,   # 是否使用因果掩码（未来信息屏蔽）\n            use_top_left_mask=self._flash_attn_uses_top_left_mask, # 是否启用特殊的Top-Left掩码\n        )\n        # 将注意力输出张量reshape为(batch_size, query_length, 隐藏层维度)\n        attn_output = attn_output.reshape(bsz, q_len, -1)\n        attn_output = self.out_proj(attn_output) # 经过输出投影层，映射回隐藏层维度\n        # 如果不需要返回注意力权重，则置为None，节省内存\n        if not output_attentions:\n            attn_weights = None\n        # 返回注意力输出，注意力权重（可能为None），以及缓存的past_key_value（用于加速推理）\n        return attn_output, attn_weights, past_key_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:17.417207Z","iopub.execute_input":"2025-05-26T17:04:17.417883Z","iopub.status.idle":"2025-05-26T17:04:17.431090Z","shell.execute_reply.started":"2025-05-26T17:04:17.417859Z","shell.execute_reply":"2025-05-26T17:04:17.430417Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def _flash_attention_forward(\n    query_states: torch.Tensor,\n    key_states: torch.Tensor,\n    value_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    query_length: int,\n    is_causal: bool,\n    dropout: float = 0.0,\n    position_ids: Optional[torch.Tensor] = None,\n    softmax_scale: Optional[float] = None,\n    sliding_window: Optional[int] = None,\n    use_top_left_mask: bool = False,\n    softcap: Optional[float] = None,\n    deterministic: Optional[bool] = None,\n    cu_seq_lens_q: Optional[torch.LongTensor] = None,\n    cu_seq_lens_k: Optional[torch.LongTensor] = None,\n    max_length_q: Optional[int] = None,\n    max_length_k: Optional[int] = None,\n    target_dtype: Optional[torch.dtype] = None,\n    **kwargs,\n):\n    \"\"\"\n    Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n    first unpad the input, then computes the attention scores and pad the final attention scores.\n\n    Args:\n        query_states (`torch.Tensor`):\n            Input query states to be passed to Flash Attention API\n        key_states (`torch.Tensor`):\n            Input key states to be passed to Flash Attention API\n        value_states (`torch.Tensor`):\n            Input value states to be passed to Flash Attention API\n        attention_mask (`torch.Tensor`, *optional*):\n            The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n            position of padding tokens and 1 for the position of non-padding tokens.\n        dropout (`float`):\n            Attention dropout\n        softmax_scale (`float`, *optional*):\n            The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        use_top_left_mask (`bool`, defaults to `False`):\n            flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference.\n        softcap (`float`, *optional*):\n            Softcap for the attention logits, used e.g. in gemma2.\n        deterministic (`bool`, *optional*):\n            Determines if the deterministic option introduced in flash_attn>=2.4.1 is enabled.\n    \"\"\"\n    if not use_top_left_mask:\n        causal = is_causal\n    else:\n        # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1.\n        causal = is_causal and query_length != 1\n\n    # Assuming 4D tensors, key_states.shape[1] is the key/value sequence length (source length).\n    use_sliding_windows = (\n        _flash_supports_window_size and sliding_window is not None and key_states.shape[1] > sliding_window\n    )\n    flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sliding_windows else {}\n\n    if flash_241:\n        if deterministic is None:\n            deterministic = deterministic_g\n        flash_kwargs[\"deterministic\"] = deterministic\n\n    if softcap is not None:\n        flash_kwargs[\"softcap\"] = softcap\n\n    # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op\n    query_states, key_states, value_states = fa_peft_integration_check(\n        query_states, key_states, value_states, target_dtype\n    )\n\n    # Contains at least one padding token in the sequence\n    if attention_mask is not None:\n        batch_size = query_states.shape[0]\n        query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = _upad_input(\n            query_states, key_states, value_states, attention_mask, query_length\n        )\n        cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n        max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n        attn_output_unpad = flash_attn_varlen_func(\n            query_states,\n            key_states,\n            value_states,\n            cu_seqlens_q=cu_seqlens_q,\n            cu_seqlens_k=cu_seqlens_k,\n            max_seqlen_q=max_seqlen_in_batch_q,\n            max_seqlen_k=max_seqlen_in_batch_k,\n            dropout_p=dropout,\n            softmax_scale=softmax_scale,\n            causal=causal,\n            **flash_kwargs,\n        )\n        attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n\n    # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n    # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n    # Use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n    elif position_ids is not None and (\n        max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\n    ):\n        batch_size = query_states.size(0)\n\n        if cu_seq_lens_q is None or cu_seq_lens_k is None:\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n                prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)\n            )\n\n            cu_seq_lens_q, cu_seq_lens_k = cu_seq_lens\n            max_length_q, max_length_k = max_seq_lens\n\n        else:\n            query_states = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n            key_states = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n            value_states = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n\n        attn_output = flash_attn_varlen_func(\n            query_states,\n            key_states,\n            value_states,\n            cu_seqlens_q=cu_seq_lens_q,\n            cu_seqlens_k=cu_seq_lens_k,\n            max_seqlen_q=max_length_q,\n            max_seqlen_k=max_length_k,\n            dropout_p=dropout,\n            softmax_scale=softmax_scale,\n            causal=causal,\n            **flash_kwargs,\n        )\n\n        attn_output = attn_output.view(batch_size, -1, attn_output.size(-2), attn_output.size(-1))\n\n    else:\n        attn_output = flash_attn_func(\n            query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal, **flash_kwargs\n        )\n\n    return attn_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:39.300878Z","iopub.execute_input":"2025-05-26T17:04:39.301596Z","iopub.status.idle":"2025-05-26T17:04:39.314257Z","shell.execute_reply.started":"2025-05-26T17:04:39.301572Z","shell.execute_reply":"2025-05-26T17:04:39.313574Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class BartSdpaAttention(BartAttention):\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # 输入维度: [batch_size, seq_len, hidden_dim]\n        # 若启用输出注意力或head mask，因SDPA不支持这些功能，回退到手动实现\n        if output_attentions or layer_head_mask is not None:\n            logger.warning_once(\n                \"BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward( # 回退到BartAttention的实现\n                hidden_states,\n                key_value_states=key_value_states,\n                past_key_value=past_key_value,\n                attention_mask=attention_mask,\n                layer_head_mask=layer_head_mask,\n                output_attentions=output_attentions,\n            )\n\n        # 判断是否是交叉注意力（key_value_states是encoder输出）\n        is_cross_attention = key_value_states is not None\n\n        bsz, tgt_len, _ = hidden_states.size() # 获取目标的批次和序列长度\n\n        # 计算query投影\n        query_states = self.q_proj(hidden_states)\n        # 交叉注意力,并且有缓存k,v\n        if (\n            is_cross_attention\n            and past_key_value is not None\n            and past_key_value[0].shape[2] == key_value_states.shape[1]\n        ):\n            # 重用缓存中的key/value\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # 交叉注意力,但是没有past_k，v,这时投影变形成(b,h,s,hd)形式\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # decoder自注意力,有缓存\n            # 先计算当前输入中的目标序列的k,v表示（b,h,s,hd）\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            # 之后再序列轴合并\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n        else:\n            # 如果不是交叉注意力,并且没有缓存 (b,h,s,hd)\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n        # 如果是decoder 则缓存k,v 否则如果是单编码器,past_key_value =None\n        if self.is_decoder:\n            # 保存当前key/value以供下一步使用（缓存）\n            past_key_value = (key_states, value_states)\n        # 将query reshape 成 [batch_size, num_heads, tgt_len, head_dim]\n        query_states = self._shape(query_states, tgt_len, bsz)\n\n        # 我们通过这个 is_causal 条件语句来分发调用 SDPA（Scaled Dot Product Attention）中的 Flash Attention 或 Efficient \n        #  内核，而不是在 SDPA 内部使用内联条件判断，这是为了兼容 torch.compile 的动态形状和完整图优化。内联条件会阻碍动态形状编译。\n        #  tgt_len > 1 是必要的，这是为了与 AttentionMaskConverter.to_causal_4d 的行为保持一致——当 tgt_len == 1 时，该方\n        #  法不会创建因果掩码。\n        # 如果is_causal标记是True,并且没有传入填充掩码,并且当前输入序列长度大于1(就是没有缓存k,v)\n        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n\n        # 使用 PyTorch 原生 SDPA（支持 FlashAttention / Memory Efficient Attention） 缩放点积注意力机制\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=attention_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n         # 校验输出形状\n        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        # 变换维度：[batch, heads, seq_len, head_dim] -> [batch, seq_len, heads, head_dim]\n        attn_output = attn_output.transpose(1, 2)\n         # 合并多头结果为 [batch, seq_len, embed_dim]\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n         # 输出线性映射\n        attn_output = self.out_proj(attn_output)\n         # 返回注意力输出、注意力权重（此处为None）、缓存的key/value\n        return attn_output, None, past_key_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:47.150023Z","iopub.execute_input":"2025-05-26T17:04:47.150286Z","iopub.status.idle":"2025-05-26T17:04:47.161715Z","shell.execute_reply.started":"2025-05-26T17:04:47.150266Z","shell.execute_reply":"2025-05-26T17:04:47.160957Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# 我们通过这个 is_causal 条件语句来分发调用 SDPA（Scaled Dot Product Attention）中的 Flash Attention 或 Efficient \n# 内核，而不是在 SDPA 内部使用内联条件判断，这是为了兼容 torch.compile 的动态形状和完整图优化。内联条件会阻碍动态形状编译。\n# tgt_len > 1 是必要的，这是为了与 AttentionMaskConverter.to_causal_4d 的行为保持一致——当 tgt_len == 1 时，该方法不会创建因果掩码。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BART各种注意力类\nBART_ATTENTION_CLASSES = {\n    \"eager\": BartAttention,\n    \"sdpa\": BartSdpaAttention,\n    \"flash_attention_2\": BartFlashAttention2,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:51.423375Z","iopub.execute_input":"2025-05-26T17:04:51.423715Z","iopub.status.idle":"2025-05-26T17:04:51.427749Z","shell.execute_reply.started":"2025-05-26T17:04:51.423691Z","shell.execute_reply":"2025-05-26T17:04:51.426940Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from transformers import BartConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:53.246729Z","iopub.execute_input":"2025-05-26T17:04:53.247227Z","iopub.status.idle":"2025-05-26T17:04:53.250884Z","shell.execute_reply.started":"2025-05-26T17:04:53.247204Z","shell.execute_reply":"2025-05-26T17:04:53.250187Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"config=BartConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:53.421833Z","iopub.execute_input":"2025-05-26T17:04:53.422209Z","iopub.status.idle":"2025-05-26T17:04:53.425511Z","shell.execute_reply.started":"2025-05-26T17:04:53.422192Z","shell.execute_reply":"2025-05-26T17:04:53.424752Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"config._attn_implementation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:55.430483Z","iopub.execute_input":"2025-05-26T17:04:55.431065Z","iopub.status.idle":"2025-05-26T17:04:55.435411Z","shell.execute_reply.started":"2025-05-26T17:04:55.431041Z","shell.execute_reply":"2025-05-26T17:04:55.434894Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'eager'"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"class BartEncoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model # 嵌入维度（模型宽度）\n        # 初始化自注意力层（支持 Flash Attention / SDPA 等多种实现）\n        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](\n            embed_dim=self.embed_dim,\n            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n            config=config,\n        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim) # 自注意力后的 LayerNorm\n        self.dropout = config.dropout  # 残差连接后的 dropout 概率\n        self.activation_fn = ACT2FN[config.activation_function] # 非线性激活函数（如 GELU、ReLU）\n        self.activation_dropout = config.activation_dropout # FFN 层激活后的 dropout 概率\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim) # 前馈神经网络第1层：升维\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)  # 前馈神经网络第2层：降维\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim) # FFN 后的 LayerNorm\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,  # 输入隐藏状态：(batch, seq_len, embed_dim)\n        attention_mask: torch.FloatTensor, # 注意力掩码：(batch, 1, tgt_len, src_len)，用于屏蔽 padding\n        layer_head_mask: torch.FloatTensor,  # 指定当前层中哪些注意力头被禁用\n        output_attentions: Optional[bool] = False,  # 是否返回注意力矩阵\n    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n        # ----------- 自注意力子层（含残差和 LayerNorm） ----------- #\n        residual = hidden_states  # 残差连接的输入\n        hidden_states, attn_weights, _ = self.self_attn(  # 执行自注意力\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        # dropout\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states # 残差连接\n        hidden_states = self.self_attn_layer_norm(hidden_states) # LayerNorm\n         # ----------- 前馈网络子层（含残差和 LayerNorm） ----------- #\n        residual = hidden_states  # 残差连接的输入\n        hidden_states = self.activation_fn(self.fc1(hidden_states)) # 第1层线性+激活\n        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training) # dropout\n        hidden_states = self.fc2(hidden_states) # 第2层线性\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training) # dropout\n        hidden_states = residual + hidden_states # 残差连接\n        hidden_states = self.final_layer_norm(hidden_states) # LayerNorm\n        # ----------- 数值稳定性防护（float16时） ----------- #\n        if hidden_states.dtype == torch.float16 and (\n            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n        ):\n            # 避免溢出：截断数值以防出现无穷大/NaN\n            # clamp(input, min=None, max=None, *, out=None) -> Tensor\n            # 将 input 中的所有元素限制在区间 [min, max] 之间\n            # torch.clamp 和 numpy.clip 在功能上是等价的\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n        outputs = (hidden_states,)   # 主输出是编码后的 hidden states\n\n        if output_attentions:\n            outputs += (attn_weights,)  # 如果需要，返回注意力权重\n\n        return outputs # 返回：编码后的 hidden states（可选 attention 权重）","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:04:55.624683Z","iopub.execute_input":"2025-05-26T17:04:55.624919Z","iopub.status.idle":"2025-05-26T17:04:55.634635Z","shell.execute_reply.started":"2025-05-26T17:04:55.624901Z","shell.execute_reply":"2025-05-26T17:04:55.633811Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:00.265062Z","iopub.execute_input":"2025-05-26T17:05:00.265363Z","iopub.status.idle":"2025-05-26T17:05:00.269340Z","shell.execute_reply.started":"2025-05-26T17:05:00.265342Z","shell.execute_reply":"2025-05-26T17:05:00.268577Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"torch.isinf(torch.from_numpy(np.array([3,6,1000000]))).any()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:00.446846Z","iopub.execute_input":"2025-05-26T17:05:00.447053Z","iopub.status.idle":"2025-05-26T17:05:00.457382Z","shell.execute_reply.started":"2025-05-26T17:05:00.447038Z","shell.execute_reply":"2025-05-26T17:05:00.456720Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor(False)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"torch.isnan(torch.from_numpy(np.array([3,6,1000000]))).any()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:00.626582Z","iopub.execute_input":"2025-05-26T17:05:00.627103Z","iopub.status.idle":"2025-05-26T17:05:00.633017Z","shell.execute_reply.started":"2025-05-26T17:05:00.627081Z","shell.execute_reply":"2025-05-26T17:05:00.632429Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"tensor(False)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"torch.from_numpy(np.array([3.,6.,1000000.])).dtype","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:03.107484Z","iopub.execute_input":"2025-05-26T17:05:03.108307Z","iopub.status.idle":"2025-05-26T17:05:03.113499Z","shell.execute_reply.started":"2025-05-26T17:05:03.108273Z","shell.execute_reply":"2025-05-26T17:05:03.112870Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"torch.float64"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"torch.finfo(torch.float64).max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:03.299551Z","iopub.execute_input":"2025-05-26T17:05:03.299750Z","iopub.status.idle":"2025-05-26T17:05:03.304287Z","shell.execute_reply.started":"2025-05-26T17:05:03.299735Z","shell.execute_reply":"2025-05-26T17:05:03.303793Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"1.7976931348623157e+308"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# 封装就是层层嵌套调用,这里会用到之前定义的作为子模块\nclass BartDecoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model # 隐藏层维度（即embedding维度）\n        # Decoder中的自注意力机制，支持因果遮盖（即只能看到过去）\n        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True, # 默认是False,这里设定是解码器\n            is_causal=True, # 设定用因果掩码\n            config=config,\n        )\n        self.dropout = config.dropout # dropout比率\n        self.activation_fn = ACT2FN[config.activation_function] # 非线性激活函数\n        self.activation_dropout = config.activation_dropout # FFN层的dropout\n        # 自注意力之后的LayerNorm\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        # 编码器-解码器交叉注意力\n        self.encoder_attn = BART_ATTENTION_CLASSES[config._attn_implementation](\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True, # 这里是解码器的交叉注意力\n            config=config,\n        )\n        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)  # 交叉注意力后的LayerNorm\n        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)  # 前馈网络（第一层）：升维\n        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim) # 前馈网络（第二层）：降维回原始维度\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim) # 前馈模块后的最终LayerNorm\n    \n    # 执行Decoder Layer的前向传播，包含三大模块：\n    #     - 自注意力\n    #     - 交叉注意力\n    #     - 前馈神经网络\n    #     支持cache机制（past_key_value）用于加速生成。\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = True,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \n        residual = hidden_states  # 残差连接保存输入\n\n        # ========== 自注意力模块 ==========\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # 将当前 self-attn 缓存添加到 present_key_value 元组的 1,2 位置\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training) # dropout\n        hidden_states = residual + hidden_states  # 残差连接\n        hidden_states = self.self_attn_layer_norm(hidden_states) # 层标准化\n\n        # ========== 交叉注意力模块（如果有encoder输出） ==========\n        cross_attn_present_key_value = None # 交叉注意力缓存的k,v\n        cross_attn_weights = None\n        if encoder_hidden_states is not None: # 如果有编码器的输出,这时解码器就有交叉注意部分\n            residual = hidden_states # 残差部分\n\n            # cross_attn缓存的key/values元组位于present_key_value元组的3,4位置\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value, # 交叉注意机制缓存的k,v\n                output_attentions=output_attentions,\n            )\n            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n            hidden_states = residual + hidden_states # 残差连接\n            hidden_states = self.encoder_attn_layer_norm(hidden_states) # 层标准化\n\n            # 将 cross-attn 添加到 present_key_value 元组的 3,4 位置\n            present_key_value = present_key_value + cross_attn_present_key_value # 更新缓存\n\n        # ========== 前馈神经网络 ==========\n        residual = hidden_states # 残差\n        hidden_states = self.activation_fn(self.fc1(hidden_states)) # 非线性变换（升维）\n        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)  # 降维\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states # 残差连接\n        hidden_states = self.final_layer_norm(hidden_states) # 层标准化\n        # ========== 返回结果 ==========\n        outputs = (hidden_states,)\n        # 如果设定要输出注意力权重\n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n        # 如果use_cache为True,就加上缓存,返回的是元组形式\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:10.018003Z","iopub.execute_input":"2025-05-26T17:05:10.018325Z","iopub.status.idle":"2025-05-26T17:05:10.030931Z","shell.execute_reply.started":"2025-05-26T17:05:10.018305Z","shell.execute_reply":"2025-05-26T17:05:10.030110Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# 用于句子级别分类任务的分类头，常用于文本分类、情感分析等场景\nclass BartClassificationHead(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,  # 输入特征维度，通常为 encoder/decoder 输出的隐藏层维度\n        inner_dim: int, # 分类头中间层的隐藏维度\n        num_classes: int,  # 输出类别数\n        pooler_dropout: float, # dropout 概率，用于正则化\n     ):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim) # 全连接层：将输入映射到中间隐藏维度，提升表达能力\n        self.dropout = nn.Dropout(p=pooler_dropout) # Dropout：防止过拟合\n        self.out_proj = nn.Linear(inner_dim, num_classes) # 输出层：将隐藏表示映射为分类 logits\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # 通常输入为 [CLS] token 或池化后的句向量表示 (b,d)\n        # 第一次 Dropout：对输入进行正则化\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.dense(hidden_states) # 非线性变换前的线性层\n        hidden_states = torch.tanh(hidden_states) # 非线性激活函数 tanh：增强模型表达能力（与 BERT 的 pooler 相似）\n        hidden_states = self.dropout(hidden_states) # 第二次 Dropout：进一步正则化\n        hidden_states = self.out_proj(hidden_states)  # 输出层，得到分类 logits\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:07.968977Z","iopub.execute_input":"2025-05-26T17:05:07.969260Z","iopub.status.idle":"2025-05-26T17:05:07.974962Z","shell.execute_reply.started":"2025-05-26T17:05:07.969239Z","shell.execute_reply":"2025-05-26T17:05:07.974349Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class BartPreTrainedModel(PreTrainedModel):\n    config_class = BartConfig # 指定使用的配置类（即与该模型关联的 config 类）\n    base_model_prefix = \"model\" # 用于命名权重时的前缀，例如 model.encoder.xxx\n    supports_gradient_checkpointing = True # 是否支持梯度检查点（用于节省显存）\n    # 加载模型时忽略这两个键的未预期警告（通常这些键是非必要的版本标识）\n    # 某些模型权重文件中可能包含额外的 key，例如保存时模型代码中存在 encoder.version 和 decoder.version，\n    # 但当前加载时的模型类没有这两个字段。\n    # 如果不加这个字段，调用 model.load_state_dict() 时就会触发 \"Unexpected key(s) in state_dict\" 的警告或报错。\n    # 设置这个属性后，加载过程会自动跳过这些 key，避免报错。\n    _keys_to_ignore_on_load_unexpected = [\"encoder.version\", \"decoder.version\"]\n    # 指定哪些模块不应该在模型并行或保存时被拆分（用于加速器或分布式训练）\n    # ✅ 在 save_pretrained 时，会考虑哪些模块保持整体保存\n    # _no_split_modules 指的是静态结构的拆分保护，而不是权重分片。\n    # _no_split_modules 控制的是 结构上能不能把某些层拆分为子模块；\n    # 和 文件存储方式无关，也和 多 GPU 训练时的权重分布无关。\n    _no_split_modules = [r\"BartEncoderLayer\", r\"BartDecoderLayer\"]\n    # 指定在加载模型时跳过某些键的设备自动迁移（用于缓存 past_key_values）\n    # 当你用 from_pretrained() 加载权重到 GPU（例如 model.to(\"cuda\")）时，transformers 库\n    # 会遍历每个权重参数，决定是否迁移设备。如果名字中包含 past_key_values，就会 跳过这个步骤，避免无效迁移或报错。\n    _skip_keys_device_placement = \"past_key_values\"\n    # 指定是否支持 Flash Attention v2（高效注意力机制，需硬件支持）\n    _supports_flash_attn_2 = True\n    # 指定是否支持 PyTorch 的 SDPA（Scaled Dot-Product Attention）\n    _supports_sdpa = True\n    # 模型权重初始化方法，确保不同模块具有合适的初始化方式\n    def _init_weights(self, module):\n        std = self.config.init_std # 读取配置中指定的初始化标准差\n        if isinstance(module, nn.Linear):\n            # 线性层权重正态分布初始化，偏置初始化为0\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            # 词嵌入初始化，padding_idx 对应位置设为0（不会影响模型输出）\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n    # 构造用于测试或模型追踪的伪输入（dummy inputs），常用于：\n    #     - 推理导出（如 ONNX/torchscript）\n    #     - 测试模型结构是否正常\n    @property\n    def dummy_inputs(self):\n        pad_token = self.config.pad_token_id   # 获取 pad token 的 ID\n        # 构造示例 input_ids，其中每行是一条伪输入序列\n        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n        # attention_mask 为非 pad 位置设置为 True,填充位置为False\n        # ne 是 not equal（不等于）的缩写。\n        # input_ids.ne(pad_token) 等价于 input_ids != pad_token。\n        dummy_inputs = {\n            \"attention_mask\": input_ids.ne(pad_token),\n            \"input_ids\": input_ids,\n        }\n        return dummy_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:15.499415Z","iopub.execute_input":"2025-05-26T17:05:15.499767Z","iopub.status.idle":"2025-05-26T17:05:15.513441Z","shell.execute_reply.started":"2025-05-26T17:05:15.499742Z","shell.execute_reply":"2025-05-26T17:05:15.512667Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# 为什么要跳过它？ _skip_keys_device_placement = \"past_key_values\"\n# 因为 past_key_values：\n# 通常是 缓存中间状态（如 Transformer 解码器中 self-attention 的 KV 缓存）；\n# 在预训练模型的权重文件里一般 没有这个字段，或者它是运行时动态生成的；\n# 强行对它进行 .to(device) 会导致 出错或无意义操作。\n# 当加载模型权重时，跳过将 past_key_values 对应的权重强制移动到目标设备（如 GPU），即：\n# 如果权重的键名中包含 \"past_key_values\"，则不要显式地为它设置 .to(device)。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔍 区分两个概念：\n# 名称\t说明\t和 _no_split_modules 的关系\n# 静态结构拆分（Module Splitting）\t指将一个大的子模块（如 BartEncoderLayer）打断为多个子子模块\t\n# ✅ 这个会参考 _no_split_modules，防止被拆开\n# 权重分片（Checkpoint Sharding）\t仅仅把模型权重保存到多个 .bin 文件中，如 \n# pytorch_model-00001-of-00005.bin\t❌ 这个和 _no_split_modules 无关，只影响 I/O 层面的读\n# 这里的“权重分片（sharding）”是指 存储时的分片，不是指运行时的设备间分片。\n# 🔍 具体说明：\n# 类型\t说明\t是否和 _no_split_modules 有关\n# 存储分片（Storage Sharding）\t把 pytorch_model.bin 拆成多个文件，比如 pytorch_model-00001-of-00005.bin。常用于大模型存储。\t❌ 无关\n# 运行时分片（Runtime Sharding）\t像 FSDP（Fully Sharded Data Parallel）那样，在多个 GPU 之间分配部分权重，仅在需要时通信\t❌ 也无关\n# 模块结构拆分（Module Splitting）\t分析 nn.Module 的结构，拆分成更小的子块，常用于部署或模型转换\t✅ _no_split_modules 就是防止这类结构性拆分","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 拆分会做什么？\n# 以 FSDP 为例，它可能会将如下结构拆分：\n# Sequential(\n#     LayerNorm,\n#     Attention,\n#     LayerNorm,\n#     MLP\n# )\n# 为多个部分分配到多个 GPU 上，但 不能拆分一个 BartEncoderLayer 的内部结构，否则就会打断 forward 流程：\n# class BartEncoderLayer(nn.Module):  # 🚫 拆分它就会坏\n#     ...\n# 总结：\n# _no_split_modules 是用来保护关键模块（如 Encoder/Decoder Layer）在结构变换时不被“打断”。\n# 它只在 分布式训练、加载大模型分片、导出 ONNX 等场景中会被使用。\n# 平常用 model = BartModel.from_pretrained(...) 是不会触发的。除非你启用了大模型加载、FSDP、DeepSpeed 等。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aa=torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2,0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:20.332358Z","iopub.execute_input":"2025-05-26T17:05:20.332896Z","iopub.status.idle":"2025-05-26T17:05:20.337132Z","shell.execute_reply.started":"2025-05-26T17:05:20.332870Z","shell.execute_reply":"2025-05-26T17:05:20.336236Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"aa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:20.511273Z","iopub.execute_input":"2025-05-26T17:05:20.511821Z","iopub.status.idle":"2025-05-26T17:05:20.517291Z","shell.execute_reply.started":"2025-05-26T17:05:20.511800Z","shell.execute_reply":"2025-05-26T17:05:20.516467Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0,  6, 10,  4,  2],\n        [ 0,  8, 12,  2,  0]])"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"aa.ne(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:20.674198Z","iopub.execute_input":"2025-05-26T17:05:20.674939Z","iopub.status.idle":"2025-05-26T17:05:20.680333Z","shell.execute_reply.started":"2025-05-26T17:05:20.674916Z","shell.execute_reply":"2025-05-26T17:05:20.679578Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"tensor([[False,  True,  True,  True,  True],\n        [False,  True,  True,  True, False]])"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"# ⚙️ __init_subclass__() 的作用：\n# 这个是 Python 类机制的一个钩子方法，当有其他类继承这个类时会触发。这里用它来发出弃用警告：\n# 也就是说，只要用户写了：class MyModel(PretrainedBartModel): ...\n# 就会看到警告：“请用 BartPreTrainedModel”。\n# 这段代码是为了兼容旧代码和拼写错误的类名，统一迁移到正确的 BartPreTrainedModel，并通过 \n# __init_subclass__() 发出弃用警告。\nclass PretrainedBartModel(BartPreTrainedModel): # ✅ 旧的类名\n    def __init_subclass__(self):\n        warnings.warn(\n            \"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\",\n            FutureWarning,\n        )\n# 这是为了兼容拼错类名的情况。有人可能写错了：\n# class MyModel(BartPretrainedModel): ...\n# 为了不直接报错，而是发出友好的警告提示：\n# 请使用正确的 BartPreTrainedModel\n# BartPretrainedModel 里面train是小写的t,这里防止的是拼错的情况\n# BartPreTrainedModel Trained是大写的T\nclass BartPretrainedModel(BartPreTrainedModel):  # ❌ 这个是拼错的类名\n    def __init_subclass__(self):\n        warnings.warn(\n            \"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\",\n            FutureWarning,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:23.760071Z","iopub.execute_input":"2025-05-26T17:05:23.760350Z","iopub.status.idle":"2025-05-26T17:05:23.765250Z","shell.execute_reply.started":"2025-05-26T17:05:23.760326Z","shell.execute_reply":"2025-05-26T17:05:23.764595Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# __init_subclass__ 是 Python 提供的一个钩子方法（hook），在一个类被 继承 时自动调用，用于在\n# 定义子类时做一些处理，而不是在实例化时调用。\n# class Base:\n#     def __init_subclass__(cls):\n#         print(f\"{cls.__name__} is a subclass of Base\")\n\n# class Sub(Base):\n#     pass  # 触发 __init_subclass__，会打印信息\n# 当你定义 Sub(Base) 这个类时，Base.__init_subclass__ 会被自动调用，并传入 cls=Sub。\n# ✅ 用法场景\n# 给子类添加默认行为或注册信息\n# 发出警告（如你看到的例子）\n# 校验子类是否实现了某些方法","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 本质上在做什么？\n# 这是为了兼容历史代码中的旧类名或拼错的类名：\n\n# 类名\t用途\n# PretrainedBartModel\t早期版本遗留下来的类，已弃用\n# BartPretrainedModel\t用户可能拼写错误的类名，也予以警告处理\n# BartPreTrainedModel\t✅ 当前推荐使用的类名（注意大小写）","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BART_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`BartConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nBART_GENERATION_EXAMPLE = r\"\"\"\n    Summarization example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n\n    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n    >>> ARTICLE_TO_SUMMARIZE = (\n    ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n    ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n    ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n    ... )\n    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n\n    >>> # Generate Summary\n    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n    ```\n\n    Mask filling example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\n    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n    >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n    >>> logits = model(input_ids).logits\n\n    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n    >>> probs = logits[0, masked_index].softmax(dim=0)\n    >>> values, predictions = probs.topk(5)\n\n    >>> tokenizer.decode(predictions).split()\n    ['not', 'good', 'healthy', 'great', 'very']\n    ```\n\"\"\"\n\nBART_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n            For translation and summarization training, `decoder_input_ids` should be provided. If no\n            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n            for denoising pre-training following the paper.\n        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n            1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n            than the model's internal embedding lookup matrix.\n        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n            input (see `past_key_values`). This is useful if you want more control over how to convert\n            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n\n            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n            of `inputs_embeds`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:27.292434Z","iopub.execute_input":"2025-05-26T17:05:27.293083Z","iopub.status.idle":"2025-05-26T17:05:27.300436Z","shell.execute_reply.started":"2025-05-26T17:05:27.293062Z","shell.execute_reply":"2025-05-26T17:05:27.299655Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"config.max_position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:32.430895Z","iopub.execute_input":"2025-05-26T17:05:32.431579Z","iopub.status.idle":"2025-05-26T17:05:32.436219Z","shell.execute_reply.started":"2025-05-26T17:05:32.431555Z","shell.execute_reply":"2025-05-26T17:05:32.435514Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"1024"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"math.sqrt(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:32.619981Z","iopub.execute_input":"2025-05-26T17:05:32.620213Z","iopub.status.idle":"2025-05-26T17:05:32.624872Z","shell.execute_reply.started":"2025-05-26T17:05:32.620197Z","shell.execute_reply":"2025-05-26T17:05:32.624085Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"1.4142135623730951"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"aa=torch.randn((2,8,32))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:32.755263Z","iopub.execute_input":"2025-05-26T17:05:32.755664Z","iopub.status.idle":"2025-05-26T17:05:32.759775Z","shell.execute_reply.started":"2025-05-26T17:05:32.755644Z","shell.execute_reply":"2025-05-26T17:05:32.758907Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"aa","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aa[:, :, -1] # 只是选取最后一个维度的32个元素的向量中,最后那个数据元素","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:36.754996Z","iopub.execute_input":"2025-05-26T17:05:36.755666Z","iopub.status.idle":"2025-05-26T17:05:36.761010Z","shell.execute_reply.started":"2025-05-26T17:05:36.755641Z","shell.execute_reply":"2025-05-26T17:05:36.760395Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.9493, -1.3439, -0.1623,  1.3046,  0.6839,  1.0765, -0.0886,  0.5509],\n        [ 0.8409,  0.5754,  0.5625, -0.6599,  1.9180, -1.1001,  0.2933,  0.5442]])"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"bb=torch.Tensor([[1,1,0],[0,5,6]])\n0 in bb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:36.902188Z","iopub.execute_input":"2025-05-26T17:05:36.902419Z","iopub.status.idle":"2025-05-26T17:05:36.907495Z","shell.execute_reply.started":"2025-05-26T17:05:36.902388Z","shell.execute_reply":"2025-05-26T17:05:36.906882Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"torch.rand([])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:53.953023Z","iopub.execute_input":"2025-05-26T17:05:53.953549Z","iopub.status.idle":"2025-05-26T17:05:53.959211Z","shell.execute_reply.started":"2025-05-26T17:05:53.953510Z","shell.execute_reply":"2025-05-26T17:05:53.958520Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"tensor(0.6346)"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"config.encoder_layerdrop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:54.163800Z","iopub.execute_input":"2025-05-26T17:05:54.164001Z","iopub.status.idle":"2025-05-26T17:05:54.168659Z","shell.execute_reply.started":"2025-05-26T17:05:54.163986Z","shell.execute_reply":"2025-05-26T17:05:54.167857Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"# BART 编码器模块，由多个 BartEncoderLayer 层堆叠组成。\nclass BartEncoder(BartPreTrainedModel):\n\n    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n        self.dropout = config.dropout # dropout 概率\n        self.layerdrop = config.encoder_layerdrop # layerdrop 概率，用于训练时跳过层\n        embed_dim = config.d_model # 嵌入维度\n        self.padding_idx = config.pad_token_id # 填充id\n        self.max_source_positions = config.max_position_embeddings # 最大位置序列长度\n        embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0  # 嵌入缩放\n        # token嵌入\n        self.embed_tokens = BartScaledWordEmbedding(\n            config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n        )\n        # 如果有指定的嵌入权重,就设置为传入的,这是有预训练词向量的情况\n        if embed_tokens is not None:\n            self.embed_tokens.weight = embed_tokens.weight\n        # 使用学习的位置编码\n        self.embed_positions = BartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            embed_dim,\n        )\n        # 构建 N 层编码器\n        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n        # 是否使用 FlashAttention 或 SDPA 优化 attention 计算\n        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n        self._use_sdpa = config._attn_implementation == \"sdpa\"\n        self.layernorm_embedding = nn.LayerNorm(embed_dim)  # 输入归一化层\n\n        self.gradient_checkpointing = False   # 控制是否使用梯度检查点以节省显存\n        self.post_init()  # 初始化权重\n    # 获取词嵌入\n    def get_input_embeddings(self):\n        return self.embed_tokens\n    # 设置词嵌入\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        # 参数优先级：函数输入 > 配置文件\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict # 是否返回字典结构\n\n         # input_ids 与 inputs_embeds 只能二选一\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None: # 如果只是设定了input_ids\n            input = input_ids\n            input_ids = input_ids.view(-1, input_ids.shape[-1])  # reshape to 2D\n        elif inputs_embeds is not None: # 只是设定了inputs_embeds\n            input = inputs_embeds[:, :, -1]  # 获取伪位置输入用于位置编码\n        else: # 如果两个都没有指定,报错\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n        # 如果还没有对token嵌入表示\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids) # 就嵌入\n\n        embed_pos = self.embed_positions(input) # 为位置嵌入,每个位置也对应一个向量表示\n        embed_pos = embed_pos.to(inputs_embeds.device)\n        # 初始输入 = token_embed + pos_embed\n        hidden_states = inputs_embeds + embed_pos\n        hidden_states = self.layernorm_embedding(hidden_states) # 层标准化\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training) # dropout\n\n        # 构建注意力 mask\n        if attention_mask is not None:\n            if self._use_flash_attention_2: # 如果是使用flash注意力\n                attention_mask = attention_mask if 0 in attention_mask else None  # FlashAttention 中 0 视为需屏蔽\n            # 如果是使用sdpa 这时就不能有head_mask,也不能指定输出注意力权重\n            elif self._use_sdpa and head_mask is None and not output_attentions:\n                # 当使用 SDPA 时，如果不满足条件,会退回到手动实现方式，在所有情况下都需要一个 4D 的因果掩码。\n                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n            else: # 这种是eager 手动形式的注意力\n                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n\n        encoder_states = () if output_hidden_states else None # 每层的编码器输出\n        all_attentions = () if output_attentions else None\n\n        # 检查 head_mask 长度合法性\n        if head_mask is not None:\n            if head_mask.size()[0] != (len(self.layers)): # head mask的最外轴的大小必须是所有的层数\n                raise ValueError( \n                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n                    f\" {head_mask.size()[0]}.\"\n                )\n        # 编码器层前向过程\n        for idx, encoder_layer in enumerate(self.layers): # 遍历每一层\n            if output_hidden_states: # 如果指定要输出每一层的编码器输出\n                encoder_states = encoder_states + (hidden_states,) # 第一个是词嵌入\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            # 层级 Dropout（LayerDrop），训练时随机跳过某层\n            to_drop = False\n            if self.training: # 如果是训练模式\n                dropout_probability = torch.rand([]) # 随机的一个dropout概率\n                # 如果随机到的数字<配置中指定的layerdrop\n                if dropout_probability < self.layerdrop:  \n                    to_drop = True # 这时设置to_drop为True\n            # if to_drop: 分支中的逻辑 确实是跳过了当前编码器层的前向计算，这是实现 LayerDrop 的核心。\n            # 此处 to_drop 是基于 self.layerdrop 的概率随机决定的（仅在训练时生效），如果为 True，则当前这层 \n            # encoder layer 会被跳过，不进行前向传播。这种技术称为 LayerDrop，类似 Dropout，但作用在整个层级结\n            # 构上，用于正则化深层模型。跳过后当前 hidden_states 会直接传给下一层。\n            if to_drop: # 如果to_drop为True\n                layer_outputs = (None, None) # 跳过该层计算，保持 hidden_states 不变\n            else: # 如果to_drop=False\n                if self.gradient_checkpointing and self.training: # 如果使用梯度检查,并且是训练模式\n                    # 用 gradient checkpoint 节省显存，代价是多次前向\n                    layer_outputs = self._gradient_checkpointing_func(\n                        encoder_layer.__call__,\n                        hidden_states,\n                        attention_mask,\n                        (head_mask[idx] if head_mask is not None else None), # 使用当前层的head_mask\n                        output_attentions,\n                    )\n                else: # 如果是评估模式或者禁用了梯度检查 # 正常前向传播\n                    layer_outputs = encoder_layer(\n                        hidden_states,\n                        attention_mask,\n                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                        output_attentions=output_attentions,\n                    )\n\n                hidden_states = layer_outputs[0] # 更新隐藏状态,供下个迭代使用\n            # 如果要输出注意力矩阵\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n        # 这个是最后一层的编码器输出状态\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n        # 根据情况返回元组或结构化对象(字典结构)\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:05:57.562716Z","iopub.execute_input":"2025-05-26T17:05:57.562972Z","iopub.status.idle":"2025-05-26T17:05:57.579097Z","shell.execute_reply.started":"2025-05-26T17:05:57.562954Z","shell.execute_reply":"2025-05-26T17:05:57.578304Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# BART 解码器模块，由多个 BartDecoderLayer 层组成（Transformer 结构的解码器部分）。\nclass BartDecoder(BartPreTrainedModel):\n    \n    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n        self.dropout = config.dropout  # dropout 概率\n        # layerdrop 是一种正则手段，在训练时以一定概率跳过整个 decoder layer\n        self.layerdrop = config.decoder_layerdrop\n        self.padding_idx = config.pad_token_id # padding token 的 id，用于 mask 掉 padding 部分\n        # 最大解码长度（即位置编码的最大长度）\n        self.max_target_positions = config.max_position_embeddings\n        # 是否使用缩放嵌入（通常 sqrt(d_model)），以控制初始嵌入的方差，避免太大或太小\n        embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n        # 词嵌入层，实际为可学习参数表，输出 shape 为 [bsz, seq_len, d_model]\n        self.embed_tokens = BartScaledWordEmbedding(\n            config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n        )\n        # 如果外部传入 embedding，则使用共享 embedding（通常 encoder 与 decoder 共享词向量）\n        if embed_tokens is not None:\n            self.embed_tokens.weight = embed_tokens.weight\n        # 位置嵌入，LearnedPositionalEmbedding 是可学习的位置编码（相比于 sinusoidal 编码）\n        self.embed_positions = BartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n        )\n        # 构建 N 层解码器，每层是一个标准的 BartDecoderLayer\n        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n         # 是否使用 Flash Attention v2（高效注意力实现，适用于较长序列）\n        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         # 是否使用 SDPA（torch.nn.functional.scaled_dot_product_attention），为 PyTorch 原生高效实现\n        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         # 输入嵌入后进行 layer norm，有助于稳定训练过程\n        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n        # 是否启用 gradient checkpointing（以计算图为单位进行中间层保存，节省显存）\n        self.gradient_checkpointing = False\n        # 初始化权重\n        self.post_init()\n    # 获取输入的词嵌入（用于生成时 tie-weights）\n    def get_input_embeddings(self):\n        return self.embed_tokens\n    # 设置输入嵌入（通常是为了权重共享）\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n        # 设置默认值（可被 forward 的入参覆盖）\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # 检查输入，input_ids 与 inputs_embeds 二选一\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input = input_ids\n            input_shape = input.shape\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1] # (b,s)\n            input = inputs_embeds[:, :, -1]\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        # 如果使用 past_key_values（推理时加速用，只需输入新 token）\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n        # 如果没提供 inputs_embeds，用 embedding 层查表\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input)\n        # 因果注意力 mask 构建（不同模式处理不同）\n        # 这段代码的作用就是构建因果注意力掩码（causal attention mask），并根据不同情况选择合适的构建方式。\n        # 以下是详细注释版代码，包含设计意图和功能说明，全部写在代码内部：\n        if self._use_flash_attention_2:\n            # flash attention 不支持全 1 mask，因此仅在存在 padding 时保留 mask\n            # Flash Attention 要求不传入全为 1 的 attention_mask（即没有 padding），\n            # 否则会报错，因此仅在存在 padding（即 attention_mask 中存在 0）时才保留 mask。\n            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n        elif self._use_sdpa and not output_attentions and cross_attn_head_mask is None:\n            # 使用 SDPA 且不返回注意力，构建 4D mask（适配 scaled_dot_product_attention）\n            # - 不需要输出注意力权重（output_attentions=False）\n            # - 不存在 cross attention 的 head mask（cross_attn_head_mask=None）\n            # 满足这些条件时，可以使用 SDPA 提供的更高效实现\n            # 构建的是 4D 版本的因果掩码（[batch_size, num_heads, query_len, key_len]），\n            # 用于告诉模型：每个位置只能看到当前和前面的 token。\n            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n                attention_mask,\n                input_shape,\n                inputs_embeds,\n                past_key_values_length,\n            )\n        else:\n            # 常规模式构建 4D causal mask（decoder 只能看自己和之前的 token）\n            # 非 Flash Attention 或 SDPA 情况（比如开启了 output_attentions 或使用了 head mask），\n            # 使用常规方法构建 4D 因果掩码，限制 decoder 每个 token 只能看到它自己和前面的 token。\n            attention_mask = _prepare_4d_causal_attention_mask(\n                attention_mask, input_shape, inputs_embeds, past_key_values_length\n            )\n\n        # 如果有 encoder_hidden_states，这时是交叉注意力,这里构建编码器填充掩码用于交叉注意力\n        # 该段代码是在交叉注意力阶段（cross-attention）构建用于编码器输出的 attention mask\n        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n            if self._use_flash_attention_2:\n                # Flash Attention v2 要求 attention_mask 不能是全 1（即没有 padding），否则会出错。\n                # 因此仅在存在 padding（即 mask 中有 0）时才保留掩码。\n                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n            elif self._use_sdpa and cross_attn_head_mask is None and not output_attentions:\n                # 上面的条件是使用sdpa,使用的话头掩码就不能有,而且不能输出注意力权重矩阵\n                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                # 从形状为 2D 的掩码创建形状为 `(batch_size, 1, query_length, key_value_length)` 的非因果 4D 掩码\n                # - 不使用 head 掩码（cross_attn_head_mask 为 None）\n                # - 不输出注意力权重（output_attentions=False）\n                # 满足这些条件才能走 SDPA 的高效实现。\n                # 将 2D mask [batch_size, src_len] 转为 4D mask [batch_size, 1, tgt_len, src_len]\n                # 这是一个 **非因果掩码**，用于 cross-attention 阶段的编码器掩码。\n                # 表示 decoder 的每个 query 可以看到 encoder 的所有 key（除非 key 是 padding）\n                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                    encoder_attention_mask,\n                    inputs_embeds.dtype,\n                    tgt_len=input_shape[-1],  # tgt_len = decoder query 的长度\n                )\n            else:\n                # 使用普通方式构建 cross-attention 的编码器填充掩码\n                # 同样将 2D mask 扩展为 4D 形状 [batch_size, 1, tgt_len, src_len]\n                encoder_attention_mask = _prepare_4d_attention_mask(\n                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n                )\n\n         # 位置嵌入（支持 past_key_values 时从中间开始编码）\n        positions = self.embed_positions(input, past_key_values_length)\n        positions = positions.to(inputs_embeds.device)\n        # 加上位置嵌入后 layernorm，然后 dropout\n        hidden_states = inputs_embeds + positions\n        hidden_states = self.layernorm_embedding(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        # 若使用 gradient checkpointing，禁用 cache（两者冲突）\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # 初始化返回值缓存（仅在用户需要时收集）\n        all_hidden_states = () if output_hidden_states else None # 是否输出每一层的解码器输出\n        all_self_attns = () if output_attentions else None # 所有的注意力权重矩阵\n        # 所有的交叉注意力权重矩阵,encoder_hidden_states is not None 是说有传入编码器输出\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n        next_decoder_cache = () if use_cache else None # 缓存的past_key_value\n\n        # 检查 head_mask / cross_attn_head_mask 是否具有正确的层数\n        # head_mask 用于控制每一层的 self-attention 中哪些头参与计算\n        # cross_attn_head_mask 用于控制每一层的 cross-attention 中哪些头参与计算\n        # 若指定了，则必须与 decoder 层数一致\n        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n            if attn_mask is not None:\n                if attn_mask.size()[0] != (len(self.layers)):\n                    raise ValueError(\n                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n                        f\" {head_mask.size()[0]}.\"\n                    )\n        # 遍历 decoder 的每一层进行前向传播\n        for idx, decoder_layer in enumerate(self.layers):\n            # 若启用输出中间层 hidden states，则保存当前层输入\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            # LayerDrop：训练时以一定概率跳过某一层，以提升模型鲁棒性\n            if self.training:\n                dropout_probability = torch.rand([])  # 生成一个标量随机数\n                if dropout_probability < self.layerdrop: \n                    continue # 跳过当前层，直接进入下一层\n            # 获取本层的 past_key_value（用于加速生成时的缓存）\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n             # 若启用梯度检查点（节省显存），则通过 checkpointing 执行当前 decoder layer 的前向计算\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,  # 被 checkpoint 的函数\n                    hidden_states,  # 当前输入\n                    attention_mask, # decoder 的 self-attention mask（通常为 causal mask）\n                    encoder_hidden_states,  # cross attention 中的 key/value（来自 encoder）\n                    encoder_attention_mask,  # cross attention 的 attention mask（通常是 padding mask）\n                    head_mask[idx] if head_mask is not None else None, # 指定当前层的自注意力的 head_mask\n                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, # cross-attention 的头掩码\n                    None, # position bias，旧版支持，现在未使用\n                    output_attentions,  # 是否输出注意力权重\n                    use_cache,   # 是否使用缓存（加速生成）\n                )\n            else: # 标准前向传播（无 gradient checkpointing）\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                    cross_attn_layer_head_mask=(\n                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                    ),\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n            hidden_states = layer_outputs[0] # 更新 hidden_states 供下一层使用\n             # 如果开启 use_cache，将当前层的 past_key_value 加入缓存中，供生成用\n            if use_cache:\n                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n            # 若开启输出 attentions，则收集当前层的 self-attention 权重\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n                # 如果存在 编码器输出（即为 cross-attention），则收集 cross-attention 权重\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        # 最后一层的 hidden_states 也加入中间层输出\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        # 最终决定是否返回缓存\n        next_cache = next_decoder_cache if use_cache else None\n        # 若不要求返回 dict，则返回 tuple 形式（为了兼容老接口）\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n                if v is not None\n            )\n        # 默认返回 BaseModelOutputWithPastAndCrossAttentions，结构化封装全部输出\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,  # decoder 最后一层输出\n            past_key_values=next_cache,   # 各层缓存（用于生成加速）\n            hidden_states=all_hidden_states,   # 所有中间层 hidden state（如果启用）\n            attentions=all_self_attns,  # 所有 self-attention 权重（如果启用）\n            cross_attentions=all_cross_attentions,  # 所有 cross-attention 权重（如果启用）\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:03.285937Z","iopub.execute_input":"2025-05-26T17:06:03.286218Z","iopub.status.idle":"2025-05-26T17:06:03.310160Z","shell.execute_reply.started":"2025-05-26T17:06:03.286197Z","shell.execute_reply":"2025-05-26T17:06:03.309349Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"config.tie_word_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:10.058745Z","iopub.execute_input":"2025-05-26T17:06:10.059605Z","iopub.status.idle":"2025-05-26T17:06:10.064435Z","shell.execute_reply.started":"2025-05-26T17:06:10.059578Z","shell.execute_reply":"2025-05-26T17:06:10.063734Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# 在 BartEncoder 和 BartDecoder 的初始化中，会将 self.shared 这个共享的嵌入层传进去。所以理论上它们内部的 embed_tokens \n# 都会指向 self.shared。\n\n# 但有些模型 checkpoint（例如 facebook/bart-large-cnn）中，实际保存的是两个独立的权重矩阵（decoder 有自己的嵌入），因此加载后：\n\n# self.encoder.embed_tokens 和 self.decoder.embed_tokens 是两个不同对象；\n\n# 此时模型内部就需要手动调用 _tie_or_clone_weights 来确保它们的 weight 是同一个张量。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n    BART_START_DOCSTRING,\n)\nclass BartModel(BartPreTrainedModel):\n    # 指定共享参数的键（用于词嵌入权重的参数共享）\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n        # 提取配置中的 pad token id 和词表大小\n        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n        # 如果启用 scale_embedding，则将嵌入放大 sqrt(d_model)，否则为 1\n        embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n        # 创建共享嵌入层，encoder 和 decoder 共用\n        self.shared = BartScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)\n         # 初始化编码器和解码器，传入共享词嵌入\n        self.encoder = BartEncoder(config, self.shared)\n        self.decoder = BartDecoder(config, self.shared)\n\n        # 初始化所有权重\n        self.post_init()\n\n    def _tie_weights(self):\n        # 如果配置中启用了词嵌入共享（tie_word_embeddings=True），则需要将 encoder、decoder 和 shared 的词嵌入层权重绑定为同一个张量。\n        # 这样做的目的是减少模型参数量，同时提升 encoder 和 decoder 在词嵌入层的一致性（如在翻译任务中共享词汇表）。\n        if self.config.tie_word_embeddings:\n            # 特殊处理 “meta” 设备的情况：\n            # 某些模型（例如 \"facebook/bart-large-cnn\"）在加载权重时，其 shared 层可能处于未实际分配内存的 “meta” 设备上\n            # （即 lazy weight loading），\n            # 而 decoder 的 embed_tokens 已被实际分配到设备上了（如 CPU/GPU）。\n            # 此时不能直接使用 shared，因为它还没有实际的内存张量。\n            # 所以我们优先将 encoder 的嵌入层权重与 decoder 的绑定，再将 shared 的也与 decoder 的绑定（\n            # 使得三者共享同一权重张量）。\n            if self.shared.weight.device == torch.device(\n                \"meta\") and self.decoder.embed_tokens.weight.device != torch.device(\"meta\"):\n                # self.encoder.embed_tokens=self.decoder.embed_tokens\n                self._tie_or_clone_weights(self.encoder.embed_tokens, self.decoder.embed_tokens)\n                # self.shared=self.decoder.embed_tokens\n                self._tie_or_clone_weights(self.shared, self.decoder.embed_tokens)\n            # 正常情况（所有权重都已实际加载，且设备一致），直接将 encoder 和 decoder 的嵌入层权重绑定到 shared 上，\n            # 这样三者都共享 shared.weight 的张量（即 encoder.embed_tokens.weight 和 decoder.embed_tokens.weight \n            # 都指向 shared.weight）\n            else:\n                # 这里把self.encoder.embed_tokens和self.decoder.embed_tokens都设置成了self.shared\n                self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n                self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n    # 返回模型输入嵌入层（共享词嵌入）\n    def get_input_embeddings(self):\n        return self.shared\n     # 设置共享词嵌入（确保 encoder/decoder 同步）\n    def set_input_embeddings(self, value):\n        self.shared = value\n        self.encoder.embed_tokens = self.shared\n        self.decoder.embed_tokens = self.shared\n     # 返回 encoder 子模块（可用于抽取特征）\n    def get_encoder(self):\n        return self.encoder\n    # 返回 decoder 子模块（可用于解码）\n    def get_decoder(self):\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=Seq2SeqModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_EXPECTED_OUTPUT_SHAPE,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqModelOutput]:\n         # 如果 decoder_input_ids 和 decoder_inputs_embeds 都没有提供，\n        # 默认用 input_ids 的右移版本初始化 decoder_input_ids（符合自回归解码器输入规范）\n        if decoder_input_ids is None and decoder_inputs_embeds is None:\n            if input_ids is None:\n                raise ValueError(\n                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n                )\n\n            decoder_input_ids = shift_tokens_right( # 解码器输入[:,:-1]\n                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n            )\n        # 使用配置中默认值更新控制参数（允许用户显式传入覆盖配置）\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         # 如果不存在编码器的输出,就先调用encoeer获取编码器输出\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids, # 编码器输入序列的 token id\n                attention_mask=attention_mask,  # 编码器输入的注意力掩码，屏蔽 padding 位置\n                head_mask=head_mask,  # 编码器自注意力头的掩码（可选）\n                inputs_embeds=inputs_embeds,  # 编码器输入的嵌入向量（可选）\n                output_attentions=output_attentions, # 是否输出注意力权重\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,    # 是否返回 dict 结构\n            )\n        # 如果外部传入的 encoder_outputs 是 tuple，但要求返回 dict，则包装成 BaseModelOutput\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],  # 编码器最后一层隐藏状态（主输出）\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,  # 编码器各层隐藏状态序列（可选）\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None, # 编码器各层注意力权重（可选）\n            )\n\n        # 解码器执行，传入编码器输出作为 cross-attention 的输入\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,  # 解码器输入的 token id（目标序列）\n            attention_mask=decoder_attention_mask,  # 解码器自注意力掩码（因果掩码+padding掩码）\n            encoder_hidden_states=encoder_outputs[0],# 编码器最后一层隐藏状态，作为 cross-attention 的 key/value\n            encoder_attention_mask=attention_mask, # 编码器的注意力掩码，防止对 padding 位置关注\n            head_mask=decoder_head_mask,   # 解码器自注意力头的掩码（可选）\n            cross_attn_head_mask=cross_attn_head_mask,   # 解码器交叉注意力头的掩码（可选）\n            past_key_values=past_key_values,   # 解码器缓存的历史 key/value（用于加速推理）\n            inputs_embeds=decoder_inputs_embeds,   # 解码器输入的嵌入向量（可选）\n            use_cache=use_cache,   # 是否启用缓存机制，提升解码效率\n            output_attentions=output_attentions,   # 是否输出注意力权重\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,   # 是否返回 dict 结构\n        )\n        # 如果不需要返回 dict，则将解码器输出和编码器输出拼接成元组返回\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n        # 否则,返回结构化输出\n        # 标准输出结构，便于使用者访问各组件（编码器/解码器的隐藏状态、注意力等）\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,   # 解码器最后一层隐藏状态（主输出）\n            past_key_values=decoder_outputs.past_key_values,  # 解码器缓存的历史 key/value\n            decoder_hidden_states=decoder_outputs.hidden_states,  # 解码器各层隐藏状态序列（可选）\n            decoder_attentions=decoder_outputs.attentions,  # 解码器各层自注意力权重（可选）\n            cross_attentions=decoder_outputs.cross_attentions,  # 解码器各层交叉注意力权重（可选）\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,  # 编码器最后一层隐藏状态\n            encoder_hidden_states=encoder_outputs.hidden_states,    # 编码器各层隐藏状态序列（可选）\n            encoder_attentions=encoder_outputs.attentions,    # 编码器各层注意力权重（可选）\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:32.520454Z","iopub.execute_input":"2025-05-26T17:06:32.520763Z","iopub.status.idle":"2025-05-26T17:06:32.538702Z","shell.execute_reply.started":"2025-05-26T17:06:32.520743Z","shell.execute_reply":"2025-05-26T17:06:32.537869Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"help(BartModel.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Example:\n    \n #    ```python\n #    >>> from transformers import AutoTokenizer, BartModel\n #    >>> import torch\n    \n #    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n #    >>> model = BartModel.from_pretrained(\"facebook/bart-base\")\n    \n #    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n #    >>> outputs = model(**inputs)\n    \n #    >>> last_hidden_states = outputs.last_hidden_state\n    # ```","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BartModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:35.489092Z","iopub.execute_input":"2025-05-26T17:06:35.489363Z","iopub.status.idle":"2025-05-26T17:06:35.540661Z","shell.execute_reply.started":"2025-05-26T17:06:35.489346Z","shell.execute_reply":"2025-05-26T17:06:35.539897Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = BartModel.from_pretrained(\"facebook/bart-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:37.790895Z","iopub.execute_input":"2025-05-26T17:06:37.791142Z","iopub.status.idle":"2025-05-26T17:06:43.485707Z","shell.execute_reply.started":"2025-05-26T17:06:37.791124Z","shell.execute_reply":"2025-05-26T17:06:43.485059Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70c64124f8946668c2c03f1ba9d2951"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5028209116594b85bd66e684a25ddfd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6e31f7b20342e3bf94d7186d199bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30cf50350f6842ceafd5e74899399eaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602fc62bb7264eb69314ca8482ba3773"}},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:45.589254Z","iopub.execute_input":"2025-05-26T17:06:45.589985Z","iopub.status.idle":"2025-05-26T17:06:45.737414Z","shell.execute_reply.started":"2025-05-26T17:06:45.589958Z","shell.execute_reply":"2025-05-26T17:06:45.736577Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:06:55.052580Z","iopub.execute_input":"2025-05-26T17:06:55.053031Z","iopub.status.idle":"2025-05-26T17:06:55.059179Z","shell.execute_reply.started":"2025-05-26T17:06:55.053008Z","shell.execute_reply":"2025-05-26T17:06:55.058394Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"print(type(outputs.past_key_values),len(outputs.past_key_values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:07:06.414091Z","iopub.execute_input":"2025-05-26T17:07:06.414361Z","iopub.status.idle":"2025-05-26T17:07:06.418657Z","shell.execute_reply.started":"2025-05-26T17:07:06.414343Z","shell.execute_reply":"2025-05-26T17:07:06.417766Z"}},"outputs":[{"name":"stdout","text":"<class 'tuple'> 6\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"print(len(outputs.past_key_values[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:07:09.136874Z","iopub.execute_input":"2025-05-26T17:07:09.137162Z","iopub.status.idle":"2025-05-26T17:07:09.141220Z","shell.execute_reply.started":"2025-05-26T17:07:09.137142Z","shell.execute_reply":"2025-05-26T17:07:09.140587Z"}},"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"[i.shape for i in outputs.past_key_values[0]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:07:12.217478Z","iopub.execute_input":"2025-05-26T17:07:12.218246Z","iopub.status.idle":"2025-05-26T17:07:12.223748Z","shell.execute_reply.started":"2025-05-26T17:07:12.218213Z","shell.execute_reply":"2025-05-26T17:07:12.222842Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"[torch.Size([1, 12, 8, 64]),\n torch.Size([1, 12, 8, 64]),\n torch.Size([1, 12, 8, 64]),\n torch.Size([1, 12, 8, 64])]"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"config.decoder_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:07:14.735644Z","iopub.execute_input":"2025-05-26T17:07:14.735910Z","iopub.status.idle":"2025-05-26T17:07:15.550638Z","shell.execute_reply.started":"2025-05-26T17:07:14.735892Z","shell.execute_reply":"2025-05-26T17:07:15.549580Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"outputs.last_hidden_state.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:07:17.656009Z","iopub.execute_input":"2025-05-26T17:07:17.656300Z","iopub.status.idle":"2025-05-26T17:07:17.661280Z","shell.execute_reply.started":"2025-05-26T17:07:17.656281Z","shell.execute_reply":"2025-05-26T17:07:17.660617Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 768])"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"print(outputs.decoder_hidden_states,outputs.encoder_last_hidden_state.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:07:19.978393Z","iopub.execute_input":"2025-05-26T17:07:19.978673Z","iopub.status.idle":"2025-05-26T17:07:19.982821Z","shell.execute_reply.started":"2025-05-26T17:07:19.978652Z","shell.execute_reply":"2025-05-26T17:07:19.982016Z"}},"outputs":[{"name":"stdout","text":"None torch.Size([1, 8, 768])\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# 虽然在 __init__() 中：\n# self.shared = BartScaledWordEmbedding(...)\n# self.encoder = BartEncoder(config, self.shared)\n# self.decoder = BartDecoder(config, self.shared)\n# 已经传入了同一个 Embedding 实例 self.shared，表面上看已经实现了共享，但实际还需要 self._tie_weights() \n# 来确保权重绑定为同一个张量对象，特别是在以下几种场景中才真正“起作用”：\n# _tie_weights() 作用场景详解：\n# 1.处理部分 checkpoint 权重结构不一致的问题：\n# 比如有些模型的权重文件中，只保存了 decoder.embed_tokens.weight，没有保存 shared.weight 或 \n# encoder.embed_tokens.weight，那就需要靠 _tie_weights() 手动绑定权重关系，否则三者会不一致。\n# 2.处理 Meta device 场景：\n# 在 lazy init / pipeline parallelism 场景中（如加载在 meta 设备时），三个模块的 Embedding 虽然逻辑上是同一\n# 个，但还没有实际分配权重张量，必须调用 _tie_weights() 来显式绑定。\n# 3.适配 from_pretrained() 加载模型后：\n# from_pretrained() 加载模型时可能只加载 decoder.embed_tokens 的权重，此时模型内部 self.shared 是一个新\n# 的 Embedding 实例，必须通过 _tie_weights() 把它和 decoder/encoder 显式绑定为同一个张量。\n# 4.模型结构定义与权重加载分离设计：\n# 为了将结构定义（__init__）和权重关系（共享 or 不共享）分开设计，Transformers 框架统一使用 _tie_weights() \n# 来在模型构造之后处理“权重绑定关系”，实现更灵活的配置控制。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean_resizing=True):\n    old_embeddings = self.get_input_embeddings()\n    new_embeddings = self._get_resized_embeddings(\n        old_embeddings, new_num_tokens, pad_to_multiple_of, mean_resizing\n    )\n    if hasattr(old_embeddings, \"_hf_hook\"):\n        hook = old_embeddings._hf_hook\n        add_hook_to_module(new_embeddings, hook)\n    old_embeddings_requires_grad = old_embeddings.weight.requires_grad\n    new_embeddings.requires_grad_(old_embeddings_requires_grad)\n    self.set_input_embeddings(new_embeddings)\n    is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n\n    # Update new_num_tokens with the actual size of new_embeddings\n    if pad_to_multiple_of is not None:\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            with deepspeed.zero.GatheredParameters(new_embeddings.weight, modifier_rank=None):\n                new_num_tokens = new_embeddings.weight.shape[0]\n        else:\n            new_num_tokens = new_embeddings.weight.shape[0]\n\n    # if word embeddings are not tied, make sure that lm head is resized as well\n    if (\n        self.get_output_embeddings() is not None\n        and not self.config.get_text_config(decoder=True).tie_word_embeddings\n    ):\n        old_lm_head = self.get_output_embeddings()\n        if isinstance(old_lm_head, torch.nn.Embedding):\n            new_lm_head = self._get_resized_embeddings(\n                old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n        else:\n            new_lm_head = self._get_resized_lm_head(\n                old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n        if hasattr(old_lm_head, \"_hf_hook\"):\n            hook = old_lm_head._hf_hook\n            add_hook_to_module(new_lm_head, hook)\n        old_lm_head_requires_grad = old_lm_head.weight.requires_grad\n        new_lm_head.requires_grad_(old_lm_head_requires_grad)\n        self.set_output_embeddings(new_lm_head)\n\n    return self.get_input_embeddings()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:04.210329Z","iopub.execute_input":"2025-05-26T17:08:04.210629Z","iopub.status.idle":"2025-05-26T17:08:04.218325Z","shell.execute_reply.started":"2025-05-26T17:08:04.210609Z","shell.execute_reply":"2025-05-26T17:08:04.217481Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# 定义了一个子类 BartForConditionalGeneration。\n# 继承自：\n# BartPreTrainedModel：提供权重加载、保存、初始化等通用模型机制。\n# GenerationMixin：混入生成逻辑，如 .generate() 方法，支持 beam search、greedy search 等。\n@add_start_docstrings(\n    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n)\nclass BartForConditionalGeneration(BartPreTrainedModel, GenerationMixin):\n    # 表示该模型包含一个子模块叫 self.model（后续代码会看到），这是原始的 BartModel，它负责编码器-解码器的主体结构。\n    base_model_prefix = \"model\"\n    # 词嵌入共享（weight tying），即：编码器词嵌入 (encoder.embed_tokens)\n    # 解码器词嵌入 (decoder.embed_tokens) 最后的 lm_head（语言建模输出层）\n    # 都共享同一组权重。\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n    # 加载权重时，如果缺失了这些 key，不抛出警告或错误。\n    # final_logits_bias 是一个额外加在输出 logits 上的偏置项（通常初始化为0），用于更灵活地调整预测概率。\n    # 不将它设置为必须项，兼容旧模型权重或不同配置下的模型加载。\n    # 如果这个权重文件里没有包含 \"final_logits_bias\"，不会抛出错误，而是由代码中的逻辑（如 self.register_buffer(\n    # ...)）自动补上默认值。\n    _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n    # 初始化BART条件生成模型，带语言建模头。\n    # 设计意图：基于BART模型，添加语言模型输出层，用于文本生成/摘要等任务。\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n        self.model = BartModel(config) # BART基础模型，包含编码器和解码器\n        # final_logits_bias是一个偏置向量，初始化为0，后续加到预测logits上，调整预测概率分布\n        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n        # 语言模型头，线性层，将隐藏状态映射到词表大小，用于生成token概率\n        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n        # 权重初始化及后续处理（如tie weights）\n        self.post_init()\n    # 返回编码器模块，方便单独访问编码器\n    def get_encoder(self):\n        return self.model.get_encoder()\n    # 返回解码器模块，方便单独访问解码器\n    def get_decoder(self):\n        return self.model.get_decoder()\n    # 调整词嵌入矩阵大小（比如新增词表词汇）。\n    # 同时调整 final_logits_bias 维度，保持与词表大小一致。\n    def resize_token_embeddings(\n        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n    ) -> nn.Embedding:\n        # 调整嵌入矩阵大小（包括 encoder 和 decoder 的 token embedding）\n        # 该方法通常在扩展词表（如添加新token）时调用\n        # super() 实际调用的是 PreTrainedModel 中的 resize_token_embeddings\n        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n        # 因为最终 logits 是通过 `lm_head + final_logits_bias` 得出的，\n        # 所以词表大小变化后，也要同步调整 final_logits_bias 的大小，以确保维度一致\n        # new_embeddings.weight.shape[0] 就是词汇表的大小，即词表中的 token 数量（vocab_size）\n        self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n        return new_embeddings\n    # 调整 final_logits_bias 的大小，确保与当前词表大小一致。\n    # 如果词表变小，裁剪偏置；变大时，补0。\n    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n        # 记录旧的词汇表大小\n        old_num_tokens = self.final_logits_bias.shape[-1]\n        # 新词汇表大小小于等于旧大小，截断 bias 以匹配新词表大小\n        if new_num_tokens <= old_num_tokens:\n            new_bias = self.final_logits_bias[:, :new_num_tokens]\n        else: # 新词汇表大小大于旧大小，扩展 bias 矩阵，新增部分用0填充\n            extra_bias = torch.zeros((\n                1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1) # 在词汇表轴合并\n        # 用新的 bias 替换旧的 final_logits_bias，注册为模型缓冲区，不作为参数训练\n        self.register_buffer(\"final_logits_bias\", new_bias)\n    # 返回语言模型头，用于获取序列token概率\n    def get_output_embeddings(self):\n        return self.lm_head\n    # 设置新的语言模型头\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n    # 绑定词嵌入权重：\n    #     如果配置允许（tie_word_embeddings=True），\n    #     绑定编码器、解码器词嵌入权重与语言模型头权重为同一个张量，\n    #     减少模型参数，提升共享一致性。\n    def _tie_weights(self):\n        if self.config.tie_word_embeddings:\n            self.model._tie_weights()\n            # self.lm_head被设置成self.model.shared的copy\n            self._tie_or_clone_weights(self.lm_head, self.model.shared)\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n        \"\"\"\n        # 在类 BartForConditionalGeneration 的 forward 方法上使用了 @replace_return_docstrings(...) \n        # 装饰器，但这个方法的 docstring 中 没有预留 Returns: 或 Return: 这一行作为占位符，\n        # 而 replace_return_docstrings 要求这样一个占位符来替换文档。所以上面的文档字符串不能删\n        # 是否返回字典结构\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # 如果有传入标签的话\n        if labels is not None:\n            # 由于提供了“labels”，因此“use_cache”参数更改为“False”\n            if use_cache: # 如果使用缓存\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False # 训练时不需要用缓存k,v,因为训练时是并行的\n            # labels存在且未传入decoder_input_ids，则自动右移生成decoder_input_ids\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                # 这时设置目标序列输入为labels[:,:-1]\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n        # 调用底层BART模型的forward，得到隐藏状态等信息\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        # 通过语言模型头将解码器最后隐藏层映射到词表大小，得到预测logits\n        lm_logits = self.lm_head(outputs[0])\n        # 加上偏置final_logits_bias，调整最终概率分布\n        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n\n        masked_lm_loss = None # 掩码语言损失\n        if labels is not None: # 如果标签不是None\n            labels = labels.to(lm_logits.device)\n            # 计算交叉熵损失，只对非-100的token计算\n            loss_fct = CrossEntropyLoss()\n            # 计算交叉熵损失\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n        # 返回元组，包含loss(若计算)、logits和其他模型输出\n        if not return_dict:\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n        # 返回结构化对象，方便访问每个组件的输出\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,  # 若提供了标签，则此项为计算得到的语言建模损失；否则为 None \n            logits=lm_logits,  # 输出的 logits 形状为 [b,s,v]\n            past_key_values=outputs.past_key_values, # 用于加速生成的缓存键值对\n            decoder_hidden_states=outputs.decoder_hidden_states, # 解码器每一层的隐藏状态（可选输出）\n            decoder_attentions=outputs.decoder_attentions,  # 解码器每一层的自注意力权重（可选输出）\n            cross_attentions=outputs.cross_attentions, # 每层解码器对编码器的交叉注意力权重（可选输出）\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state, # 编码器最后一层输出的隐藏状态\n            encoder_hidden_states=outputs.encoder_hidden_states, # 编码器每一层的隐藏状态（可选输出）\n            encoder_attentions=outputs.encoder_attentions, # 编码器每一层的自注意力权重（可选输出）\n        )\n    # 根据标签生成decoder输入id（右移一位），\n    # 用于训练时teacher forcing，保证预测时的输入合理。\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n    # 用于在 beam search 过程中，按照 beam 的重排序索引 `beam_idx` 对缓存的 past_key_values 重新排序，\n    # 以确保后续解码步骤使用正确顺序的上下文信息\n    @staticmethod  # 声明为静态方法，不依赖类实例状态\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()  # 用于收集重排后的每层缓存\n        # layer_past 是一个元组，通常包含：\n        # (self_attn_key, self_attn_value, cross_attn_key, cross_attn_value)\n        # 其中 cross attention 部分在 beam search 时是静态不变的，不需要重排\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) # 按 beam_idx 对 batch 维重排\n                      for past_state in layer_past[:2])  # 只对 self-attn 的 key/value 进行重排\n                + layer_past[2:],# cross-attn 的 key/value 保持原样\n            )\n        return reordered_past","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:07.855028Z","iopub.execute_input":"2025-05-26T17:08:07.855511Z","iopub.status.idle":"2025-05-26T17:08:07.878056Z","shell.execute_reply.started":"2025-05-26T17:08:07.855488Z","shell.execute_reply":"2025-05-26T17:08:07.877282Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# help(BartForConditionalGeneration.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BartForConditionalGeneration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:14.640268Z","iopub.execute_input":"2025-05-26T17:08:14.640933Z","iopub.status.idle":"2025-05-26T17:08:14.644416Z","shell.execute_reply.started":"2025-05-26T17:08:14.640910Z","shell.execute_reply":"2025-05-26T17:08:14.643684Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:16.900870Z","iopub.execute_input":"2025-05-26T17:08:16.901491Z","iopub.status.idle":"2025-05-26T17:08:24.332373Z","shell.execute_reply.started":"2025-05-26T17:08:16.901470Z","shell.execute_reply":"2025-05-26T17:08:24.331753Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3fea176286846fd96f6bc1ad6ccd455"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e089278388942eb86698f68d4b8f325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27887142129d4f289f2400b6e1c4b3b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6967d28b4d804d9faab6cc3be0eee01f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f918e5e7db204e8d8797ca9654726bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f38e917153874473b236d7f295978164"}},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"# PG&E 表示，他们安排了停电措施，是为了应对在干燥天气中预测到的大风。其目的是为了降低野火风险。\n# 预计将有近 80 万用户受到此次停电的影响，停电将至少持续到明天中午。\nARTICLE_TO_SUMMARIZE = (\n    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:29.397875Z","iopub.execute_input":"2025-05-26T17:08:29.398134Z","iopub.status.idle":"2025-05-26T17:08:29.402294Z","shell.execute_reply.started":"2025-05-26T17:08:29.398117Z","shell.execute_reply":"2025-05-26T17:08:29.401210Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:32.534582Z","iopub.execute_input":"2025-05-26T17:08:32.535294Z","iopub.status.idle":"2025-05-26T17:08:32.543199Z","shell.execute_reply.started":"2025-05-26T17:08:32.535270Z","shell.execute_reply":"2025-05-26T17:08:32.542179Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:08:42.562418Z","iopub.execute_input":"2025-05-26T17:08:42.563286Z","iopub.status.idle":"2025-05-26T17:08:42.569635Z","shell.execute_reply.started":"2025-05-26T17:08:42.563254Z","shell.execute_reply":"2025-05-26T17:08:42.568360Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0,  8332,   947,   717,  2305,    24,  1768,     5,   909,  4518,\n            11,  1263,     7,  5876,    13,   239,  2372,  2876,  3841,  1274,\n             4,    20,  4374,    16,     7,  1888,     5,   810,     9, 12584,\n             4,  9221,  5735,  7673,   916,    58,  1768,     7,    28,  2132,\n            30,     5,  2572, 10816,    61,    58,   421,     7,    94,   149,\n            23,   513, 15372,  3859,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"# 生成摘要\nsummary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:09:31.670442Z","iopub.execute_input":"2025-05-26T17:09:31.671085Z","iopub.status.idle":"2025-05-26T17:09:33.124677Z","shell.execute_reply.started":"2025-05-26T17:09:31.671061Z","shell.execute_reply":"2025-05-26T17:09:33.123925Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"summary_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:09:40.297480Z","iopub.execute_input":"2025-05-26T17:09:40.297796Z","iopub.status.idle":"2025-05-26T17:09:40.304012Z","shell.execute_reply.started":"2025-05-26T17:09:40.297774Z","shell.execute_reply":"2025-05-26T17:09:40.303273Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"tensor([[   2,    0, 8332,  947,  717, 1768,    5,  909, 4518,   11, 1263,    7,\n         5876,   13,  239, 2372, 2876, 3841, 1274,    2]])"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"# 跳过特殊token\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n# PG&E 因干燥天气中预测到的大风而安排了停电。","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:12:37.977758Z","iopub.execute_input":"2025-05-26T17:12:37.978458Z","iopub.status.idle":"2025-05-26T17:12:37.984062Z","shell.execute_reply.started":"2025-05-26T17:12:37.978437Z","shell.execute_reply":"2025-05-26T17:12:37.983334Z"}},"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'"},"metadata":{}}],"execution_count":89},{"cell_type":"code","source":"from transformers import AutoTokenizer, BartForConditionalGeneration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:12:56.140091Z","iopub.execute_input":"2025-05-26T17:12:56.140386Z","iopub.status.idle":"2025-05-26T17:12:56.144303Z","shell.execute_reply.started":"2025-05-26T17:12:56.140366Z","shell.execute_reply":"2025-05-26T17:12:56.143598Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:12:58.707207Z","iopub.execute_input":"2025-05-26T17:12:58.707751Z","iopub.status.idle":"2025-05-26T17:12:59.388488Z","shell.execute_reply.started":"2025-05-26T17:12:58.707727Z","shell.execute_reply":"2025-05-26T17:12:59.387869Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"# 我朋友们是 <mask>，但他们吃了太多碳水。\nTXT = \"My friends are <mask> but they eat too many carbs.\"\ninput_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:13:03.328871Z","iopub.execute_input":"2025-05-26T17:13:03.329155Z","iopub.status.idle":"2025-05-26T17:13:03.337183Z","shell.execute_reply.started":"2025-05-26T17:13:03.329134Z","shell.execute_reply":"2025-05-26T17:13:03.336581Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:13:15.503640Z","iopub.execute_input":"2025-05-26T17:13:15.504380Z","iopub.status.idle":"2025-05-26T17:13:15.509607Z","shell.execute_reply.started":"2025-05-26T17:13:15.504354Z","shell.execute_reply":"2025-05-26T17:13:15.508999Z"}},"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"tensor([[    0,  2387,   964,    32, 50264,    53,    51,  3529,   350,   171,\n         33237,     4,     2]])"},"metadata":{}}],"execution_count":93},{"cell_type":"code","source":"logits = model(input_ids).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:13:18.781122Z","iopub.execute_input":"2025-05-26T17:13:18.781658Z","iopub.status.idle":"2025-05-26T17:13:18.927929Z","shell.execute_reply.started":"2025-05-26T17:13:18.781635Z","shell.execute_reply":"2025-05-26T17:13:18.927313Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:13:21.203713Z","iopub.execute_input":"2025-05-26T17:13:21.203978Z","iopub.status.idle":"2025-05-26T17:13:21.209304Z","shell.execute_reply.started":"2025-05-26T17:13:21.203959Z","shell.execute_reply":"2025-05-26T17:13:21.208481Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 13, 50265])"},"metadata":{}}],"execution_count":95},{"cell_type":"code","source":"(input_ids[0] == tokenizer.mask_token_id).nonzero()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:14:20.357908Z","iopub.execute_input":"2025-05-26T17:14:20.358213Z","iopub.status.idle":"2025-05-26T17:14:20.364455Z","shell.execute_reply.started":"2025-05-26T17:14:20.358191Z","shell.execute_reply":"2025-05-26T17:14:20.363801Z"}},"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"tensor([[4]])"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:13:30.087792Z","iopub.execute_input":"2025-05-26T17:13:30.088558Z","iopub.status.idle":"2025-05-26T17:13:30.092813Z","shell.execute_reply.started":"2025-05-26T17:13:30.088508Z","shell.execute_reply":"2025-05-26T17:13:30.091992Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"masked_index # 掩码位置的索引","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:13:32.952720Z","iopub.execute_input":"2025-05-26T17:13:32.953017Z","iopub.status.idle":"2025-05-26T17:13:32.958161Z","shell.execute_reply.started":"2025-05-26T17:13:32.952997Z","shell.execute_reply":"2025-05-26T17:13:32.957435Z"}},"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":97},{"cell_type":"code","source":"logits[0, masked_index].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:15:06.994553Z","iopub.execute_input":"2025-05-26T17:15:06.995077Z","iopub.status.idle":"2025-05-26T17:15:07.001228Z","shell.execute_reply.started":"2025-05-26T17:15:06.995055Z","shell.execute_reply":"2025-05-26T17:15:07.000597Z"}},"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"torch.Size([50265])"},"metadata":{}}],"execution_count":103},{"cell_type":"code","source":"probs = logits[0, masked_index].softmax(dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:15:15.627498Z","iopub.execute_input":"2025-05-26T17:15:15.628250Z","iopub.status.idle":"2025-05-26T17:15:15.632454Z","shell.execute_reply.started":"2025-05-26T17:15:15.628226Z","shell.execute_reply":"2025-05-26T17:15:15.631668Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"probs.topk(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:15:30.786010Z","iopub.execute_input":"2025-05-26T17:15:30.786889Z","iopub.status.idle":"2025-05-26T17:15:30.792746Z","shell.execute_reply.started":"2025-05-26T17:15:30.786856Z","shell.execute_reply":"2025-05-26T17:15:30.791922Z"}},"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"torch.return_types.topk(\nvalues=tensor([0.0929, 0.0917, 0.0855, 0.0579, 0.0412], grad_fn=<TopkBackward0>),\nindices=tensor([  45,  205, 2245,  372,  182]))"},"metadata":{}}],"execution_count":106},{"cell_type":"code","source":"values, predictions = probs.topk(5) # 返回最大的5个概率,还有对应的索引","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:15:19.726853Z","iopub.execute_input":"2025-05-26T17:15:19.727327Z","iopub.status.idle":"2025-05-26T17:15:19.731252Z","shell.execute_reply.started":"2025-05-26T17:15:19.727304Z","shell.execute_reply":"2025-05-26T17:15:19.730451Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"tokenizer.decode(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:16:20.072888Z","iopub.execute_input":"2025-05-26T17:16:20.073751Z","iopub.status.idle":"2025-05-26T17:16:20.078472Z","shell.execute_reply.started":"2025-05-26T17:16:20.073725Z","shell.execute_reply":"2025-05-26T17:16:20.077903Z"}},"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"' not good healthy great very'"},"metadata":{}}],"execution_count":107},{"cell_type":"code","source":"# 这个“but”表示转折，“吃太多碳水”通常与“不健康”有关，因此前面很可能是说“他们是健康的”，但\n# 后半句指出了一个与健康相矛盾的行为。因此，“healthy”作为对比最自然。\ntokenizer.decode(predictions).split()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:16:28.733619Z","iopub.execute_input":"2025-05-26T17:16:28.734334Z","iopub.status.idle":"2025-05-26T17:16:28.739441Z","shell.execute_reply.started":"2025-05-26T17:16:28.734313Z","shell.execute_reply":"2025-05-26T17:16:28.738697Z"}},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"['not', 'good', 'healthy', 'great', 'very']"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"config.num_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:22:27.961575Z","iopub.execute_input":"2025-05-26T17:22:27.961829Z","iopub.status.idle":"2025-05-26T17:22:27.966686Z","shell.execute_reply.started":"2025-05-26T17:22:27.961812Z","shell.execute_reply":"2025-05-26T17:22:27.966002Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"# Bart模型，适用于序列分类任务（如GLUE），在 pooled output 上加了一个线性层作为分类头。\n@add_start_docstrings(\n    \"\"\"\n    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n    tasks.\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForSequenceClassification(BartPreTrainedModel):\n    # 指定权重共享的参数，encoder和decoder共享词嵌入表示\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n\n    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(config, **kwargs)\n        self.model = BartModel(config) # 初始化主模型（包含 encoder + decoder）\n        self.classification_head = BartClassificationHead( # 分类头：对encoder输出的句子表示进行分类\n            config.d_model, # 输入维度：encoder输出维度\n            config.d_model,   # 中间维度（可能用于非线性变换）\n            config.num_labels,    # 输出类别数\n            config.classifier_dropout, # dropout概率\n        )\n\n         # 初始化权重，包括 classification head 和模型内部参数\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n        output_type=Seq2SeqSequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False  # 如果有标签，表示训练阶段，禁用缓存以节省显存\n         # 当前不支持 inputs_embeds 输入方式\n        if input_ids is None and inputs_embeds is not None:\n            raise NotImplementedError(\n                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n            )\n        # 调用 BartModel 得到输出\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            encoder_outputs=encoder_outputs,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]  # encoder最后一层的输出 (b,s,d)\n        # 定位每个样本中<eos> token的位置，用于抽取句子表示\n        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n        # 每个样本必须有相同数量的eos标记，否则报错（模型设计依赖这个假设）\n        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n        # 抽取每个样本中最后一个<eos> token对应的hidden state作为句子表示\n        sentence_representation = hidden_states[eos_mask, :].view(\n            hidden_states.size(0), -1, hidden_states.size(-1))[\n            :, -1, :\n        ]\n        logits = self.classification_head(sentence_representation)  # 分类头得到 logits\n\n        loss = None # 初始化损失\n        if labels is not None: # 如果传入了labels\n            labels = labels.to(logits.device)\n            # 如果配置里没有设定问题类型\n            if self.config.problem_type is None:\n                if self.config.num_labels == 1: # 如果配置中的标签数为1,就设定问题类型为回归\n                    self.config.problem_type = \"regression\"\n                # 如果标签数>1 并且标签类型是整数型\n                elif self.config.num_labels > 1 and (\n                    labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\" # 设定为单标签分类\n                else: # 其他情况 就是标签类型不是整数的情况\n                    self.config.problem_type = \"multi_label_classification\" # 问题类型设定为多标签分类\n            # 如果是回归问题\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss() # 设定损失函数为mse\n                if self.config.num_labels == 1: # 如果标签类别是1\n                    loss = loss_fct(logits.squeeze(), labels.squeeze()) # 计算损失\n                else: # 如果是其他情况 不用squeeze\n                    loss = loss_fct(logits, labels)\n            # 如果是单标签分类问题\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss() # 损失为交叉熵损失\n                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1)) # 计算损失\n            # 如果是多标签分类问题\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss() # 设定损失为二元交叉熵损失\n                loss = loss_fct(logits, labels) # 计算损失\n        if not return_dict: # 返回元组\n            output = (logits,) + outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n        # 返回结构化输出\n        return Seq2SeqSequenceClassifierOutput(\n            loss=loss, # 损失\n            logits=logits, # 分类的logits分数\n            past_key_values=outputs.past_key_values, # past_k_v\n            decoder_hidden_states=outputs.decoder_hidden_states, \n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state, # 编码器输出\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:38:27.087685Z","iopub.execute_input":"2025-05-26T17:38:27.088221Z","iopub.status.idle":"2025-05-26T17:38:27.107150Z","shell.execute_reply.started":"2025-05-26T17:38:27.088198Z","shell.execute_reply":"2025-05-26T17:38:27.106305Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"help(BartForSequenceClassification.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 单标签分类示例：\n# import torch\nfrom transformers import AutoTokenizer, BartForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:40:20.717118Z","iopub.execute_input":"2025-05-26T17:40:20.717884Z","iopub.status.idle":"2025-05-26T17:40:20.721102Z","shell.execute_reply.started":"2025-05-26T17:40:20.717861Z","shell.execute_reply":"2025-05-26T17:40:20.720506Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\nmodel = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:40:35.595117Z","iopub.execute_input":"2025-05-26T17:40:35.595412Z","iopub.status.idle":"2025-05-26T17:41:21.194977Z","shell.execute_reply.started":"2025-05-26T17:40:35.595391Z","shell.execute_reply":"2025-05-26T17:41:21.194165Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3440809e2e09422580b38782374037ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82cbc07af6d44bafb506804b791371b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0834dc3050f54294a58633ab58b5d167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5d4a7bd201445f87019aa20f38b581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1432ea4a8744109437010f9965e11a"}},"metadata":{}},{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f0f00c389304717b9064bb6343137ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58cdf087fb754e12b199e46340fb3b1e"}},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\") # 你好，我的狗狗很可爱","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:41:21.196658Z","iopub.execute_input":"2025-05-26T17:41:21.196876Z","iopub.status.idle":"2025-05-26T17:41:21.202465Z","shell.execute_reply.started":"2025-05-26T17:41:21.196861Z","shell.execute_reply":"2025-05-26T17:41:21.201524Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:41:21.203483Z","iopub.execute_input":"2025-05-26T17:41:21.203741Z","iopub.status.idle":"2025-05-26T17:41:21.218799Z","shell.execute_reply.started":"2025-05-26T17:41:21.203717Z","shell.execute_reply":"2025-05-26T17:41:21.217851Z"}},"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"print(config.problem_type,config.num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:42:30.730792Z","iopub.execute_input":"2025-05-26T17:42:30.731051Z","iopub.status.idle":"2025-05-26T17:42:30.735147Z","shell.execute_reply.started":"2025-05-26T17:42:30.731032Z","shell.execute_reply":"2025-05-26T17:42:30.734506Z"}},"outputs":[{"name":"stdout","text":"None 3\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:42:35.643937Z","iopub.execute_input":"2025-05-26T17:42:35.644420Z","iopub.status.idle":"2025-05-26T17:42:35.932777Z","shell.execute_reply.started":"2025-05-26T17:42:35.644400Z","shell.execute_reply":"2025-05-26T17:42:35.931942Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:42:48.400011Z","iopub.execute_input":"2025-05-26T17:42:48.400279Z","iopub.status.idle":"2025-05-26T17:42:48.405152Z","shell.execute_reply.started":"2025-05-26T17:42:48.400261Z","shell.execute_reply":"2025-05-26T17:42:48.404438Z"}},"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2])"},"metadata":{}}],"execution_count":119},{"cell_type":"code","source":"predicted_class_id = logits.argmax().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:43:14.502382Z","iopub.execute_input":"2025-05-26T17:43:14.502681Z","iopub.status.idle":"2025-05-26T17:43:14.506624Z","shell.execute_reply.started":"2025-05-26T17:43:14.502659Z","shell.execute_reply":"2025-05-26T17:43:14.505939Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"predicted_class_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:43:24.438815Z","iopub.execute_input":"2025-05-26T17:43:24.439072Z","iopub.status.idle":"2025-05-26T17:43:24.444192Z","shell.execute_reply.started":"2025-05-26T17:43:24.439053Z","shell.execute_reply":"2025-05-26T17:43:24.443430Z"}},"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"model.config.id2label[predicted_class_id]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:43:32.131250Z","iopub.execute_input":"2025-05-26T17:43:32.131527Z","iopub.status.idle":"2025-05-26T17:43:32.136518Z","shell.execute_reply.started":"2025-05-26T17:43:32.131506Z","shell.execute_reply":"2025-05-26T17:43:32.135911Z"}},"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"'POSITIVE'"},"metadata":{}}],"execution_count":122},{"cell_type":"code","source":"model.config.id2label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:44:49.582284Z","iopub.execute_input":"2025-05-26T17:44:49.582587Z","iopub.status.idle":"2025-05-26T17:44:49.587675Z","shell.execute_reply.started":"2025-05-26T17:44:49.582566Z","shell.execute_reply":"2025-05-26T17:44:49.587023Z"}},"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}],"execution_count":123},{"cell_type":"code","source":"# 要在“num_labels”类上训练模型，您可以将“num_labels=num_labels”传递给“.from_pretrained(...)”\nnum_labels = len(model.config.id2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:44:51.624683Z","iopub.execute_input":"2025-05-26T17:44:51.624939Z","iopub.status.idle":"2025-05-26T17:44:51.628819Z","shell.execute_reply.started":"2025-05-26T17:44:51.624922Z","shell.execute_reply":"2025-05-26T17:44:51.628023Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\", num_labels=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:45:05.239324Z","iopub.execute_input":"2025-05-26T17:45:05.239579Z","iopub.status.idle":"2025-05-26T17:45:06.675201Z","shell.execute_reply.started":"2025-05-26T17:45:05.239562Z","shell.execute_reply":"2025-05-26T17:45:06.674683Z"}},"outputs":[{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n","output_type":"stream"}],"execution_count":125},{"cell_type":"code","source":"labels = torch.tensor([1])\nloss = model(**inputs, labels=labels).loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:45:47.770058Z","iopub.execute_input":"2025-05-26T17:45:47.770324Z","iopub.status.idle":"2025-05-26T17:45:48.054035Z","shell.execute_reply.started":"2025-05-26T17:45:47.770305Z","shell.execute_reply":"2025-05-26T17:45:48.053381Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"round(loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:45:56.859006Z","iopub.execute_input":"2025-05-26T17:45:56.859277Z","iopub.status.idle":"2025-05-26T17:45:56.864678Z","shell.execute_reply.started":"2025-05-26T17:45:56.859258Z","shell.execute_reply":"2025-05-26T17:45:56.863809Z"}},"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}],"execution_count":127},{"cell_type":"code","source":"# 多标签分类示例","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\nmodel = BartForSequenceClassification.from_pretrained(\n    \"valhalla/bart-large-sst2\", problem_type=\"multi_label_classification\") # 问题类型=多标签分类","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:47:33.113669Z","iopub.execute_input":"2025-05-26T17:47:33.114123Z","iopub.status.idle":"2025-05-26T17:47:34.324850Z","shell.execute_reply.started":"2025-05-26T17:47:33.114101Z","shell.execute_reply":"2025-05-26T17:47:34.324000Z"}},"outputs":[{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:47:41.560191Z","iopub.execute_input":"2025-05-26T17:47:41.560497Z","iopub.status.idle":"2025-05-26T17:47:41.565003Z","shell.execute_reply.started":"2025-05-26T17:47:41.560476Z","shell.execute_reply":"2025-05-26T17:47:41.564392Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:47:48.068997Z","iopub.execute_input":"2025-05-26T17:47:48.069288Z","iopub.status.idle":"2025-05-26T17:47:48.075388Z","shell.execute_reply.started":"2025-05-26T17:47:48.069268Z","shell.execute_reply":"2025-05-26T17:47:48.074639Z"}},"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:48:11.925735Z","iopub.execute_input":"2025-05-26T17:48:11.926010Z","iopub.status.idle":"2025-05-26T17:48:12.176944Z","shell.execute_reply.started":"2025-05-26T17:48:11.925988Z","shell.execute_reply":"2025-05-26T17:48:12.176249Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:48:20.136862Z","iopub.execute_input":"2025-05-26T17:48:20.137435Z","iopub.status.idle":"2025-05-26T17:48:20.143376Z","shell.execute_reply.started":"2025-05-26T17:48:20.137410Z","shell.execute_reply":"2025-05-26T17:48:20.142579Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"tensor([[-5.4279,  4.7800]])"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"torch.sigmoid(logits).squeeze(dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:49:04.529105Z","iopub.execute_input":"2025-05-26T17:49:04.529425Z","iopub.status.idle":"2025-05-26T17:49:04.535884Z","shell.execute_reply.started":"2025-05-26T17:49:04.529404Z","shell.execute_reply":"2025-05-26T17:49:04.535187Z"}},"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"tensor([0.0044, 0.9917])"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"torch.arange(0, logits.shape[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:49:42.858109Z","iopub.execute_input":"2025-05-26T17:49:42.858341Z","iopub.status.idle":"2025-05-26T17:49:42.863997Z","shell.execute_reply.started":"2025-05-26T17:49:42.858323Z","shell.execute_reply":"2025-05-26T17:49:42.863365Z"}},"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"tensor([0, 1])"},"metadata":{}}],"execution_count":134},{"cell_type":"code","source":"torch.sigmoid(logits).squeeze(dim=0) > 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:50:52.465186Z","iopub.execute_input":"2025-05-26T17:50:52.465469Z","iopub.status.idle":"2025-05-26T17:50:52.472077Z","shell.execute_reply.started":"2025-05-26T17:50:52.465447Z","shell.execute_reply":"2025-05-26T17:50:52.471289Z"}},"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"tensor([False,  True])"},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"predicted_class_ids = (torch.sigmoid(logits).squeeze(dim=0) > 0.5).long()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:55:21.577149Z","iopub.execute_input":"2025-05-26T17:55:21.577844Z","iopub.status.idle":"2025-05-26T17:55:21.581987Z","shell.execute_reply.started":"2025-05-26T17:55:21.577818Z","shell.execute_reply":"2025-05-26T17:55:21.581296Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"predicted_class_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:55:23.584637Z","iopub.execute_input":"2025-05-26T17:55:23.585169Z","iopub.status.idle":"2025-05-26T17:55:23.590421Z","shell.execute_reply.started":"2025-05-26T17:55:23.585146Z","shell.execute_reply":"2025-05-26T17:55:23.589802Z"}},"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"tensor([0, 1])"},"metadata":{}}],"execution_count":146},{"cell_type":"code","source":"num_labels = len(model.config.id2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:55:25.878152Z","iopub.execute_input":"2025-05-26T17:55:25.878405Z","iopub.status.idle":"2025-05-26T17:55:25.881926Z","shell.execute_reply.started":"2025-05-26T17:55:25.878386Z","shell.execute_reply":"2025-05-26T17:55:25.881152Z"}},"outputs":[],"execution_count":147},{"cell_type":"code","source":"num_labels ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:55:26.112977Z","iopub.execute_input":"2025-05-26T17:55:26.113567Z","iopub.status.idle":"2025-05-26T17:55:26.119012Z","shell.execute_reply.started":"2025-05-26T17:55:26.113522Z","shell.execute_reply":"2025-05-26T17:55:26.118228Z"}},"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":148},{"cell_type":"code","source":"model = BartForSequenceClassification.from_pretrained(\n        \"valhalla/bart-large-sst2\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:55:28.818424Z","iopub.execute_input":"2025-05-26T17:55:28.818943Z","iopub.status.idle":"2025-05-26T17:55:29.841933Z","shell.execute_reply.started":"2025-05-26T17:55:28.818919Z","shell.execute_reply":"2025-05-26T17:55:29.841197Z"}},"outputs":[{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n","output_type":"stream"}],"execution_count":149},{"cell_type":"code","source":"predicted_class_ids[None, :].clone()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:55:47.135026Z","iopub.execute_input":"2025-05-26T17:55:47.135572Z","iopub.status.idle":"2025-05-26T17:55:47.141047Z","shell.execute_reply.started":"2025-05-26T17:55:47.135527Z","shell.execute_reply":"2025-05-26T17:55:47.140364Z"}},"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 1]])"},"metadata":{}}],"execution_count":151},{"cell_type":"code","source":"torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:56:24.194865Z","iopub.execute_input":"2025-05-26T17:56:24.195156Z","iopub.status.idle":"2025-05-26T17:56:24.201698Z","shell.execute_reply.started":"2025-05-26T17:56:24.195137Z","shell.execute_reply":"2025-05-26T17:56:24.200988Z"}},"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"tensor([[[1, 0],\n         [0, 1]]])"},"metadata":{}}],"execution_count":152},{"cell_type":"code","source":"labels = torch.sum(\n       torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n    ).to(torch.float)\nlabels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:57:20.900309Z","iopub.execute_input":"2025-05-26T17:57:20.900592Z","iopub.status.idle":"2025-05-26T17:57:20.907408Z","shell.execute_reply.started":"2025-05-26T17:57:20.900566Z","shell.execute_reply":"2025-05-26T17:57:20.906694Z"}},"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 1.]])"},"metadata":{}}],"execution_count":154},{"cell_type":"code","source":"loss = model(**inputs, labels=labels).loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:57:30.568259Z","iopub.execute_input":"2025-05-26T17:57:30.568901Z","iopub.status.idle":"2025-05-26T17:57:30.876392Z","shell.execute_reply.started":"2025-05-26T17:57:30.568877Z","shell.execute_reply":"2025-05-26T17:57:30.875858Z"}},"outputs":[],"execution_count":155},{"cell_type":"code","source":"loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:57:37.601672Z","iopub.execute_input":"2025-05-26T17:57:37.601926Z","iopub.status.idle":"2025-05-26T17:57:37.607786Z","shell.execute_reply.started":"2025-05-26T17:57:37.601906Z","shell.execute_reply":"2025-05-26T17:57:37.607100Z"}},"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"tensor(2.7203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"},"metadata":{}}],"execution_count":156},{"cell_type":"code","source":"aa=torch.randn((2,5,2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:28:51.427476Z","iopub.execute_input":"2025-05-26T18:28:51.428248Z","iopub.status.idle":"2025-05-26T18:28:51.431880Z","shell.execute_reply.started":"2025-05-26T18:28:51.428222Z","shell.execute_reply":"2025-05-26T18:28:51.431200Z"}},"outputs":[],"execution_count":207},{"cell_type":"code","source":"aa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:28:51.837169Z","iopub.execute_input":"2025-05-26T18:28:51.837390Z","iopub.status.idle":"2025-05-26T18:28:51.843678Z","shell.execute_reply.started":"2025-05-26T18:28:51.837372Z","shell.execute_reply":"2025-05-26T18:28:51.843073Z"}},"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"tensor([[[-0.0324,  0.6286],\n         [ 0.0170, -1.5568],\n         [ 1.5358,  0.1274],\n         [-0.7292, -0.2785],\n         [ 0.4030, -0.1629]],\n\n        [[-0.5587, -0.5935],\n         [-0.0166, -1.4312],\n         [ 0.5255,  1.5923],\n         [ 1.3108, -0.2773],\n         [-2.4679,  0.2038]]])"},"metadata":{}}],"execution_count":208},{"cell_type":"code","source":"aa.split(2, dim=-1)  # 这个不管用,是一个长度为1的元组","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:28:55.122999Z","iopub.execute_input":"2025-05-26T18:28:55.123275Z","iopub.status.idle":"2025-05-26T18:28:55.129918Z","shell.execute_reply.started":"2025-05-26T18:28:55.123255Z","shell.execute_reply":"2025-05-26T18:28:55.129240Z"}},"outputs":[{"execution_count":209,"output_type":"execute_result","data":{"text/plain":"(tensor([[[-0.0324,  0.6286],\n          [ 0.0170, -1.5568],\n          [ 1.5358,  0.1274],\n          [-0.7292, -0.2785],\n          [ 0.4030, -0.1629]],\n \n         [[-0.5587, -0.5935],\n          [-0.0166, -1.4312],\n          [ 0.5255,  1.5923],\n          [ 1.3108, -0.2773],\n          [-2.4679,  0.2038]]]),)"},"metadata":{}}],"execution_count":209},{"cell_type":"code","source":"len(aa.split(2, dim=-1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:28:57.415418Z","iopub.execute_input":"2025-05-26T18:28:57.416053Z","iopub.status.idle":"2025-05-26T18:28:57.421086Z","shell.execute_reply.started":"2025-05-26T18:28:57.416027Z","shell.execute_reply":"2025-05-26T18:28:57.420350Z"}},"outputs":[{"execution_count":210,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":210},{"cell_type":"code","source":"aa.split(1, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:28:57.851626Z","iopub.execute_input":"2025-05-26T18:28:57.852263Z","iopub.status.idle":"2025-05-26T18:28:57.858949Z","shell.execute_reply.started":"2025-05-26T18:28:57.852238Z","shell.execute_reply":"2025-05-26T18:28:57.858098Z"}},"outputs":[{"execution_count":211,"output_type":"execute_result","data":{"text/plain":"(tensor([[[-0.0324],\n          [ 0.0170],\n          [ 1.5358],\n          [-0.7292],\n          [ 0.4030]],\n \n         [[-0.5587],\n          [-0.0166],\n          [ 0.5255],\n          [ 1.3108],\n          [-2.4679]]]),\n tensor([[[ 0.6286],\n          [-1.5568],\n          [ 0.1274],\n          [-0.2785],\n          [-0.1629]],\n \n         [[-0.5935],\n          [-1.4312],\n          [ 1.5923],\n          [-0.2773],\n          [ 0.2038]]]))"},"metadata":{}}],"execution_count":211},{"cell_type":"code","source":"print(type(aa.split(1, dim=-1)),len(aa.split(1, dim=-1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:29:01.497693Z","iopub.execute_input":"2025-05-26T18:29:01.497945Z","iopub.status.idle":"2025-05-26T18:29:01.502857Z","shell.execute_reply.started":"2025-05-26T18:29:01.497928Z","shell.execute_reply":"2025-05-26T18:29:01.502287Z"}},"outputs":[{"name":"stdout","text":"<class 'tuple'> 2\n","output_type":"stream"}],"execution_count":212},{"cell_type":"code","source":"(aa.split(1, dim=-1))[0].squeeze(-1).contiguous()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:29:03.856747Z","iopub.execute_input":"2025-05-26T18:29:03.857293Z","iopub.status.idle":"2025-05-26T18:29:03.863199Z","shell.execute_reply.started":"2025-05-26T18:29:03.857259Z","shell.execute_reply":"2025-05-26T18:29:03.862587Z"}},"outputs":[{"execution_count":213,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0324,  0.0170,  1.5358, -0.7292,  0.4030],\n        [-0.5587, -0.0166,  0.5255,  1.3108, -2.4679]])"},"metadata":{}}],"execution_count":213},{"cell_type":"code","source":"(aa.split(1, dim=-1))[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:29:33.360614Z","iopub.execute_input":"2025-05-26T18:29:33.360910Z","iopub.status.idle":"2025-05-26T18:29:33.366254Z","shell.execute_reply.started":"2025-05-26T18:29:33.360875Z","shell.execute_reply":"2025-05-26T18:29:33.365601Z"}},"outputs":[{"execution_count":217,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 5, 1])"},"metadata":{}}],"execution_count":217},{"cell_type":"code","source":"aa = (aa.split(1, dim=-1))[0].squeeze(-1).contiguous()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:29:35.778799Z","iopub.execute_input":"2025-05-26T18:29:35.779072Z","iopub.status.idle":"2025-05-26T18:29:35.783284Z","shell.execute_reply.started":"2025-05-26T18:29:35.779052Z","shell.execute_reply":"2025-05-26T18:29:35.782711Z"}},"outputs":[],"execution_count":218},{"cell_type":"code","source":"aa.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:29:37.911814Z","iopub.execute_input":"2025-05-26T18:29:37.912088Z","iopub.status.idle":"2025-05-26T18:29:37.917367Z","shell.execute_reply.started":"2025-05-26T18:29:37.912066Z","shell.execute_reply":"2025-05-26T18:29:37.916671Z"}},"outputs":[{"execution_count":219,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 5])"},"metadata":{}}],"execution_count":219},{"cell_type":"code","source":"aa.size(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:30:05.489107Z","iopub.execute_input":"2025-05-26T18:30:05.489839Z","iopub.status.idle":"2025-05-26T18:30:05.494903Z","shell.execute_reply.started":"2025-05-26T18:30:05.489815Z","shell.execute_reply":"2025-05-26T18:30:05.494161Z"}},"outputs":[{"execution_count":221,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}],"execution_count":221},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForQuestionAnswering(BartPreTrainedModel):\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        config.num_labels = 2 # 设置标签数为2，分别用于start位置和end位置\n        self.num_labels = config.num_labels\n\n        self.model = BartModel(config)   # 初始化底层的BART模型（encoder + decoder）\n        # QA头部，用于预测start和end位置。输出形状为 (batch_size, seq_len, 2)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n        # 初始化所有模块参数\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_QA,\n        output_type=Seq2SeqQuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_loss=_QA_EXPECTED_LOSS,\n        expected_output=_QA_EXPECTED_OUTPUT,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        # 默认使用配置文件中的return_dict设定\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # 如果提供了标签，关闭缓存机制（不保留KV缓存）\n        if start_positions is not None and end_positions is not None:\n            use_cache = False\n        # 调用BART主模型，返回包含last_hidden_state在内的多个输出\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            encoder_outputs=encoder_outputs,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]  # 获取encoder输出的hidden states，形状为 (batch, seq_len, hidden_dim)\n        # 线性层输出两个logits：start位置和end位置，形状为 (batch, seq_len, 2)\n        logits = self.qa_outputs(sequence_output)\n        # 拆分start和end的logits，并去掉最后一个维度，结果形状为 (batch, seq_len)\n        # 这个拆分后,左侧是start_logits,右侧是end_logits \n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous() # 紧凑维度\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None # 问答的损失\n        # 如果起始和结束位置都不是None\n        if start_positions is not None and end_positions is not None:\n             # 如果标签维度是 (batch, 1)，先squeeze成 (batch,)\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # 获取模型输出的最大序列长度，用于后续裁剪非法标签位置\n            ignored_index = start_logits.size(1)\n            # 将标签位置限制在 [0, 序列长度] 范围内\n            # 设计意图：部分数据集中可能存在越界标签，这里使用 clamp 保证标签合法，防止计算损失时报错\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n            # 定义交叉熵损失函数，并忽略非法的标签位置（被置为 ignored_index）\n            # 设计意图：模型输出的是每个 token 的 logits，需要用 token 索引作为 target 计算分类损失\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n             # 分别计算起始位置和结束位置的分类损失\n            start_loss = loss_fct(start_logits, start_positions)  # 答案起始位置的预测损失\n            end_loss = loss_fct(end_logits, end_positions) # 答案结束位置的预测损失\n            # 最终损失是起始位置与结束位置损失的平均\n            # 设计意图：起始与结束同等重要，平均损失作为整体优化目标\n            total_loss = (start_loss + end_loss) / 2 \n        # 返回元组\n        if not return_dict:\n            output = (\n                start_logits,\n                end_logits,\n            ) + outputs[1:]\n            return ((total_loss,) + output) if total_loss is not None else output\n        # 返回结构化输出\n        return Seq2SeqQuestionAnsweringModelOutput(\n            loss=total_loss, # 损失\n            start_logits=start_logits, # 答案开始的logits\n            end_logits=end_logits,  # 答案结束的logits\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:37:26.747000Z","iopub.execute_input":"2025-05-26T18:37:26.747288Z","iopub.status.idle":"2025-05-26T18:37:26.764239Z","shell.execute_reply.started":"2025-05-26T18:37:26.747268Z","shell.execute_reply":"2025-05-26T18:37:26.763393Z"}},"outputs":[],"execution_count":224},{"cell_type":"code","source":"help(BartForQuestionAnswering.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BartForQuestionAnswering","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:38:38.461729Z","iopub.execute_input":"2025-05-26T18:38:38.462223Z","iopub.status.idle":"2025-05-26T18:38:38.465746Z","shell.execute_reply.started":"2025-05-26T18:38:38.462197Z","shell.execute_reply":"2025-05-26T18:38:38.464992Z"}},"outputs":[],"execution_count":226},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\nmodel = BartForQuestionAnswering.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:38:52.347827Z","iopub.execute_input":"2025-05-26T18:38:52.348429Z","iopub.status.idle":"2025-05-26T18:39:05.230040Z","shell.execute_reply.started":"2025-05-26T18:38:52.348408Z","shell.execute_reply":"2025-05-26T18:39:05.229162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af609f5d832f46f9b2d8d342237496a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501eea258be745aaa7574111922db013"}},"metadata":{}},{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7562868c7104476ca9b64a05236fe148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5182d4b6f842409c8936d11ce3ab502a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"294d23f0eaa540b5b7f07d040248536f"}},"metadata":{}},{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ccc8db84414ec69b438dec6e6089e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f0657aee104b1992d8c25891f87943"}},"metadata":{}}],"execution_count":227},{"cell_type":"code","source":"question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:39:14.985572Z","iopub.execute_input":"2025-05-26T18:39:14.985865Z","iopub.status.idle":"2025-05-26T18:39:14.989833Z","shell.execute_reply.started":"2025-05-26T18:39:14.985844Z","shell.execute_reply":"2025-05-26T18:39:14.989180Z"}},"outputs":[],"execution_count":228},{"cell_type":"code","source":"inputs = tokenizer(question, text, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:39:38.041734Z","iopub.execute_input":"2025-05-26T18:39:38.042333Z","iopub.status.idle":"2025-05-26T18:39:38.046333Z","shell.execute_reply.started":"2025-05-26T18:39:38.042311Z","shell.execute_reply":"2025-05-26T18:39:38.045809Z"}},"outputs":[],"execution_count":229},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:39:45.208240Z","iopub.execute_input":"2025-05-26T18:39:45.208615Z","iopub.status.idle":"2025-05-26T18:39:45.215248Z","shell.execute_reply.started":"2025-05-26T18:39:45.208586Z","shell.execute_reply":"2025-05-26T18:39:45.214498Z"}},"outputs":[{"execution_count":230,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0, 12375,    21,  2488,   289, 13919,   116,     2,     2, 24021,\n           289, 13919,    21,    10,  2579, 29771,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":230},{"cell_type":"code","source":"with torch.no_grad():\n  outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:40:10.732071Z","iopub.execute_input":"2025-05-26T18:40:10.732825Z","iopub.status.idle":"2025-05-26T18:40:11.025481Z","shell.execute_reply.started":"2025-05-26T18:40:10.732801Z","shell.execute_reply":"2025-05-26T18:40:11.024883Z"}},"outputs":[],"execution_count":231},{"cell_type":"code","source":"answer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:40:43.726975Z","iopub.execute_input":"2025-05-26T18:40:43.727528Z","iopub.status.idle":"2025-05-26T18:40:43.731811Z","shell.execute_reply.started":"2025-05-26T18:40:43.727504Z","shell.execute_reply":"2025-05-26T18:40:43.730868Z"}},"outputs":[],"execution_count":233},{"cell_type":"code","source":"print(answer_start_index,answer_end_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:41:01.390076Z","iopub.execute_input":"2025-05-26T18:41:01.390379Z","iopub.status.idle":"2025-05-26T18:41:01.395446Z","shell.execute_reply.started":"2025-05-26T18:41:01.390349Z","shell.execute_reply":"2025-05-26T18:41:01.394922Z"}},"outputs":[{"name":"stdout","text":"tensor(14) tensor(15)\n","output_type":"stream"}],"execution_count":234},{"cell_type":"code","source":"predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:41:19.476346Z","iopub.execute_input":"2025-05-26T18:41:19.477017Z","iopub.status.idle":"2025-05-26T18:41:19.481126Z","shell.execute_reply.started":"2025-05-26T18:41:19.476985Z","shell.execute_reply":"2025-05-26T18:41:19.480181Z"}},"outputs":[],"execution_count":235},{"cell_type":"code","source":"tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:41:28.873210Z","iopub.execute_input":"2025-05-26T18:41:28.873498Z","iopub.status.idle":"2025-05-26T18:41:28.878728Z","shell.execute_reply.started":"2025-05-26T18:41:28.873476Z","shell.execute_reply":"2025-05-26T18:41:28.878015Z"}},"outputs":[{"execution_count":236,"output_type":"execute_result","data":{"text/plain":"' nice puppet'"},"metadata":{}}],"execution_count":236},{"cell_type":"code","source":"# target is \"nice puppet\"\ntarget_start_index = torch.tensor([14])\ntarget_end_index = torch.tensor([15])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:42:06.817515Z","iopub.execute_input":"2025-05-26T18:42:06.817825Z","iopub.status.idle":"2025-05-26T18:42:06.822079Z","shell.execute_reply.started":"2025-05-26T18:42:06.817804Z","shell.execute_reply":"2025-05-26T18:42:06.821278Z"}},"outputs":[],"execution_count":237},{"cell_type":"code","source":"outputs = model(\n    **inputs, start_positions=target_start_index, end_positions=target_end_index)\nloss = outputs.loss\nround(loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:42:35.532773Z","iopub.execute_input":"2025-05-26T18:42:35.533046Z","iopub.status.idle":"2025-05-26T18:42:35.861519Z","shell.execute_reply.started":"2025-05-26T18:42:35.533028Z","shell.execute_reply":"2025-05-26T18:42:35.860816Z"}},"outputs":[{"execution_count":238,"output_type":"execute_result","data":{"text/plain":"0.59"},"metadata":{}}],"execution_count":238},{"cell_type":"code","source":"# 该包装类用于在与 EncoderDecoderModel 框架结合使用时，正确加载 BART 的解码器权重。\n# 设计意图是将 BART 解码器封装为一个独立模块，使其能被统一接口调用并加载预训练检查点。\nclass BartDecoderWrapper(BartPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        # 初始化 BART 解码器组件，传入配置对象\n        # 此处不包括编码器部分，仅构造解码器，便于与其他编码器组合使用\n        self.decoder = BartDecoder(config)\n    # 将所有输入直接传给解码器的前向函数，确保与 BARTDecoder 接口一致\n    def forward(self, *args, **kwargs):\n        return self.decoder(*args, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:45:32.469819Z","iopub.execute_input":"2025-05-26T18:45:32.470348Z","iopub.status.idle":"2025-05-26T18:45:32.474818Z","shell.execute_reply.started":"2025-05-26T18:45:32.470326Z","shell.execute_reply":"2025-05-26T18:45:32.474065Z"}},"outputs":[],"execution_count":239},{"cell_type":"code","source":"# BART 解码器 + 语言建模头（lm_head），用于自回归文本生成任务。lm_head 权重与输入嵌入权重共享。\n@add_start_docstrings(\n    \"\"\"\n    BART decoder with a language modeling head on top (linear layer with weights tied to the input embeddings).\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForCausalLM(BartPreTrainedModel, GenerationMixin):\n    _tied_weights_keys = [\"lm_head.weight\"]  # 指定哪些权重需要与其它模块（如嵌入层）共享\n\n    def __init__(self, config):\n        config = copy.deepcopy(config)\n        config.is_decoder = True # 明确当前模型只作为 decoder 使用\n        config.is_encoder_decoder = False # 禁止使用 encoder-decoder 模式\n        super().__init__(config)\n        # 包装解码器结构，隐藏底层 decoder 实现细节\n        self.model = BartDecoderWrapper(config)\n         # 输出层：将 decoder 的隐藏状态映射为词表 logits\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # 初始化所有模块参数\n        self.post_init()\n     # 获取输入嵌入层（与 decoder 的嵌入层一致）\n    def get_input_embeddings(self):\n        return self.model.decoder.embed_tokens\n     # 设置 decoder 的嵌入层（用于 weight tying）\n    def set_input_embeddings(self, value):\n        self.model.decoder.embed_tokens = value\n    \n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n    # 设置解码器\n    def set_decoder(self, decoder):\n        self.model.decoder = decoder\n    # 获取解码器\n    def get_decoder(self):\n        return self.model.decoder\n\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                if the model is configured as a decoder.\n            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, BartForCausalLM\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n        >>> model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n        >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n        >>> outputs = model(**inputs)\n\n        >>> logits = outputs.logits\n        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n        >>> list(logits.shape) == expected_shape\n        True\n        ```\"\"\"\n        # 如果未显式指定，使用 config 中默认值\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # 解码器输出由（dec_features、layer_state、dec_hidden、dec_attn）组成\n        # 解码器前向传播，返回 decoder 的输出（包含隐藏状态、注意力等）\n        outputs = self.model.decoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            head_mask=head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        # 使用语言模型头将隐藏状态映射为 logits（词表维度）\n        logits = self.lm_head(outputs[0])\n\n        loss = None # 初始损失\n        if labels is not None:\n            labels = labels.to(logits.device)\n            loss_fct = CrossEntropyLoss() # 设置损失为交叉熵\n            # 将 logits 和 labels 展平后计算损失（忽略 label=-100 的位置）\n            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\n        # 根据 return_dict 决定返回格式\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n        # 返回带有结构化字段的输出（包含 logits, loss, past key values 等）\n        return CausalLMOutputWithCrossAttentions(\n            loss=loss, # 损失\n            logits=logits, # 下个token的概率分布\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    @staticmethod # 静态方法\n    def _reorder_cache(past_key_values, beam_idx):\n        # 在 beam search 解码过程中重排序 past_key_values\n        # 每层的 past_state 维度为 (batch, num_heads, seq_len, head_dim)，根据 beam_idx 选择新的顺序\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T19:02:56.294616Z","iopub.execute_input":"2025-05-26T19:02:56.295213Z","iopub.status.idle":"2025-05-26T19:02:56.311281Z","shell.execute_reply.started":"2025-05-26T19:02:56.295190Z","shell.execute_reply":"2025-05-26T19:02:56.310440Z"}},"outputs":[],"execution_count":263},{"cell_type":"code","source":"help(BartForCausalLM.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:55:08.316984Z","iopub.execute_input":"2025-05-26T18:55:08.317646Z","iopub.status.idle":"2025-05-26T18:55:08.321217Z","shell.execute_reply.started":"2025-05-26T18:55:08.317623Z","shell.execute_reply":"2025-05-26T18:55:08.320482Z"}},"outputs":[],"execution_count":242},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n# model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:55:34.990663Z","iopub.execute_input":"2025-05-26T18:55:34.991397Z","iopub.status.idle":"2025-05-26T18:55:35.338565Z","shell.execute_reply.started":"2025-05-26T18:55:34.991376Z","shell.execute_reply":"2025-05-26T18:55:35.337994Z"}},"outputs":[],"execution_count":243},{"cell_type":"code","source":"model=BartForCausalLM(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T19:01:38.355101Z","iopub.execute_input":"2025-05-26T19:01:38.355628Z","iopub.status.idle":"2025-05-26T19:01:43.280968Z","shell.execute_reply.started":"2025-05-26T19:01:38.355605Z","shell.execute_reply":"2025-05-26T19:01:43.280217Z"}},"outputs":[],"execution_count":262},{"cell_type":"code","source":"model.config.is_decoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:56:46.839742Z","iopub.execute_input":"2025-05-26T18:56:46.839990Z","iopub.status.idle":"2025-05-26T18:56:46.845097Z","shell.execute_reply.started":"2025-05-26T18:56:46.839973Z","shell.execute_reply":"2025-05-26T18:56:46.844324Z"}},"outputs":[{"execution_count":246,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":246},{"cell_type":"code","source":"model.__class__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:57:12.667513Z","iopub.execute_input":"2025-05-26T18:57:12.668105Z","iopub.status.idle":"2025-05-26T18:57:12.672947Z","shell.execute_reply.started":"2025-05-26T18:57:12.668066Z","shell.execute_reply":"2025-05-26T18:57:12.672292Z"}},"outputs":[{"execution_count":247,"output_type":"execute_result","data":{"text/plain":"__main__.BartForCausalLM"},"metadata":{}}],"execution_count":247},{"cell_type":"code","source":"# 断言\nassert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:57:15.290309Z","iopub.execute_input":"2025-05-26T18:57:15.290627Z","iopub.status.idle":"2025-05-26T18:57:15.294343Z","shell.execute_reply.started":"2025-05-26T18:57:15.290607Z","shell.execute_reply":"2025-05-26T18:57:15.293513Z"}},"outputs":[],"execution_count":248},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:57:24.291257Z","iopub.execute_input":"2025-05-26T18:57:24.291997Z","iopub.status.idle":"2025-05-26T18:57:24.296150Z","shell.execute_reply.started":"2025-05-26T18:57:24.291972Z","shell.execute_reply":"2025-05-26T18:57:24.295460Z"}},"outputs":[],"execution_count":249},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:57:31.782102Z","iopub.execute_input":"2025-05-26T18:57:31.782400Z","iopub.status.idle":"2025-05-26T18:57:31.788980Z","shell.execute_reply.started":"2025-05-26T18:57:31.782381Z","shell.execute_reply":"2025-05-26T18:57:31.788316Z"}},"outputs":[{"execution_count":250,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":250},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:57:41.867135Z","iopub.execute_input":"2025-05-26T18:57:41.867425Z","iopub.status.idle":"2025-05-26T18:57:42.008402Z","shell.execute_reply.started":"2025-05-26T18:57:41.867406Z","shell.execute_reply":"2025-05-26T18:57:42.007728Z"}},"outputs":[],"execution_count":251},{"cell_type":"code","source":"outputs.logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:58:02.596105Z","iopub.execute_input":"2025-05-26T18:58:02.596865Z","iopub.status.idle":"2025-05-26T18:58:02.601432Z","shell.execute_reply.started":"2025-05-26T18:58:02.596844Z","shell.execute_reply":"2025-05-26T18:58:02.600711Z"}},"outputs":[{"execution_count":253,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 50265])"},"metadata":{}}],"execution_count":253},{"cell_type":"code","source":"logits = outputs.logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:58:16.095002Z","iopub.execute_input":"2025-05-26T18:58:16.095275Z","iopub.status.idle":"2025-05-26T18:58:16.099125Z","shell.execute_reply.started":"2025-05-26T18:58:16.095255Z","shell.execute_reply":"2025-05-26T18:58:16.098297Z"}},"outputs":[],"execution_count":254},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:58:41.182980Z","iopub.execute_input":"2025-05-26T18:58:41.183684Z","iopub.status.idle":"2025-05-26T18:58:41.188310Z","shell.execute_reply.started":"2025-05-26T18:58:41.183658Z","shell.execute_reply":"2025-05-26T18:58:41.187572Z"}},"outputs":[{"execution_count":256,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 50265])"},"metadata":{}}],"execution_count":256},{"cell_type":"code","source":"inputs.input_ids.shape[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:59:03.343614Z","iopub.execute_input":"2025-05-26T18:59:03.344382Z","iopub.status.idle":"2025-05-26T18:59:03.348968Z","shell.execute_reply.started":"2025-05-26T18:59:03.344348Z","shell.execute_reply":"2025-05-26T18:59:03.348323Z"}},"outputs":[{"execution_count":257,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}],"execution_count":257},{"cell_type":"code","source":"model.config.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:59:13.817142Z","iopub.execute_input":"2025-05-26T18:59:13.817422Z","iopub.status.idle":"2025-05-26T18:59:13.822806Z","shell.execute_reply.started":"2025-05-26T18:59:13.817403Z","shell.execute_reply":"2025-05-26T18:59:13.821877Z"}},"outputs":[{"execution_count":258,"output_type":"execute_result","data":{"text/plain":"50265"},"metadata":{}}],"execution_count":258},{"cell_type":"code","source":"expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size] # 预期形状","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:59:16.562936Z","iopub.execute_input":"2025-05-26T18:59:16.563240Z","iopub.status.idle":"2025-05-26T18:59:16.567162Z","shell.execute_reply.started":"2025-05-26T18:59:16.563218Z","shell.execute_reply":"2025-05-26T18:59:16.566544Z"}},"outputs":[],"execution_count":259},{"cell_type":"code","source":"list(logits.shape) == expected_shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:59:24.731466Z","iopub.execute_input":"2025-05-26T18:59:24.732242Z","iopub.status.idle":"2025-05-26T18:59:24.736868Z","shell.execute_reply.started":"2025-05-26T18:59:24.732219Z","shell.execute_reply":"2025-05-26T18:59:24.736199Z"}},"outputs":[{"execution_count":260,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":260},{"cell_type":"code","source":"__all__ = [\n    \"BartForCausalLM\",\n    \"BartForConditionalGeneration\",\n    \"BartForQuestionAnswering\",\n    \"BartForSequenceClassification\",\n    \"BartModel\",\n    \"BartPreTrainedModel\",\n    \"BartPretrainedModel\",\n    \"PretrainedBartModel\",\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}