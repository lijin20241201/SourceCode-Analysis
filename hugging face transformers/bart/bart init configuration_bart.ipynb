{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import TYPE_CHECKING # 类型检查\n\nfrom transformers.utils import _LazyModule\nfrom transformers.utils.import_utils import define_import_structure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:38:27.312348Z","iopub.execute_input":"2025-05-25T01:38:27.312633Z","iopub.status.idle":"2025-05-25T01:38:33.940201Z","shell.execute_reply.started":"2025-05-25T01:38:27.312613Z","shell.execute_reply":"2025-05-25T01:38:33.939095Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"define_import_structure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:38:36.679367Z","iopub.execute_input":"2025-05-25T01:38:36.679845Z","iopub.status.idle":"2025-05-25T01:38:36.687456Z","shell.execute_reply.started":"2025-05-25T01:38:36.679818Z","shell.execute_reply":"2025-05-25T01:38:36.686439Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<function transformers.utils.import_utils.define_import_structure(module_path: str) -> Dict[FrozenSet[str], Dict[str, Set[str]]]>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"TYPE_CHECKING","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:38:39.155449Z","iopub.execute_input":"2025-05-25T01:38:39.155816Z","iopub.status.idle":"2025-05-25T01:38:39.161444Z","shell.execute_reply.started":"2025-05-25T01:38:39.155791Z","shell.execute_reply":"2025-05-25T01:38:39.160648Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"if TYPE_CHECKING: # 静态检查工具检测时\n    from transformers.models.bart.configuration_bart import *\n    from transformers.models.bart.modeling_bart import *\n    from transformers.models.bart.modeling_flax_bart import *\n    from transformers.models.bart.modeling_tf_bart import *\n    from transformers.models.bart.tokenization_bart import *\n    from transformers.models.bart.tokenization_bart_fast import *\nelse: # 否则,懒加载\n    import sys\n    _file = globals()[\"__file__\"]\n    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings  # 用于显示警告信息（如弃用警告）\nfrom collections import OrderedDict # 保持字典中键值对的顺序（在构建有序模型配置等场景下常用）\n # 类型提示：Any 表示任意类型，Mapping 表示映射类型，Optional 表示可选（可以是某类型或 None）\nfrom typing import Any, Mapping, Optional\n# HuggingFace 的预训练分词器基类，用于文本与 token 之间的转换\nfrom transformers import PreTrainedTokenizer\n# 模型配置类的基类，处理模型的参数定义和序列化\nfrom transformers.configuration_utils import PretrainedConfig\n# ONNX 导出相关的配置类：\n# OnnxConfig：基本导出配置类\n# OnnxConfigWithPast：支持 past key-value（通常用于自回归模型）\n# OnnxSeq2SeqConfigWithPast：用于支持 encoder-decoder 架构导出，且带有 past 支持\nfrom transformers.onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n# 工具函数，用于计算某一维的有效大小（可能用于 batch size 或 sequence length 推断）\nfrom transformers.onnx.utils import compute_effective_axis_dimension\n# TensorType：用于指明输入的张量类型（如 PyTorch、TensorFlow）\n# is_torch_available：判断是否安装了 PyTorch\n# logging：Transformers 的日志记录工具（比标准 logging 更一致）\nfrom transformers.utils import TensorType, is_torch_available, logging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:38:55.150638Z","iopub.execute_input":"2025-05-25T01:38:55.150941Z","iopub.status.idle":"2025-05-25T01:38:55.328989Z","shell.execute_reply.started":"2025-05-25T01:38:55.150921Z","shell.execute_reply":"2025-05-25T01:38:55.327970Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"is_torch_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:38:57.600950Z","iopub.execute_input":"2025-05-25T01:38:57.601257Z","iopub.status.idle":"2025-05-25T01:38:57.607659Z","shell.execute_reply.started":"2025-05-25T01:38:57.601234Z","shell.execute_reply":"2025-05-25T01:38:57.606833Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"logger = logging.get_logger(__name__) # 日志对象","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:38:59.998175Z","iopub.execute_input":"2025-05-25T01:38:59.998534Z","iopub.status.idle":"2025-05-25T01:39:00.004980Z","shell.execute_reply.started":"2025-05-25T01:38:59.998508Z","shell.execute_reply":"2025-05-25T01:39:00.003633Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 推理时产生的 past_key_values 不参与模型最终输出的字典中计算 loss 或评估结果，也就是说它是中间\n# 缓存结果，用于加速自回归生成的，不用于模型最终输出的结果比较或后处理阶段。\n# 我们来精要解释：\n#  keys_to_ignore_at_inference 是什么？\n# 这是 Hugging Face 的 PretrainedConfig 提供的机制，指示在 model.generate() 或类似推理过程中\n# ，模型返回的 output 字典中，这些 key 会被自动忽略，常用于评估、序列解码等场景。\n# past_key_values 是什么？\n# 这是 Transformer 解码器缓存的 attention key 和 value，用于加速自回归生成。在生成第 t 步时，\n# 只需计算当前 token 的注意力，而不是每次都重复前面的 token。\n#  为什么要忽略？\n# 在推理时它是为了效率存在的，不参与模型主任务（如文本分类、问答等）中的最终输出。指定它为 \n# keys_to_ignore_at_inference 的目的：\n# 防止影响评估指标或日志记录\n# 避免与训练阶段输出做比较时引入多余字段\n# ✅ 总结一句话：\n# \"past_key_values\" 被加入 keys_to_ignore_at_inference 表示：它是模型推理时的中间缓存，\n# 参与生成，不参与最终输出或结果计算。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BART 模型的配置类，用于定义模型结构参数，实例化配置后可用于初始化 BART 模型。\n# 默认参数对应 [facebook/bart-large] 架构。\nclass BartConfig(PretrainedConfig):\n    \n    model_type = \"bart\" # 指定模型类型\n    keys_to_ignore_at_inference = [\"past_key_values\"]  # 推理时忽略的键\n    # 属性映射:外部统一接口映射到内部参数名\n    attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n\n    def __init__(\n        self,\n        vocab_size=50265, # 词表大小\n        max_position_embeddings=1024,  # 最大支持的序列长度\n        encoder_layers=12,  # 编码器层数\n        encoder_ffn_dim=4096,  # 编码器前馈层维度\n        encoder_attention_heads=16,  # 编码器 attention 头数\n        decoder_layers=12, # 解码器层数\n        decoder_ffn_dim=4096,  # 解码器前馈层维度\n        decoder_attention_heads=16,  # 解码器 attention 头数\n        encoder_layerdrop=0.0,  # 编码器的 LayerDrop 概率\n        decoder_layerdrop=0.0,  # 解码器的 LayerDrop 概率\n        activation_function=\"gelu\",  # 激活函数，支持 gelu/relu/silu/gelu_new\n        d_model=1024,  # 模型隐藏维度，也是 embedding 和 attention 输出的维度\n        dropout=0.1,  # 全连接层 dropout 概率\n        attention_dropout=0.0,   # attention score 的 dropout\n        activation_dropout=0.0,  # 激活函数之后的 dropout\n        init_std=0.02,  # 权重初始化标准差（截断正态分布）\n        classifier_dropout=0.0,  # 分类头 dropout（用于下游任务）\n        scale_embedding=False, # 是否对 embedding 缩放（除以 sqrt(d_model)）\n        use_cache=True, # 推理时是否缓存 KV，用于加速自回归生成\n        num_labels=3, # 用于分类任务的 label 数（例如 BartForSequenceClassification）\n        pad_token_id=1, # pad token id\n        bos_token_id=0, # 起始 token id\n        eos_token_id=2,  # 结束 token id\n        is_encoder_decoder=True,  # 指明该模型为 encoder-decoder 架构\n        decoder_start_token_id=2,   # 解码器开始 token\n        forced_eos_token_id=2,  # 强制生成结束时最后一个 token\n        **kwargs,\n    ):\n        # 保存参数为实例变量\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.d_model = d_model\n        self.encoder_ffn_dim = encoder_ffn_dim\n        self.encoder_layers = encoder_layers\n        self.encoder_attention_heads = encoder_attention_heads\n        self.decoder_ffn_dim = decoder_ffn_dim\n        self.decoder_layers = decoder_layers\n        self.decoder_attention_heads = decoder_attention_heads\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.activation_dropout = activation_dropout\n        self.activation_function = activation_function\n        self.init_std = init_std\n        self.encoder_layerdrop = encoder_layerdrop\n        self.decoder_layerdrop = decoder_layerdrop\n        self.classifier_dropout = classifier_dropout\n        self.use_cache = use_cache\n        self.num_hidden_layers = encoder_layers  # 兼容部分接口用法\n        self.scale_embedding = scale_embedding  # 如果为 True，比例因子将为 sqrt(d_model)\n         # 调用父类构造，传入特殊 token 和结构配置\n        super().__init__(\n            num_labels=num_labels,\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            is_encoder_decoder=is_encoder_decoder,\n            decoder_start_token_id=decoder_start_token_id,\n            forced_eos_token_id=forced_eos_token_id,\n            **kwargs,\n        )\n\n        # 兼容旧版 CNN 模型中强制生成 BOS token 的逻辑\n        if self.forced_bos_token_id is None and kwargs.get(\"force_bos_token_to_be_generated\", False):\n            self.forced_bos_token_id = self.bos_token_id # 设置强制BOS token id\n            warnings.warn(\n                f\"Please make sure the config includes `forced_bos_token_id={self.bos_token_id}` in future versions. \"\n                \"The config can simply be saved and uploaded again to be fixed.\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:39:03.013922Z","iopub.execute_input":"2025-05-25T01:39:03.014248Z","iopub.status.idle":"2025-05-25T01:39:03.025741Z","shell.execute_reply.started":"2025-05-25T01:39:03.014223Z","shell.execute_reply":"2025-05-25T01:39:03.024594Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import BartModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:39:10.444275Z","iopub.execute_input":"2025-05-25T01:39:10.444690Z","iopub.status.idle":"2025-05-25T01:39:39.581589Z","shell.execute_reply.started":"2025-05-25T01:39:10.444641Z","shell.execute_reply":"2025-05-25T01:39:39.580431Z"}},"outputs":[{"name":"stderr","text":"2025-05-25 01:39:22.661649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748137162.932442      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748137163.008479      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Initializing a BART facebook/bart-large style configuration\nconfiguration = BartConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:39:39.583334Z","iopub.execute_input":"2025-05-25T01:39:39.584082Z","iopub.status.idle":"2025-05-25T01:39:39.589259Z","shell.execute_reply.started":"2025-05-25T01:39:39.584054Z","shell.execute_reply":"2025-05-25T01:39:39.588022Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = BartModel(configuration)\nconfiguration = model.config\n# \"_attn_implementation_autoset\": true 自动设置底层注意力实现方式，通常无须手动控制。\nconfiguration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:39:39.590540Z","iopub.execute_input":"2025-05-25T01:39:39.590931Z","iopub.status.idle":"2025-05-25T01:39:48.912352Z","shell.execute_reply.started":"2025-05-25T01:39:39.590898Z","shell.execute_reply":"2025-05-25T01:39:48.911337Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BartConfig {\n  \"_attn_implementation_autoset\": true,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_eos_token_id\": 2,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# ✅ 功能说明汇总：\n# 主要用途：用于导出ONNX模型或集成推理框架时明确每个输入张量维度的语义（如 batch、sequence 长度），便于动态维度标记与图优化。\n# self.task：控制当前模型是 seq2seq（如BART）还是 causal-lm（如GPT）。\n# self.use_past：控制是否使用 KV 缓存（适用于生成时加速解码）。\n# fill_with_past_key_values_：负责动态添加各层的 past key/value 输入定义。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BartOnnxConfig(OnnxSeq2SeqConfigWithPast):\n    # 返回模型的输入映射（input name → 维度索引 → 语义名称），用于推理引擎（如 ONNX）进行维度标记。\n    # 返回值:一个有序字典，key 是输入名（如 \"input_ids\"），value 是另一个 dict，映射输入张量的维度\n    # （int）到语义名称（str），如 \"batch\"、\"sequence\" 等。\n    @property\n    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n        # 对于 seq2seq 任务\n        if self.task in [\"default\", \"seq2seq-lm\"]:\n            # 定义通用 encoder 部分输入（输入张量的维度语义）\n            common_inputs = OrderedDict(\n                [\n                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),  # 编码器输入id，形状 (batch, seq_len)\n                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),  # 编码器注意力mask，同上\n                ]\n            )\n             # 启用了 past_key_values（KV缓存机制），表示 decoder 使用的是缓存形式\n            # decoder_input_ids 不再需要完整序列，只输入最后一个token\n            if self.use_past:\n                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}  # 解码器输入仅为单步 (batch,)\n                # 注意力 mask 维度需支持累积长度（past + 当前）0是批次,1是序列轴 掩码输入就是(b,s)\n                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n            else: # 普通 decoder 输入情况，输入完整序列\n                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}   # (batch, seq_len)\n                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n            # 为 KV 缓存填充历史key/value输入信息（动态添加past_key_values）\n            if self.use_past:\n                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n        # 对于 causal LM 模型，如 GPT、OPT（只含decoder）\n        elif self.task == \"causal-lm\":\n            # encoder_sequence 实际表示自回归序列（兼容GPT的输入）\n            common_inputs = OrderedDict(\n                [\n                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n                ]\n            )\n            if self.use_past: # 提取层数（注意 causal-lm 模型只有decoder部分，所以只需decoder层数）\n                num_encoder_layers, _ = self.num_layers\n                for i in range(num_encoder_layers):\n                    # past key 和 value 的时间维度索引为 2，维度名为 past_sequence + sequence\n                    # 说明这两个张量是 (batch, num_heads, total_seq_len, head_dim)\n                    # 0是批次,2是序列维度,因为这里的past_key_values一般是(b,h,s,dk) 2才是需要标记的序列轴\n                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n        else: # fallback 默认行为：返回完整decoder序列输入\n            common_inputs = OrderedDict(\n                [\n                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n                ]\n            )\n\n        return common_inputs\n    # 该 @property def outputs(...) 方法定义了模型导出时的 输出张量的命名与维度语义映射，用于 ONNX 结构化定义，使导\n    # 出的模型能兼容不同的推理场景（如普通解码和 KV-cache 加速的解码）。\n    # 定义 outputs 属性，返回一个映射（如 OrderedDict），描述输出张量的 每个维度的语义含义，供 ONNX 导出使用。\n    @property\n    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n        if self.task in [\"default\", \"seq2seq-lm\"]: # 对于 seq2seq-lm 或默认任务：\n            common_outputs = super().outputs # 直接调用父类的输出定义，不做额外处理。\n        else:  # 对于其他任务（如 causal-lm）\n            # 仍调用父类，但此处用的是显示的类名调用方式，兼容某些继承链可能复杂的情况。：\n            common_outputs = super(OnnxConfigWithPast, self).outputs\n            # 添加支持 KV 缓存的 present key/value 输出：\n            # 如果模型支持 use_past，则表示启用 KV 缓存 推理优化。\n            if self.use_past:\n                num_encoder_layers, _ = self.num_layers\n                # 循环每一层，为当前层的 present key 和 present value 添加输出\n                # 这些 present.key/value 是下一步推理的 past_key_values 的输入，即缓存的 attention KV。\n                # 输出的 present.{i}.key/value 张量 shape 为 (batch_size, num_heads, total_seq_len, head_dim)\n                # 因此第 0 轴为 batch，第 2 轴为时间维度，所以标注 {0: \"batch\", 2: \"past_sequence + sequence\"}。\n                for i in range(num_encoder_layers):\n                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n        return common_outputs\n    # 生成支持 Seq2Seq（Encoder-Decoder）架构 的模型导出所需的 dummy inputs\n    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n        self,\n        tokenizer: PreTrainedTokenizer, # 用于生成 token 级输入的 tokenizer\n        batch_size: int = -1, # dummy 输入的批大小\n        seq_length: int = -1, # 输入序列长度\n        is_pair: bool = False, # 是否是句对（如问答），用于控制 tokenizer 的输入模式\n        framework: Optional[TensorType] = None, # 所用后端框架类型（如 TensorFlow、PyTorch）\n    ) -> Mapping[str, Any]:\n        # 重用用于 classification/QA 的通用接口，生成 input_ids、attention_mask 等\n        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n            tokenizer, batch_size, seq_length, is_pair, framework\n        )\n\n        # 解码器输入生成：若启用 KV 缓存，只需一个 token（单步解码），否则用完整序列。\n        decoder_seq_length = seq_length if not self.use_past else 1\n        # 同样重用 QA/classification 方法生成 decoder 相关输入，并重命名 key 为 decoder_input_ids 等。\n        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n            tokenizer, batch_size, decoder_seq_length, is_pair, framework\n        )\n        \n        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n        common_inputs = dict(**encoder_inputs, **decoder_inputs) # 合并 encoder 和 decoder 的输入字典。\n        # 构造 past_key_values（若开启 use_past）： KV cache 仅支持在 PyTorch 中构造，若未安装则报错。\n        if self.use_past:\n            if not is_torch_available():\n                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n            else:\n                import torch\n            # 获取编码器和解码器输入的实际维度。\n            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n            # 获取 encoder 和 decoder 的 attention 头数，用于确定 KV 缓存的形状。\n            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n            # 计算 past key/value 的形状：Encoder KV 缓存张量形状：(B, H, L, D)，其中 D = hidden_size // H\n            encoder_shape = (\n                batch,\n                num_encoder_attention_heads,\n                encoder_seq_length,\n                self._config.hidden_size // num_encoder_attention_heads,\n            )\n            # 历史序列长度为 decoder_seq_length + 3（特殊token）\n            decoder_past_length = decoder_seq_length + 3\n            decoder_shape = (\n                batch,\n                num_decoder_attention_heads,\n                decoder_past_length,\n                self._config.hidden_size // num_decoder_attention_heads,\n            )\n            # 拼接解码器 attention_mask 以支持 past：\n            # 对解码器 attention_mask 添加 past 部分，使其支持完整长度（past + 当前）\n            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n            )\n            # 构造 past_key_values：\n            common_inputs[\"past_key_values\"] = []\n            # 初始化 past_key_values 列表 获取 encoder 和 decoder 的层数，并处理它们层数不一致的情况\n            num_encoder_layers, num_decoder_layers = self.num_layers\n            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n            # 构造（最小层数）内的4元组 past：\n            # 每一层有 4 个 tensor：decoder 的 key/value 和 encoder 的 key/value\n            for _ in range(min_num_layers):\n                common_inputs[\"past_key_values\"].append(\n                    (\n                        torch.zeros(decoder_shape), # decoder.key\n                        torch.zeros(decoder_shape), # decoder.value\n                        torch.zeros(encoder_shape), # encoder.key\n                        torch.zeros(encoder_shape),  # encoder.value\n                    )\n                )\n            # 构造（剩余层）内的2元组 past：\n            # 如果 encoder 和 decoder 层数不等，需要补齐多出的层，仅有 key/value 对，不再区分 encoder/decoder。\n            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n            for _ in range(min_num_layers, max_num_layers):\n                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n        # 最终返回结构：\n        # 包含 input_ids、attention_mask、decoder_input_ids、decoder_attention_mask\n        # 若启用 use_past，还包含 past_key_values（一个 list，每层一个 tuple）\n        return common_inputs\n\n    def _generate_dummy_inputs_for_causal_lm(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        batch_size: int = -1,\n        seq_length: int = -1,\n        is_pair: bool = False,\n        framework: Optional[TensorType] = None,\n    ) -> Mapping[str, Any]:\n        # 通用方法：复用用于分类/问答任务的 dummy 输入生成逻辑\n        # 主要生成 input_ids、attention_mask 等基础输入\n        common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n            tokenizer, batch_size, seq_length, is_pair, framework\n        )\n        # 若模型支持 KV 缓存（use_past=True），则构造 past_key_values 和扩展 attention_mask\n        if self.use_past:\n            # past_key_values 构造仅支持 PyTorch 后端\n            if not is_torch_available():\n                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n            else:\n                import torch\n             # 获取当前输入的 batch 和序列长度\n            batch, seqlen = common_inputs[\"input_ids\"].shape\n            # 假设历史缓存长度为当前长度 + 2，用于测试 past key/value 的拼接兼容性\n            past_key_values_length = seqlen + 2\n            # 取出 encoder 层数和注意力头数（GPT 等 causal LM 通常只有 encoder）\n            num_encoder_layers, _ = self.num_layers\n            num_encoder_attention_heads, _ = self.num_attention_heads\n            # 构造 past_key/past_value 的张量形状：(batch, num_heads, past_len, head_dim)\n            past_shape = (\n                batch,\n                num_encoder_attention_heads,\n                past_key_values_length,\n                self._config.hidden_size // num_encoder_attention_heads,\n            )\n            # 获取 attention_mask 的数据类型，保证拼接后类型一致\n            mask_dtype = common_inputs[\"attention_mask\"].dtype\n            # 拼接 attention_mask，扩展 past 部分为 1，表示 \"已存在\" 的 token\n            common_inputs[\"attention_mask\"] = torch.cat(\n                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n            )\n            # 构造 past_key_values，为每一层提供 (key, value) 的零张量\n            # 注意：causal LM 中只构造 (key, value)，没有 encoder 部分\n            common_inputs[\"past_key_values\"] = [\n                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_encoder_layers)\n            ]\n         # 返回包含 input_ids、attention_mask（以及可选的 past_key_values）的输入字典\n        return common_inputs\n    # 此函数为生成伪输入数据的核心方法，供 sequence classification 和 QA 模型导出 ONNX 使用\n    # 注：并未使用 super().generate_dummy_inputs 是为了增强代码清晰度和自定义灵活性\n    # 若 batch_size 为 -1，表示采用动态轴，为避免 ONNX 导出中某些静态优化行为，\n    # 则将其替换为固定值（如 2），确保推理兼容性和导出稳定性\n    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        batch_size: int = -1,\n        seq_length: int = -1,\n        is_pair: bool = False,\n        framework: Optional[TensorType] = None,\n    ) -> Mapping[str, Any]:\n        \n        batch_size = compute_effective_axis_dimension(\n            batch_size,\n            fixed_dimension=OnnxConfig.default_fixed_batch, # 设置默认批次大小为2\n            num_token_to_add=0  # 无需添加特殊 token\n        )\n\n        # 如果是动态轴（-1），我们以 8 个 token 的固定维度进行forward，以避免 ONNX 进行的优化\n        # 同理，如果 seq_length 为 -1，则设为固定值（如 8），避免 ONNX 静态路径优化\n        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)  # 根据是否是 pair 输入决定是否添加特殊 token\n        seq_length = compute_effective_axis_dimension(\n            seq_length,\n            fixed_dimension=OnnxConfig.default_fixed_sequence, # 设置默认序列长度为8\n            num_token_to_add=token_to_add  # 需要预留特殊 token 长度\n        )\n\n        # 根据计算批次和序列生成虚拟输入\n        # 构造 dummy 输入：使用 unknown token 构造文本串，个数为 batch_size，每条长度为 seq_length\n        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n        # 使用 tokenizer 编码输入，返回张量形式的输入字典（如 input_ids, attention_mask）\n        # return_tensors 指定输出格式（如 \"pt\" or \"np\"）\n        common_inputs = dict(tokenizer(dummy_input, return_tensors=framework))\n        return common_inputs  # 返回输入张量字典，供后续模型或 ONNX trace 使用\n    # 生成虚拟输入,分支判断并调用上面定义的私有的方法\n    def generate_dummy_inputs(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        batch_size: int = -1,\n        seq_length: int = -1,\n        is_pair: bool = False,\n        framework: Optional[TensorType] = None,\n    ) -> Mapping[str, Any]:\n        if self.task in [\"default\", \"seq2seq-lm\"]: # 如果任务类型是\"default\", \"seq2seq-lm\"\n            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n            )\n\n        elif self.task == \"causal-lm\":\n            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n            )\n        else:\n            common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n            )\n\n        return common_inputs\n    # 该函数用于处理 past_key_values 的展开（flatten）操作，\n    # 以便在 ONNX 导出过程中将嵌套结构（如列表中的多个张量）转换为扁平的字典形式，\n    # 例如：将 [(k0, v0), (k1, v1), ...] 展开为 {\"past_key_values.0.key\": ..., \"past_key_values.0.value\": ..., ...}\n    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n        if self.task in [\"default\", \"seq2seq-lm\"]: \n            # 对于 \"default\" 和 \"seq2seq-lm\" 类型任务，调用父类默认实现，\n            # 通常适用于 encoder-decoder 模型如 BART、T5 等\n            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n        else:\n            # 对于其他任务（如 causal-lm，纯 decoder 架构），显式调用 OnnxSeq2SeqConfigWithPast 的父类实现\n            # 防止因多重继承或方法重写引发冲突或歧义\n            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n                flattened_output, name, idx, t\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:00:23.909642Z","iopub.execute_input":"2025-05-25T03:00:23.910177Z","iopub.status.idle":"2025-05-25T03:00:23.946221Z","shell.execute_reply.started":"2025-05-25T03:00:23.910141Z","shell.execute_reply":"2025-05-25T03:00:23.945132Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"onnxConfig=BartOnnxConfig(configuration)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:33:27.569584Z","iopub.execute_input":"2025-05-25T02:33:27.570976Z","iopub.status.idle":"2025-05-25T02:33:27.577444Z","shell.execute_reply.started":"2025-05-25T02:33:27.570935Z","shell.execute_reply":"2025-05-25T02:33:27.575752Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"onnxConfig.task","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:57:12.517372Z","iopub.execute_input":"2025-05-25T02:57:12.517785Z","iopub.status.idle":"2025-05-25T02:57:12.524801Z","shell.execute_reply.started":"2025-05-25T02:57:12.517758Z","shell.execute_reply":"2025-05-25T02:57:12.523550Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'default'"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"onnxConfig.inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:33:43.092540Z","iopub.execute_input":"2025-05-25T02:33:43.092934Z","iopub.status.idle":"2025-05-25T02:33:43.103107Z","shell.execute_reply.started":"2025-05-25T02:33:43.092907Z","shell.execute_reply":"2025-05-25T02:33:43.101830Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('input_ids', {0: 'batch', 1: 'encoder_sequence'}),\n             ('attention_mask', {0: 'batch', 1: 'encoder_sequence'}),\n             ('decoder_input_ids', {0: 'batch', 1: 'decoder_sequence'}),\n             ('decoder_attention_mask', {0: 'batch', 1: 'decoder_sequence'})])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"onnxConfig.outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:35:20.346122Z","iopub.execute_input":"2025-05-25T02:35:20.346499Z","iopub.status.idle":"2025-05-25T02:35:20.353934Z","shell.execute_reply.started":"2025-05-25T02:35:20.346467Z","shell.execute_reply":"2025-05-25T02:35:20.352638Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('last_hidden_state', {0: 'batch', 1: 'decoder_sequence'})])"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"onnxConfig.use_past","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:01:16.716805Z","iopub.execute_input":"2025-05-25T03:01:16.717149Z","iopub.status.idle":"2025-05-25T03:01:16.723588Z","shell.execute_reply.started":"2025-05-25T03:01:16.717128Z","shell.execute_reply":"2025-05-25T03:01:16.722615Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"print(OnnxConfig.default_fixed_batch,OnnxConfig.default_fixed_sequence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:51:10.296228Z","iopub.execute_input":"2025-05-25T02:51:10.297008Z","iopub.status.idle":"2025-05-25T02:51:10.303465Z","shell.execute_reply.started":"2025-05-25T02:51:10.296976Z","shell.execute_reply":"2025-05-25T02:51:10.302470Z"}},"outputs":[{"name":"stdout","text":"2 8\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers import BartTokenizer\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:56:31.444492Z","iopub.execute_input":"2025-05-25T02:56:31.444886Z","iopub.status.idle":"2025-05-25T02:56:32.792227Z","shell.execute_reply.started":"2025-05-25T02:56:31.444861Z","shell.execute_reply":"2025-05-25T02:56:32.790984Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597571ab2ac7414d84207f8107ee435d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192d667d64a44ddca1bd5b6a4822afda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9711ed231314b16b0a0aa898398b64c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d41c99ada54aa08ad1f8f52d50808a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb348410e194a8c84776d177ddd08be"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"onnxConfig.generate_dummy_inputs(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:56:37.528327Z","iopub.execute_input":"2025-05-25T02:56:37.529028Z","iopub.status.idle":"2025-05-25T02:56:37.537798Z","shell.execute_reply.started":"2025-05-25T02:56:37.529000Z","shell.execute_reply":"2025-05-25T02:56:37.536461Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[0, 3, 3, 3, 3, 3, 3, 2], [0, 3, 3, 3, 3, 3, 3, 2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n 'decoder_input_ids': [[0, 3, 3, 3, 3, 3, 3, 2], [0, 3, 3, 3, 3, 3, 3, 2]],\n 'decoder_attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n  [1, 1, 1, 1, 1, 1, 1, 1]]}"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"__all__ = [\"BartConfig\", \"BartOnnxConfig\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}