{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Any, Optional, Tuple\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom flax.linen.attention import dot_product_attention_weights\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax import lax\nfrom transformers.modeling_flax_outputs import (\n    FlaxBaseModelOutputWithPastAndCrossAttentions,\n    FlaxCausalLMOutputWithCrossAttentions,\n)\nfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel, append_call_sample_docstring\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\nfrom transformers.models.gpt2.configuration_gpt2 import GPT2Config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:46:52.323197Z","iopub.execute_input":"2025-05-31T23:46:52.323619Z","iopub.status.idle":"2025-05-31T23:46:52.328662Z","shell.execute_reply.started":"2025-05-31T23:46:52.323599Z","shell.execute_reply":"2025-05-31T23:46:52.327994Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"logger = logging.get_logger(__name__)\n_CHECKPOINT_FOR_DOC = \"openai-community/gpt2\"\n_CONFIG_FOR_DOC = \"GPT2Config\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:46:52.332947Z","iopub.execute_input":"2025-05-31T23:46:52.333150Z","iopub.status.idle":"2025-05-31T23:46:52.370270Z","shell.execute_reply.started":"2025-05-31T23:46:52.333133Z","shell.execute_reply":"2025-05-31T23:46:52.369683Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"GPT2_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a Flax Linen\n    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a\n    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.\n\n    Finally, this model supports inherent JAX features such as:\n\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        config ([`GPT2Config`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n            `jax.numpy.bfloat16` (on TPUs).\n\n            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n            specified all the computation will be performed with the given `dtype`.\n\n            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n            parameters.**\n\n            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n            [`~FlaxPreTrainedModel.to_bf16`].\n\"\"\"\n\nGPT2_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`):\n            `input_ids_length` = `sequence_length`. Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:46:52.371044Z","iopub.execute_input":"2025-05-31T23:46:52.371309Z","iopub.status.idle":"2025-05-31T23:46:52.386255Z","shell.execute_reply.started":"2025-05-31T23:46:52.371269Z","shell.execute_reply":"2025-05-31T23:46:52.385623Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Any 是 Python 的类型注解，来自 typing 模块，意思是“可以是任何类型”。\n# 它本身不影响运行时行为，只是为了类型检查工具（如 mypy、Pyright）识别。\nclass FlaxConv1D(nn.Module):\n    features: int  # 输出特征数，即卷积核的数量，决定输出通道维度\n    use_bias: bool = True  # 是否使用偏置项\n    # 表示这个 dtype 参数可以是任何类型（虽然实际使用中你期望它是 jnp.dtype 类型）。\n    # 在混合精度训练（如 fp16/bfloat16）中，用于控制是否使用低精度参与前向和反向计算。\n    dtype: Any = jnp.float32  # 数据类型，可用于混合精度等优化\n    # precision（数值计算精度提示）：\n    # 是 jax.lax.dot_general 的一个参数，控制 XLA 编译器在矩阵乘法等操作中的数值精度策略\n    # 典型取值（来自 jax.lax.Precision）：\n    # None（默认）\n    # 'fastest'（追求速度）\n    # 'high'（高精度）\n    # 'default'（JAX的默认折中）\n    # 用于微调 性能 vs 数值稳定性 的权衡，特别是在 TPU 上很重要。\n    precision: Any = None  # 控制 XLA 编译时的精度选项，用于优化性能或精度权衡\n    # @nn.compact 是 Flax 的装饰器，用于定义模块的“紧凑模式”。\n    # 在这个模式中，子层（如 self.param(...), nn.Dense(...)）直接写在 __call__ 中定义。\n    # 它的作用是：\n    # 允许在 __call__ 方法中创建参数和子模块（否则需要在 setup() 方法中定义）\n    # 使代码更简洁直观（常用于小模块，如 MLP 层）\n    @nn.compact\n    def __call__(self, inputs):\n        # 将输入转换为指定dtype，保证计算过程中的数值一致性和稳定性\n        inputs = jnp.asarray(inputs, self.dtype)\n        # 定义可训练权重 kernel，初始化 shape 为 (features, 输入通道数)\n        # 类似于Dense层的权重矩阵，但Flax中习惯将[输出, 输入]的排列\n        kernel = self.param(\"kernel\", jax.nn.initializers.normal(stddev=0.02), (self.features, inputs.shape[-1]))\n        # 转置 kernel，使其 shape 从 (features, in_dim) → (in_dim, features)\n        # 原因是 dot_general 中 inputs 的最后一个维度与 kernel 的第一个维度对齐\n        kernel = jnp.asarray(kernel.transpose(), self.dtype)\n        # 进行通用张量乘法（dot_general）：\n        # 等价于：y = inputs @ kernel，其中 inputs.shape=[..., in_dim]，kernel.shape=[in_dim, features]\n        # 配置 (((inputs.ndim - 1,), (0,)), ((), ()))：\n        #     inputs 的最后一维 与 kernel 的第 0 维做点积；\n        #     其余维度保持不变（批次维或序列长度等）\n        y = lax.dot_general(inputs, kernel, (((inputs.ndim - 1,), (0,)), ((), ())), precision=self.precision)\n        # 若设置使用偏置，则添加一组 shape 为 (features,) 的偏置项，逐元素加到输出中\n        if self.use_bias:\n            bias = self.param(\"bias\", jax.nn.initializers.zeros, (self.features,))\n            bias = jnp.asarray(bias, self.dtype)\n            y = y + bias\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:46:52.387193Z","iopub.execute_input":"2025-05-31T23:46:52.387445Z","iopub.status.idle":"2025-05-31T23:46:52.406574Z","shell.execute_reply.started":"2025-05-31T23:46:52.387423Z","shell.execute_reply":"2025-05-31T23:46:52.405791Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"config=GPT2Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:46:52.407935Z","iopub.execute_input":"2025-05-31T23:46:52.408161Z","iopub.status.idle":"2025-05-31T23:46:52.424395Z","shell.execute_reply.started":"2025-05-31T23:46:52.408136Z","shell.execute_reply":"2025-05-31T23:46:52.423799Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"config.max_position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:46:58.069549Z","iopub.execute_input":"2025-05-31T23:46:58.070078Z","iopub.status.idle":"2025-05-31T23:46:58.075982Z","shell.execute_reply.started":"2025-05-31T23:46:58.070051Z","shell.execute_reply":"2025-05-31T23:46:58.075131Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"1024"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"jnp.ones((1,5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:47:00.138508Z","iopub.execute_input":"2025-05-31T23:47:00.139211Z","iopub.status.idle":"2025-05-31T23:47:02.066435Z","shell.execute_reply.started":"2025-05-31T23:47:00.139188Z","shell.execute_reply":"2025-05-31T23:47:02.065653Z"}},"outputs":[{"name":"stderr","text":"INFO:2025-05-31 23:47:00,832:jax._src.xla_bridge:924: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\nINFO:2025-05-31 23:47:00,846:jax._src.xla_bridge:924: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Array([[1., 1., 1., 1., 1.]], dtype=float32)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"make_causal_mask(\n                jnp.ones((1,5), dtype=\"bool\"), dtype=\"bool\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:47:04.155791Z","iopub.execute_input":"2025-05-31T23:47:04.156327Z","iopub.status.idle":"2025-05-31T23:47:04.401793Z","shell.execute_reply.started":"2025-05-31T23:47:04.156301Z","shell.execute_reply":"2025-05-31T23:47:04.401206Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Array([[[[ True, False, False, False, False],\n         [ True,  True, False, False, False],\n         [ True,  True,  True, False, False],\n         [ True,  True,  True,  True, False],\n         [ True,  True,  True,  True,  True]]]], dtype=bool)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 总结（设计深意）\n# ✅ 解决问题：将当前 token 的 KV 信息拼接进缓存，避免重复计算所有历史 KV；\n# ✅ 效率优化：使用 lax.dynamic_update_slice 原地更新，避免不必要的张量复制；\n# ✅ 跨步状态维护：利用 Flax 的 self.variable() 实现跨 forward 的缓存管理；\n# ✅ 解码核心机制：此函数是 autoregressive decoding（如 beam search、sampling）的关键构件；\n# ✅ 与 PyTorch 区别：Flax 在模块内部显式管理 cache state，更显函数式设计哲学。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"jnp.broadcast_to(\n                jnp.arange(2) < 2 + 1,\n                tuple([2]) + (1, 1,2),\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:47:07.635617Z","iopub.execute_input":"2025-05-31T23:47:07.636343Z","iopub.status.idle":"2025-05-31T23:47:07.761878Z","shell.execute_reply.started":"2025-05-31T23:47:07.636318Z","shell.execute_reply":"2025-05-31T23:47:07.761237Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Array([[[[ True,  True]]],\n\n\n       [[[ True,  True]]]], dtype=bool)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"class FlaxGPT2Attention(nn.Module):\n    config: GPT2Config  # 包含 GPT2 的超参数\n    dtype: jnp.dtype = jnp.float32   # 控制内部张量的数值类型，支持混合精度\n    causal: bool = True # 是否启用因果遮蔽，决定是否是自回归模式（如 decoder）\n    is_cross_attention: bool = False # 是否为交叉注意力（decoder 中的 encoder-decoder attention）\n    def setup(self):\n        config = self.config  \n        self.embed_dim = config.hidden_size # 嵌入维度，也是 attention 输入/输出的总通道数\n        self.num_heads = config.num_attention_heads # 多头注意力中的“头”数量\n        self.head_dim = self.embed_dim // self.num_heads # 每个头的维度，GPT2中一般为64（如 hidden_size=768, num_heads=12）\n        # 在交叉注意力中：\n        # q_attn 仅对 decoder 输入做 Q 投影；\n        # c_attn 对 encoder 的输出做 K 和 V 投影（因此是 2 倍维度）。\n        if self.is_cross_attention:\n            self.c_attn = FlaxConv1D(2 * self.embed_dim, dtype=self.dtype)\n            self.q_attn = FlaxConv1D(self.embed_dim, dtype=self.dtype)\n        # 在自注意力中：\n        # c_attn 一次性生成 Q、K、V，拼接在一起；\n        # 用一个 Conv1D 实现一次性投影（GPT-2 的原始做法，效率优于分别建三层）\n        else:\n            self.c_attn = FlaxConv1D(3 * self.embed_dim, dtype=self.dtype)\n        # 用于注意力输出的线性变换（即 attn_output @ W_o），回到原始维度。\n        self.c_proj = FlaxConv1D(self.embed_dim, dtype=self.dtype)\n        # residual dropout，用于 transformer 中的残差路径，在训练时随机屏蔽一部分元素，防止过拟合。\n        self.resid_dropout = nn.Dropout(rate=config.resid_pdrop)\n        # 提前生成 causal mask：\n        # 它是一个 [1, 1, seq_len, seq_len] 的布尔张量；\n        # 用于保证 decoder 在第 i 个 token 时只关注 ≤ i 的位置；\n        # 提前创建是为了避免在前向传播时每次重复计算。\n        if self.causal:\n            self.causal_mask = make_causal_mask(\n                jnp.ones((1, config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\"\n            )\n    # 拆分头 -->(b,s,h,dk)\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n    # 合并头 -->(b,s,d)\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n    # 将当前时间步的 key、value 拼接进缓存（cache），以支持自回归解码过程中的高效推理。\n    # 使用方式类似于 GPT 或 T5 的 autoregressive decoding。\n    # 来自 Flax 官方 attention 实现的适配版本。\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        # 缓存变量的创建/读取\n        # 判断是否已有缓存（即是否是首次推理）；\n        # 解码过程中的第一个 token 会触发变量创建，而后续则读取和更新。\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        # cache 是 Flax 中一个专用的可变状态容器；\n        # 创建缓存变量：cached_key, cached_value 预留空间；\n        # cache_index 记录当前写入位置（类似 指针），主要用于确保序列逐 token 更新。\n        # 若变量已存在（is_initialized == True），这几句不会覆盖变量内容，只返回句柄。\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n        # 如果缓存已存在，进行增量更新\n        if is_initialized:\n            # 提取张量维度，支持多批次（batch）；\n            # max_length 是缓存的序列最大长度，来自配置；\n            # num_heads 和 depth_per_head 用于处理多头注意力。*batch_dims会是元组的样子(2,)\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # 构造 lax.dynamic_update_slice 所需的索引；\n            # 每次只更新当前时间步的 key/value，而非整段替换；\n            # indices 指定从哪个位置写入，例如写入第 cur_index 个位置。\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0) # (0,cur_index, 0, 0)\n            # 将当前 key / value 写入缓存；\n            # dynamic_update_slice 是 XLA 中用于写入张量特定位置的原语（避免新建张量）；\n            # 写入后更新变量值，以便下次使用。\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            # 注意 GPT-2/decoder 的 decoding 是按 token 或 token block 来；\n            # 所以更新后需要前移 cache_index。\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # 缓存部分的掩码 \n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            # 将动态 causal mask 与传入的 attention mask 合并\n            # 得到最终的 attention mask（用于注意力得分遮蔽）。\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        # 返回更新后的状态\n        # 输出新的 key/value（含缓存逻辑），以及更新后的 attention mask；\n        # 在解码中，这些会被用于计算 QK^T、softmax 等操作。\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        key_value_states: Optional[jnp.ndarray] = None, # 表示来自 encoder 的上下文，用于 decoder 的 cross-attention\n        attention_mask=None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n    ):\n        # 若传入了 key_value_states，说明当前是 cross-attention 层\n        is_cross_attention = key_value_states is not None\n        batch_size = hidden_states.shape[0]\n        # QKV 构建（按是否 cross attention）\n        if not is_cross_attention:\n            qkv_out = self.c_attn(hidden_states)\n            query, key, value = jnp.split(qkv_out, 3, axis=2)\n        else: # 在 cross-attention 中，query 来自 decoder 输入，key/value 来自 encoder 输出；\n            q_out = self.q_attn(hidden_states)\n            (query,) = jnp.split(q_out, 1, axis=2)\n            kv_out = self.c_attn(key_value_states)\n            key, value = jnp.split(kv_out, 2, axis=2)\n        # 拆分多头（[batch, seq, dim] → [batch, seq, n_heads, head_dim]）\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n        # 准备注意力掩码\n        query_length, key_length = query.shape[1], key.shape[1] # q_len,k_len\n        if self.causal:\n            if self.has_variable(\"cache\", \"cached_key\"):\n                mask_shift = self.variables[\"cache\"][\"cache_index\"] # 当前时间步位置\n                # 之前缓存的key序列长度\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1] \n                # 利用 lax.dynamic_slice 动态裁剪出 [query_length, max_length] 的因果遮蔽；\n                causal_mask = lax.dynamic_slice(\n                    self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n                )\n            else: # 没有缓存的情况\n                causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n            # 在批次轴广播 (b,1,q_len,k_len)\n            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        # attention mask 转为 bias（float mask）\n        # 将布尔遮蔽转换为数值遮蔽；\n        # masked 的位置被赋值 -inf（即最小 float 值），以使 softmax 输出趋于 0；\n        if attention_mask is not None and self.causal:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n            # 合并填充和因果掩码\n            attention_mask = combine_masks(attention_mask, causal_mask)\n        elif self.causal: # 因果,自回归的情况 没传入attention_mask\n            attention_mask = causal_mask\n        elif attention_mask is not None: # 传入了attention_mask,编码器的情况\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\") # drop 流\n\n        # 若为 decoder 的自回归推理阶段，则用 cache；\n        # cache 中存的是历史生成的 key/value；\n        # 每步只输入一个 token，但 key/value 会动态增长\n        # init_cache是个标记,标记缓存状态是否已经创建,满足条件,就更新状态,attention_mask这时其实是因果掩码\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n        # attention mask 转为 bias（float mask）\n        # 遮挡位置会是很大的负数\n        if attention_mask is not None:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n        else: \n            attention_bias = None\n        # 注意力权重计算\n        # 核心注意力函数（Flax 封装的 softmax(QK^T + bias)）；\n        # 注意这里权重仅 softmax 后的概率，还没乘 V；\n        # dropout_rng 仅训练时有效。\n        attn_weights = dot_product_attention_weights(\n            query,\n            key,\n            bias=attention_bias,\n            dropout_rng=dropout_rng,\n            dropout_rate=self.config.attn_pdrop,\n            deterministic=deterministic,\n            dtype=self.dtype,\n            precision=None,\n        )\n        # attention 输出计算\n        # einsum 实现注意力值加权求和；\n        # 合并多头；\n        # 最后线性映射回残差路径所需维度，添加 dropout；\n        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        outputs = (attn_output, attn_weights) if output_attentions else (attn_output,)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:48:51.068552Z","iopub.execute_input":"2025-05-31T23:48:51.069175Z","iopub.status.idle":"2025-05-31T23:48:51.089449Z","shell.execute_reply.started":"2025-05-31T23:48:51.069150Z","shell.execute_reply":"2025-05-31T23:48:51.088529Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class FlaxGPT2MLP(nn.Module):\n    config: GPT2Config  # 模型配置\n    intermediate_size: int  # 中间层维度\n    dtype: jnp.dtype = jnp.float32   # 参数和计算所用的数值精度，默认 float32\n    def setup(self):\n        embed_dim = self.config.hidden_size # 输入/输出特征维度（GPT中通常为768/1024/1280等）\n        # 输入从 hidden_size -> intermediate_size 的线性层（使用Conv1D实现，等价于 Dense）\n        self.c_fc = FlaxConv1D(self.intermediate_size, dtype=self.dtype)\n        # 激活后的特征再映射回原始维度：intermediate_size -> hidden_size\n        self.c_proj = FlaxConv1D(embed_dim, dtype=self.dtype)\n        self.act = ACT2FN[self.config.activation_function]  # 激活函数，如 GELU、ReLU 等\n        self.dropout = nn.Dropout(rate=self.config.resid_pdrop)  # 残差 dropout，提升泛化能力，控制训练阶段随机性\n\n    def __call__(self, hidden_states, deterministic: bool = True):\n        # 第一步：线性变换，将 hidden_states 从 [batch, seq, hidden] 映射为 [batch, seq, intermediate]\n        hidden_states = self.c_fc(hidden_states)\n        # 第二步：非线性激活，引入复杂特征变换，使模型具有更强表达力（核心提升点）\n        hidden_states = self.act(hidden_states)\n        # 第三步：投影回原始维度，保持残差连接一致性（原始 residual 需要维度一致）\n        hidden_states = self.c_proj(hidden_states)\n         # 第四步：Dropout（仅在训练时启用），防止过拟合，改善训练稳定性\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n        return hidden_states  # 输出：[batch, seq, hidden]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:49:01.241379Z","iopub.execute_input":"2025-05-31T23:49:01.242122Z","iopub.status.idle":"2025-05-31T23:49:01.248688Z","shell.execute_reply.started":"2025-05-31T23:49:01.242098Z","shell.execute_reply":"2025-05-31T23:49:01.247876Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# causal=False, is_cross_attention=True 使用跨注意力\n# causal=True,is_cross_attention=False 使用解码器自注意力","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlaxGPT2Block(nn.Module):\n    config: GPT2Config  # 配置类，包含所有结构超参数\n    dtype: jnp.dtype = jnp.float32   # 精度设定，通常为 float32 / bfloat16\n    def setup(self):\n        hidden_size = self.config.hidden_size\n        # inner_dim：MLP 的中间层维度\n        inner_dim = self.config.n_inner if self.config.n_inner is not None else 4 * hidden_size\n        # 第一个 LayerNorm，用于 Attention 之前（Pre-LN 架构）\n        self.ln_1 = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n        # 自注意力模块（支持缓存机制，用于自回归生成）\n        self.attn = FlaxGPT2Attention(self.config, dtype=self.dtype)\n         # 第二个 LayerNorm，用于 FFN 之前\n        self.ln_2 = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n        # 如果开启 cross-attention（如 GPT 解码器对 encoder 输出进行 cross-attend）\n        if self.config.add_cross_attention:\n            # 创建 cross attention 模块：本质仍是一个 MultiHeadAttention，只是将 encoder 的输出作为 key/value\n            # 注意:交叉时,causal=False, is_cross_attention=True 表示不是用因果掩码,是用交叉注意力机制\n            self.crossattention = FlaxGPT2Attention(\n                config=self.config, dtype=self.dtype, causal=False, is_cross_attention=True\n            )\n            # 第三个 LayerNorm，cross-attn 使用独立的归一化层\n            self.ln_cross_attn = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n        # 前馈网络模块（MLP），包含两层 Conv1D + 激活函数 + Dropout\n        self.mlp = FlaxGPT2MLP(self.config, inner_dim, dtype=self.dtype)\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask=None,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        deterministic: bool = True,  # 控制 dropout 行为（训练时 False，推理时 True）\n        init_cache: bool = False,  # 自回归生成时启用，初始化缓存结构\n        output_attentions: bool = False,  # 是否返回注意力权重（debug/可视化用）\n    ):\n         # ====== 自注意力前处理 ======\n        residual = hidden_states   # 保存残差连接用的输入\n        hidden_states = self.ln_1(hidden_states)  # LayerNorm 在注意力之前\n        attn_outputs = self.attn(  # 自注意力机制（多头、带缓存、自因果mask）\n            hidden_states,\n            attention_mask=attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n        )\n        # 注意力输出（不含 residual）\n        attn_output = attn_outputs[0]  # output_attn: a, (attentions)\n        outputs = attn_outputs[1:]  # 其余是注意力权重（如果请求输出）\n         # 残差连接：原始输入 + 注意力输出\n        hidden_states = attn_output + residual\n\n         # ====== Cross Attention（可选）======\n        if encoder_hidden_states is not None:\n            # 需要启用 cross_attention\n            if not hasattr(self, \"crossattention\"):\n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n                )\n            \n            residual = hidden_states\n            hidden_states = self.ln_cross_attn(hidden_states)\n            # cross-attention：将 encoder 的输出作为 key/value，对当前序列 query 做交叉注意力\n            cross_attn_outputs = self.crossattention(\n                hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                deterministic=deterministic,\n                output_attentions=output_attentions,\n            )\n            attn_output = cross_attn_outputs[0]\n             # 残差连接\n            hidden_states = residual + attn_output\n            outputs = outputs + cross_attn_outputs[1:] # 拼接 cross-attn 权重\n        # ====== 前馈网络（MLP）=======\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)  # 第二次 LayerNorm\n        # 两层线性层 + 激活函数 + dropout：映射维度→非线性→映射回原始维度\n        feed_forward_hidden_states = self.mlp(hidden_states, deterministic=deterministic)\n        # 残差连接\n        hidden_states = residual + feed_forward_hidden_states\n        # ====== 输出结果 ======\n        # 第一个为主输出，后续是 attention weights（如果启用）\n        outputs = (hidden_states,) + outputs\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T23:51:32.096702Z","iopub.execute_input":"2025-05-31T23:51:32.097007Z","iopub.status.idle":"2025-05-31T23:51:32.107588Z","shell.execute_reply.started":"2025-05-31T23:51:32.096987Z","shell.execute_reply":"2025-05-31T23:51:32.106852Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"jnp.arange(5)[None, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:25:16.750618Z","iopub.execute_input":"2025-06-01T00:25:16.750997Z","iopub.status.idle":"2025-06-01T00:25:16.758356Z","shell.execute_reply.started":"2025-06-01T00:25:16.750972Z","shell.execute_reply":"2025-06-01T00:25:16.757647Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Array([[0, 1, 2, 3, 4]], dtype=int32)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"jnp.broadcast_to(jnp.arange(5)[None, :], (2,5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:25:31.625229Z","iopub.execute_input":"2025-06-01T00:25:31.625533Z","iopub.status.idle":"2025-06-01T00:25:31.681348Z","shell.execute_reply.started":"2025-06-01T00:25:31.625514Z","shell.execute_reply":"2025-06-01T00:25:31.680723Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Array([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]], dtype=int32)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"class FlaxGPT2PreTrainedModel(FlaxPreTrainedModel):\n    \"\"\"\n    一个抽象类，用于处理权重初始化，并提供一个用于下载和加载预训练模型的简单接口。\n    \"\"\"\n    config_class = GPT2Config # 配置\n    base_model_prefix = \"transformer\" # 在权重字典中的基键\n    module_class: nn.Module = None # 内部封装的用来干活的主模块\n    def __init__(\n        self,\n        config: GPT2Config,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs) # 主模块\n        # 调用父类的初始化方法\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # 初始化输入张量，模拟模型输入结构（input_ids, attention_mask, position_ids）\n        # 注意：只用于初始化权重，不会真实训练或推理\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\") # 输入 token id，占位用，全为 0\n        attention_mask = jnp.ones_like(input_ids) # 模拟 attention mask，全部为 1 表示无遮挡\n        # 广播生成位置编码\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)  # 拆分 PRNGKey：分别用于初始化参数和初始化带 dropout 的模块\n        # 多流随机源，Flax 通用写法，确保参数/随机过程独立\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        # 分支逻辑:if 使用交叉注意力\n        if self.config.add_cross_attention:\n            # 创建 encoder 侧输入，全为 0，仅用于初始化形状，不参与实际计算\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.n_embd,))\n            encoder_attention_mask = attention_mask\n            # 初始化模块参数，传入 encoder 输入以激活 cross-attention 路径，确保初始化能覆盖所有子模块\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False, # 返回元组\n            )\n        else: # 解码器自注意力初始化\n            module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n        # 提取初始化后的权重（FrozenDict），结构对应 nn.Module 中的所有子模块参数\n        random_params = module_init_outputs[\"params\"]\n        # 如果提供了已有参数，表示是某种 partial restore 模式\n        if params is not None:\n            # 先对随机初始化的参数处理\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params)) # 对传入的参数处理\n            for missing_key in self._missing_keys:  # 使用记录的缺失参数 key，从随机初始化结果中填补到已有参数中\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set() # 初始化完成后清除缺失记录，避免下一次重复添加\n            return freeze(unflatten_dict(params)) # 返回重新冻结 + 结构还原后的参数树（FrozenDict）\n        else: # 如果没传入params,直接返回随机初始化的完整参数树\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n         # 创建虚拟输入：用于模拟推理时的输入结构（input_ids, attention_mask, position_ids）\n        # 这里只是形状匹配，内容无实际意义，关键在于触发 init_cache 分支\n        input_ids = jnp.ones((batch_size, max_length)) # 模拟一批输入，token 全为 1\n        attention_mask = jnp.ones_like(input_ids)\n        # 构造位置编码，广播成 batch_size x max_length\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n        # init_cache=True 会在子模块中调用forward(call方法) 内部有判断这个标记,之后init_cache module.init调用的是apply方法\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        # 这里返回init_variables中的集合cache\n        return unfreeze(init_variables[\"cache\"]) \n\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n    def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n        # 如果传入了编码器输出 并且没有传入编码器填充掩码\n        if encoder_hidden_states is not None and encoder_attention_mask is None:\n            batch_size, sequence_length = encoder_hidden_states.shape[:2] # 编码器 b,s\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length)) # 默认的编码器填充掩码\n        batch_size, sequence_length = input_ids.shape # 目标序列 b,s\n        # 如果没传入位置ids\n        if position_ids is None:\n            # 有缓存的话,必须传入位置ids\n            if past_key_values is not None: \n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n            # 这个是没缓存的情况 \n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n        # 设置默认的填充掩码\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n        # Handle any PRNG if needed\n        rngs = {} # 设置默认的rngs dropout流\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n        inputs = {\"params\": params or self.params}\n        # 如果传入了 past_key_values，说明缓存（cache）已经初始化；\n        # 此时必须向下传递一个私有标志 init_cache，以确保使用缓存；\n        # 还必须确保缓存被标记为 mutable（可变），这样它才能在 FlaxGPT2Attention 模块中被修改\n        if past_key_values: \n            inputs[\"cache\"] = past_key_values \n            mutable = [\"cache\"] # 指定可以修改的集合\n        else: # 没有缓存的情况\n            mutable = False\n        outputs = self.module.apply(\n            inputs, # 变量字典\n            jnp.array(input_ids, dtype=\"i4\"), # \"i4\" 等价于 jnp.int32，表示 32位有符号整数\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            encoder_hidden_states,\n            encoder_attention_mask,\n            not train,  # deterministic\n            False, # init_cache\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # 将更新的缓存添加到模型输出\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        # 如果传入了缓存,指定返回元组\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:41:11.016141Z","iopub.execute_input":"2025-06-01T00:41:11.016434Z","iopub.status.idle":"2025-06-01T00:41:11.033966Z","shell.execute_reply.started":"2025-06-01T00:41:11.016410Z","shell.execute_reply":"2025-06-01T00:41:11.033424Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class FlaxGPT2BlockCollection(nn.Module):\n    config: GPT2Config # 配置\n    dtype: jnp.dtype = jnp.float32 # 矩阵运算的数据类型\n    def setup(self): # 子模块初始化于此\n        # 层集合\n        self.blocks = [\n            FlaxGPT2Block(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask=None,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n        # 遍历解码器的每一层\n        for block in self.blocks:\n            if output_hidden_states: # 添加每个层的输入 carry\n                all_hidden_states += (hidden_states,)\n            layer_outputs = block(\n                hidden_states,\n                attention_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n            )\n            hidden_states = layer_outputs[0]\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n                if encoder_hidden_states is not None: # 编码器输出有传入,说明是交叉注意力\n                    all_cross_attentions += (layer_outputs[2],) # 交叉注意力权重\n        # 这包含可能的 `None` 值 - `FlaxGPT2Module` 会将它们过滤掉\n        outputs = (hidden_states, all_hidden_states, all_attentions, all_cross_attentions)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:46:37.487491Z","iopub.execute_input":"2025-06-01T00:46:37.487793Z","iopub.status.idle":"2025-06-01T00:46:37.496203Z","shell.execute_reply.started":"2025-06-01T00:46:37.487772Z","shell.execute_reply":"2025-06-01T00:46:37.495558Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class FlaxGPT2Module(nn.Module):\n    config: GPT2Config \n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self): # 定义的一些子模块\n        self.embed_dim = self.config.hidden_size # 嵌入维度\n        self.wte = nn.Embed( # 词嵌入\n            self.config.vocab_size,\n            self.embed_dim,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        self.wpe = nn.Embed( # 位置嵌入\n            self.config.max_position_embeddings,\n            self.embed_dim,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        self.dropout = nn.Dropout(rate=self.config.embd_pdrop) # dropout\n        self.h = FlaxGPT2BlockCollection(self.config, dtype=self.dtype) # 主模块\n        self.ln_f = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        position_ids,\n        encoder_hidden_states: Optional[jnp.ndarray] = None, # 编码器输出\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        deterministic=True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        input_embeds = self.wte(input_ids.astype(\"i4\")) \n        position_embeds = self.wpe(position_ids.astype(\"i4\"))\n        hidden_states = input_embeds + position_embeds # 隐藏状态:序列中token的表示\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic) \n        outputs = self.h(\n            hidden_states,\n            attention_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0] # transformer decoder的输出\n        hidden_states = self.ln_f(hidden_states) # norm\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states, # 最后一层的隐藏状态\n            hidden_states=outputs[1], \n            attentions=outputs[2],\n            cross_attentions=outputs[3],\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:51:59.864584Z","iopub.execute_input":"2025-06-01T00:51:59.865063Z","iopub.status.idle":"2025-06-01T00:51:59.874314Z","shell.execute_reply.started":"2025-06-01T00:51:59.865043Z","shell.execute_reply":"2025-06-01T00:51:59.873566Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# FlaxGPT2Model 是面向用户的高层接口（包装器），FlaxGPT2Module 是实际的网络结构。\n# 通过 module_class 将两者解耦，是 HuggingFace 在 Flax 中对模块化、灵活性的一种工程化实现。\n# FlaxGPT2Model 并不硬编码模块实现，而是通过类变量 module_class 持有结构定义。\n# 允许在上层模型复用逻辑的同时，替换底层架构（如换成 FlaxGPTNeoXModule、FlaxGPT2WithCrossAttnModule 等）。\n@add_start_docstrings(\n    \"The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top.\",\n    GPT2_START_DOCSTRING,\n)\nclass FlaxGPT2Model(FlaxGPT2PreTrainedModel):\n    module_class = FlaxGPT2Module","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:54:15.869123Z","iopub.execute_input":"2025-06-01T00:54:15.869432Z","iopub.status.idle":"2025-06-01T00:54:15.873720Z","shell.execute_reply.started":"2025-06-01T00:54:15.869410Z","shell.execute_reply":"2025-06-01T00:54:15.872923Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"_CHECKPOINT_FOR_DOC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:54:37.181734Z","iopub.execute_input":"2025-06-01T00:54:37.182022Z","iopub.status.idle":"2025-06-01T00:54:37.186689Z","shell.execute_reply.started":"2025-06-01T00:54:37.182000Z","shell.execute_reply":"2025-06-01T00:54:37.185933Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'openai-community/gpt2'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# 这行代码的目的是：向 FlaxGPT2Model.__call__ 方法自动追加 docstring 示例\nappend_call_sample_docstring(\n    FlaxGPT2Model, # 目标类对象，其 __call__ 方法会被补充文档。\n    _CHECKPOINT_FOR_DOC, # 一个字符串，代表参考的预训练模型（如 \"gpt2\"），用于生成示例\n    FlaxBaseModelOutputWithPastAndCrossAttentions, # 模型输出的类型，告知用户 .call() 返回的是这个结构\n    _CONFIG_FOR_DOC, # 指定配置类（如 \"GPT2Config\"），用于文档中展示如何加载对应模型\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T00:56:48.373562Z","iopub.execute_input":"2025-06-01T00:56:48.373845Z","iopub.status.idle":"2025-06-01T00:56:48.377874Z","shell.execute_reply.started":"2025-06-01T00:56:48.373826Z","shell.execute_reply":"2025-06-01T00:56:48.377340Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class FlaxGPT2LMHeadModule(nn.Module):\n    config: GPT2Config\n    dtype: jnp.dtype = jnp.float32\n    def setup(self): # 定义子模块\n        self.transformer = FlaxGPT2Module(self.config, dtype=self.dtype) \n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        )\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        position_ids,\n        encoder_hidden_states: Optional[jnp.ndarray] = None,\n        encoder_attention_mask: Optional[jnp.ndarray] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        outputs = self.transformer(\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0] # 经过各个解码器层的输出\n        # 如果配置中指定词嵌入共享\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T # 转置输入的词嵌入\n            # self.lm_head 是一个 nn.Module 的实例（如 nn.Dense）\n            # 它定义了权重结构（如 kernel、bias）和前向传播逻辑（__call__），但本身是无状态的。\n            # Flax 设计为纯函数式，所以使用 .apply() 显式提供参数。hidden_states 是输入张量\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else: # 如果不共享\n            lm_logits = self.lm_head(hidden_states)\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n        return FlaxCausalLMOutputWithCrossAttentions(\n            logits=lm_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:03:51.566404Z","iopub.execute_input":"2025-06-01T01:03:51.566679Z","iopub.status.idle":"2025-06-01T01:03:51.574830Z","shell.execute_reply.started":"2025-06-01T01:03:51.566659Z","shell.execute_reply":"2025-06-01T01:03:51.574107Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"jnp.ones((1,5), dtype=\"i4\").cumsum(axis=-1) - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:11:26.497301Z","iopub.execute_input":"2025-06-01T01:11:26.497566Z","iopub.status.idle":"2025-06-01T01:11:26.654966Z","shell.execute_reply.started":"2025-06-01T01:11:26.497547Z","shell.execute_reply":"2025-06-01T01:11:26.654324Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Array([[0, 1, 2, 3, 4]], dtype=int32)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input\n    embeddings).\n    \"\"\",\n    GPT2_START_DOCSTRING,\n)\nclass FlaxGPT2LMHeadModel(FlaxGPT2PreTrainedModel):  # GPT2 带语言建模头的完整模型\n    module_class = FlaxGPT2LMHeadModule  # 指定用于初始化的核心模块类（定义 forward 行为）\n    # 准备生成任务所需的输入结构，尤其适配 cache 的初始设定\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jax.Array] = None):\n        batch_size, seq_length = input_ids.shape   # 当前输入的批大小与序列长度\n        # 初始化用于自回归生成的缓存结构（含 key/value attention 缓存）\n        # 注意 cache 长度 = max_length（目标生成长度），而非当前输入长度\n        past_key_values = self.init_cache(batch_size, max_length) \n        # GPT2 使用 causal mask，因此可以用静态全 1 的 attention_mask：\n        # 即便后续 token 是 padding，仍由 causal mask 自动屏蔽，不影响因果逻辑\n        # 使用静态 mask 避免 XLA 编译过程中因 shape 动态变化导致的重编译开销\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        # 如果传入了attention_mask\n        if attention_mask is not None:\n            # 为了保证位置编码连续性，需要根据 attention_mask 生成 position_ids\n            # 用累加方式将有效 token 的位置编码设置为 0,1,2,...（padding 仍为负值）\n            position_ids = attention_mask.cumsum(axis=-1) - 1 #  位置ids\n            # 更新 extended_attention_mask 的前部，使得有效输入位置保留原 mask\n            # 后部保持为 1，因 causal mask 已处理未来 token，无需显式屏蔽\n            # operand：要被更新的原始数组。update：包含要写入到 operand 中的新值的数组。\n            # start_indices：表示每个维度起始更新位置的标量索引列表\n            # 返回一个数组，其对应位置被 update 数组内容替换（插入），其余保持 operand 原值。\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask.astype(\"i4\"), (0, 0)\n            )\n        else:  # 若未传入 attention_mask，则默认按顺序构造位置编码\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,  # 初始化的 缓存（key/value）\n            \"attention_mask\": extended_attention_mask,  # 静态或更新后的注意力掩码\n            \"position_ids\": position_ids,    # 当前输入 token 的位置编码\n        }\n    # 在每轮生成后更新输入状态，用于下一步生成\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        # 将上一轮输出的缓存继续传入下一轮，用于加速自回归 attention\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        # 自增 position_ids，仅保留最后一个 token 的位置编码，加速推理\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n        \nappend_call_sample_docstring(\n    FlaxGPT2LMHeadModel,\n    _CHECKPOINT_FOR_DOC,\n    FlaxCausalLMOutputWithCrossAttentions,\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:24:39.937383Z","iopub.execute_input":"2025-06-01T01:24:39.937893Z","iopub.status.idle":"2025-06-01T01:24:39.945454Z","shell.execute_reply.started":"2025-06-01T01:24:39.937870Z","shell.execute_reply":"2025-06-01T01:24:39.944780Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"x = jnp.zeros(6)        # 原始数组：6 个 0\ny = jnp.ones(3)         # 要更新进去的数组：3 个 1\nlax.dynamic_update_slice(x, y, (2,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:24:43.775496Z","iopub.execute_input":"2025-06-01T01:24:43.775755Z","iopub.status.idle":"2025-06-01T01:24:43.783460Z","shell.execute_reply.started":"2025-06-01T01:24:43.775737Z","shell.execute_reply":"2025-06-01T01:24:43.782814Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"Array([0., 0., 1., 1., 1., 0.], dtype=float32)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"# 若更新数组过大超出原数组尾部，会自动向前调整插入位置以适配：\nlax.dynamic_update_slice(x, y, (3,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:23:10.588923Z","iopub.execute_input":"2025-06-01T01:23:10.589672Z","iopub.status.idle":"2025-06-01T01:23:10.595589Z","shell.execute_reply.started":"2025-06-01T01:23:10.589644Z","shell.execute_reply":"2025-06-01T01:23:10.594958Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Array([0., 0., 0., 1., 1., 1.], dtype=float32)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"lax.dynamic_update_slice(x, y, (5,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:23:34.030756Z","iopub.execute_input":"2025-06-01T01:23:34.031384Z","iopub.status.idle":"2025-06-01T01:23:34.037206Z","shell.execute_reply.started":"2025-06-01T01:23:34.031361Z","shell.execute_reply":"2025-06-01T01:23:34.036552Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Array([0., 0., 0., 1., 1., 1.], dtype=float32)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# 二维数组更新示例：\nx = jnp.zeros((4, 4))       # 4x4 零矩阵\ny = jnp.ones((2, 2))        # 2x2 的更新值矩阵\nlax.dynamic_update_slice(x, y, (1, 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:24:06.514885Z","iopub.execute_input":"2025-06-01T01:24:06.515129Z","iopub.status.idle":"2025-06-01T01:24:06.647060Z","shell.execute_reply.started":"2025-06-01T01:24:06.515113Z","shell.execute_reply":"2025-06-01T01:24:06.646271Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Array([[0., 0., 0., 0.],\n       [0., 0., 1., 1.],\n       [0., 0., 1., 1.],\n       [0., 0., 0., 0.]], dtype=float32)"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"__all__ = [\"FlaxGPT2LMHeadModel\", \"FlaxGPT2Model\", \"FlaxGPT2PreTrainedModel\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T01:22:49.253343Z","iopub.execute_input":"2025-06-01T01:22:49.253613Z","iopub.status.idle":"2025-06-01T01:22:49.257152Z","shell.execute_reply.started":"2025-06-01T01:22:49.253593Z","shell.execute_reply":"2025-06-01T01:22:49.256473Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}