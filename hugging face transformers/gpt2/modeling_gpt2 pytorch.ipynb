{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Callable, Optional, Tuple, Union\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers.activations import ACT2FN\nfrom transformers.generation import GenerationMixin\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel, SequenceSummary\nfrom transformers.pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\nfrom transformers.utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.utils.model_parallel_utils import assert_device_map, get_device_map\nfrom transformers.models.gpt2.configuration_gpt2 import GPT2Config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:42:44.350786Z","iopub.execute_input":"2025-05-31T02:42:44.351383Z","iopub.status.idle":"2025-05-31T02:43:25.578981Z","shell.execute_reply.started":"2025-05-31T02:42:44.351359Z","shell.execute_reply":"2025-05-31T02:43:25.578097Z"}},"outputs":[{"name":"stderr","text":"2025-05-31 02:43:05.397065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748659385.854641      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748659385.974600      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"logger = logging.get_logger(__name__) # 日志对象\n_CHECKPOINT_FOR_DOC = \"openai-community/gpt2\"\n_CONFIG_FOR_DOC = \"GPT2Config\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.580519Z","iopub.execute_input":"2025-05-31T02:43:25.581171Z","iopub.status.idle":"2025-05-31T02:43:25.585190Z","shell.execute_reply.started":"2025-05-31T02:43:25.581147Z","shell.execute_reply":"2025-05-31T02:43:25.584470Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 在pytorch模型中使用tf检查点\n# model:pytorch模型实例 gpt2_checkpoint_path:本地tf检查点路径\ndef load_tf_weights_in_gpt2(model, config, gpt2_checkpoint_path):\n    try:\n        import re\n        import tensorflow as tf\n    except ImportError:  # 捕获导入错误,之后抛出\n        logger.error( \n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path) # 绝对路径\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n    # 从tf检查点加载 返回name,shape \n    init_vars = tf.train.list_variables(tf_path)\n    names = [] # 用来存放检查点中权重的name\n    arrays = [] # \n    for name, shape in init_vars: # 遍历每个权重的名称和形状\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name) # 对应的具体的值\n        names.append(name)\n        arrays.append(array.squeeze())\n    for name, array in zip(names, arrays): # 遍历对应的权重名和值\n        name = name[6:]  # 跳过 \"model/\" \n        name = name.split(\"/\") # 按/切分为列表形式\n        pointer = model # 从模型根部开始\n        for m_name in name: # 遍历其中的每个name分片\n            if re.fullmatch(r\"[A-Za-z]+\\d+\", m_name): \n                scope_names = re.split(r\"(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n            if scope_names[0] == \"w\" or scope_names[0] == \"g\":\n                pointer = getattr(pointer, \"weight\") # 获取具体的模块值,这时是随机初始化值\n            elif scope_names[0] == \"b\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"wpe\" or scope_names[0] == \"wte\":\n                pointer = getattr(pointer, scope_names[0])\n                pointer = getattr(pointer, \"weight\")\n            else:\n                pointer = getattr(pointer, scope_names[0])\n            if len(scope_names) >= 2: # 更新pointer指针指向子层\n                num = int(scope_names[1])\n                pointer = pointer[num]\n        try: \n            if pointer.shape != array.shape:\n                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n        except ValueError as e:\n            e.args += (pointer.shape, array.shape)\n            raise # 抛出错误\n        logger.info(f\"Initialize PyTorch weight {name}\")\n        pointer.data = torch.from_numpy(array) # 更新pointer(这时是随机的权重)为检查点中指定的值\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.586242Z","iopub.execute_input":"2025-05-31T02:43:25.586568Z","iopub.status.idle":"2025-05-31T02:43:25.625342Z","shell.execute_reply.started":"2025-05-31T02:43:25.586530Z","shell.execute_reply":"2025-05-31T02:43:25.624649Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"torch.full( [], 64** 0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.626995Z","iopub.execute_input":"2025-05-31T02:43:25.627206Z","iopub.status.idle":"2025-05-31T02:43:25.681870Z","shell.execute_reply.started":"2025-05-31T02:43:25.627189Z","shell.execute_reply":"2025-05-31T02:43:25.681213Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"tensor(8.)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"torch.finfo(torch.float32).min","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.682506Z","iopub.execute_input":"2025-05-31T02:43:25.682730Z","iopub.status.idle":"2025-05-31T02:43:25.687403Z","shell.execute_reply.started":"2025-05-31T02:43:25.682707Z","shell.execute_reply":"2025-05-31T02:43:25.686772Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"-3.4028234663852886e+38"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n    attn_weights = torch.matmul(query, key.transpose(-1, -2)) # q@k -->(b,h,q_len,k_len)\n    if module.scale_attn_weights:  # 如果设定要缩放权重矩阵\n        attn_weights = attn_weights / torch.full(\n            [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n        )\n    # scale_attn_by_inverse_layer_idx = True：表示要对每一层的注意力权重按层编号缩放。\n    # layer_idx + 1：当前层编号（从 0 开始，所以 +1 避免除以 0）。\n    # 除以它，相当于 越靠后的层，缩放因子越小。\n    # 防止高层注意力过强：\n    # 因为多层 Transformer 堆叠后，如果不做处理，越往后的层容易产生较大注意力权重（尤其是在残差连接后），可能导\n    # 致模型训练不稳定或过度依赖最后几层。\n    # 引入层的归一化因子：\n    # 这种缩放方式就像 ResNet 中的技巧，用层数反比因子平衡每层的影响，属于 一种手工归一化的策略。\n    # 学术和工程习惯中，“缩放因子”通常指用于乘/除的数值本身，即这里应是 1 / (layer_idx + 1)\n    if module.scale_attn_by_inverse_layer_idx:\n        attn_weights = attn_weights / float(module.layer_idx + 1)\n    # 如果不是交叉注意力\n    if not module.is_cross_attention:\n        # 如果只有“正常”注意层实现因果掩码\n        query_length, key_length = query.size(-2), key.size(-2) # q_len,k_len\n        # 从模块掩码中切分出(:,:,q_len,k_len)\n        causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n        mask_value = torch.finfo(attn_weights.dtype).min # 很大的负数\n        # 掩码值:很大的负数\n        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n        # 因果掩码中True的地方用attn_weights中对应的值,False的地方用mask_value(很大的负数)\n        attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n    # 如果有传入attention_mask\n    if attention_mask is not None:\n        # 加上填充掩码,这时attention_mask里面应该是0的地方是不遮挡,而遮挡位置会是很大的负数\n        attn_weights = attn_weights + attention_mask\n    # 在k_len上归一化,k_len上所有的值的和等于1,这样得到的就是每行表示q中的token和k中所有token的相似性\n    # 越大表示语义越近，越小表示越无关\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n    # # 向下转换（如有必要）回 V 的 dtype（如果是混合精度）——否则为无操作\n    attn_weights = attn_weights.type(value.dtype)\n    attn_weights = module.attn_dropout(attn_weights) # dropout\n    # 遮盖某些头,head_mask会是[1,0,1,...]的形式,0的地方会忽略,权重矩阵被置为0\n    if head_mask is not None:\n        attn_weights = attn_weights * head_mask \n    # 对value加权,(b,h,q_len,k_len)@(b,h,v_len,hd)-->(b,h,q_len,hd)\n    # 这样q中每行token被加权表示成value中各个token的加权形式,语义相似的权重大,\n    # 语义无关的几乎不被加权,这样得到的上下文表示中每个token就具有了上下文整体表示的能力\n    attn_output = torch.matmul(attn_weights, value)\n    attn_output = attn_output.transpose(1, 2) # -->(b,q_len,h,hd)\n    return attn_output, attn_weights # 返回注意力输出和权重矩阵","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.688283Z","iopub.execute_input":"2025-05-31T02:43:25.688557Z","iopub.status.idle":"2025-05-31T02:43:25.706190Z","shell.execute_reply.started":"2025-05-31T02:43:25.688538Z","shell.execute_reply":"2025-05-31T02:43:25.705616Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import GPT2Config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.707050Z","iopub.execute_input":"2025-05-31T02:43:25.707279Z","iopub.status.idle":"2025-05-31T02:43:25.727333Z","shell.execute_reply.started":"2025-05-31T02:43:25.707253Z","shell.execute_reply":"2025-05-31T02:43:25.726655Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"config=GPT2Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.728163Z","iopub.execute_input":"2025-05-31T02:43:25.728798Z","iopub.status.idle":"2025-05-31T02:43:25.744453Z","shell.execute_reply.started":"2025-05-31T02:43:25.728776Z","shell.execute_reply":"2025-05-31T02:43:25.743592Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"config.max_position_embeddings ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:25.745310Z","iopub.execute_input":"2025-05-31T02:43:25.745560Z","iopub.status.idle":"2025-05-31T02:43:25.762007Z","shell.execute_reply.started":"2025-05-31T02:43:25.745544Z","shell.execute_reply":"2025-05-31T02:43:25.761405Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"1024"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 这种下三角矩阵常用于 causal mask（因果掩码）场景，确保位置 i 只能看见 ≤i 的位置，防止模型“偷看未来”\ntorch.tril(torch.ones((8,8), dtype=torch.bool)).view(\n                1, 1,8, 8\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:26.010142Z","iopub.execute_input":"2025-05-31T02:43:26.010764Z","iopub.status.idle":"2025-05-31T02:43:26.020203Z","shell.execute_reply.started":"2025-05-31T02:43:26.010731Z","shell.execute_reply":"2025-05-31T02:43:26.019385Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ True, False, False, False, False, False, False, False],\n          [ True,  True, False, False, False, False, False, False],\n          [ True,  True,  True, False, False, False, False, False],\n          [ True,  True,  True,  True, False, False, False, False],\n          [ True,  True,  True,  True,  True, False, False, False],\n          [ True,  True,  True,  True,  True,  True, False, False],\n          [ True,  True,  True,  True,  True,  True,  True, False],\n          [ True,  True,  True,  True,  True,  True,  True,  True]]]])"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"torch.tensor(-1e4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:43:29.128666Z","iopub.execute_input":"2025-05-31T02:43:29.129258Z","iopub.status.idle":"2025-05-31T02:43:29.135623Z","shell.execute_reply.started":"2025-05-31T02:43:29.129238Z","shell.execute_reply":"2025-05-31T02:43:29.134615Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor(-10000.)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"config.scale_attn_by_inverse_layer_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:24.639881Z","iopub.execute_input":"2025-05-31T02:44:24.640739Z","iopub.status.idle":"2025-05-31T02:44:24.645969Z","shell.execute_reply.started":"2025-05-31T02:44:24.640706Z","shell.execute_reply":"2025-05-31T02:44:24.645118Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"config.reorder_and_upcast_attn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:26.606315Z","iopub.execute_input":"2025-05-31T02:44:26.606614Z","iopub.status.idle":"2025-05-31T02:44:26.611438Z","shell.execute_reply.started":"2025-05-31T02:44:26.606593Z","shell.execute_reply":"2025-05-31T02:44:26.610783Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# 在 OPT 模型中，相关代码如下：\n# if self.reorder_and_upcast_attn:\n#     attn_weights = torch.matmul(query, key.transpose(-1, -2)).float()  # upcast to float32\n#     ...\n#     attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1).type_as(query)  # softmax in float32\n# else:\n#     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n#     attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ALL_ATTENTION_FUNCTIONS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:29.453450Z","iopub.execute_input":"2025-05-31T02:44:29.454100Z","iopub.status.idle":"2025-05-31T02:44:29.459075Z","shell.execute_reply.started":"2025-05-31T02:44:29.454075Z","shell.execute_reply":"2025-05-31T02:44:29.458135Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<transformers.modeling_utils.AttentionInterface at 0x7ee98aab6b50>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"class GPT2Attention(nn.Module): \n    def __init__(self, config, is_cross_attention=False, layer_idx=None):\n        super().__init__() \n        self.config = config\n        max_positions = config.max_position_embeddings # 最大位置\n        self.register_buffer( # 全局因果掩码 使用时切片 \n            \"bias\",\n            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n                1, 1, max_positions, max_positions\n            ),\n            persistent=False, # 不会作为权重参数保存到检查点\n        )\n        # masked_bias 用来作为很大负数  persistent=False就是不会作为检查点中键保存\n        # 但仍然可以在前向传播中使用。\n        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads # hd\n        self.split_size = self.embed_dim \n        # 嵌入维度必须能被头数整除\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(\n                f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n                f\" {self.num_heads}).\"\n            )\n        # 是否缩放注意力权重矩阵\n        self.scale_attn_weights = config.scale_attn_weights\n        self.is_cross_attention = is_cross_attention # 是否是交叉注意力\n        # 是否按层索引缩放注意力权重矩阵\n        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx\n        self.layer_idx = layer_idx # 层索引\n        # 是否对注意力权重进行重排序并在计算前进行类型提升（通常到 float32），以提高数值稳定性，尤其是在\n        # float16/bfloat16 精度下推理时。\n        self.reorder_and_upcast_attn = config.reorder_and_upcast_attn \n        if self.is_cross_attention: # 如果是交叉注意力\n            # 用来投影key,value的线性层\n            self.c_attn = Conv1D(2 * self.embed_dim, self.embed_dim)\n            self.q_attn = Conv1D(self.embed_dim, self.embed_dim)\n        else: # 如果是自注意力 \n            self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n        self.c_proj = Conv1D(self.embed_dim, self.embed_dim) # 输出线性层\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.is_causal = True # 自回归\n        self.pruned_heads = set() # 存放已经修剪的头索引\n    def prune_heads(self, heads):\n        if len(heads) == 0: # 如果没有需要修剪的头,直接返回\n            return\n        # 返回已经修剪过的头索引,index是没有被修剪的头索引中的表示位置\n        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n        # 修剪 conv1d 层 更改其中的权重和截距\n        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n        # Update hyper params\n        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n        self.num_heads = self.num_heads - len(heads) # 这时模块的头数变成减去修剪后的头之后的\n        self.pruned_heads = self.pruned_heads.union(heads) # 更新已修剪的头集合\n\n    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n        # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n        bsz, num_heads, q_seq_len, dk = query.size()\n        _, _, k_seq_len, _ = key.size()\n        # Preallocate attn_weights for `baddbmm`\n        attn_weights = torch.empty(\n            bsz * num_heads, q_seq_len, k_seq_len, dtype=torch.float32, device=query.device)\n        # Compute Scale Factor\n        scale_factor = 1.0\n        if self.scale_attn_weights:\n            scale_factor /= float(value.size(-1)) ** 0.5\n\n        if self.scale_attn_by_inverse_layer_idx:\n            scale_factor /= float(self.layer_idx + 1)\n        # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n        with torch.amp.autocast(query.device.type, enabled=False):\n            q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n            attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)\n        if not self.is_cross_attention:\n            # if only \"normal\" attention layer implements causal mask\n            query_length, key_length = query.size(-2), key.size(-2)\n            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n            mask_value = torch.finfo(attn_weights.dtype).min\n            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n        if attention_mask is not None:\n            # Apply the attention mask\n            attn_weights = attn_weights + attention_mask\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\n        if attn_weights.dtype != torch.float32:\n            raise RuntimeError(\"Error with upcasting, attn_weights does not have dtype torch.float32\")\n        attn_weights = attn_weights.type(value.dtype)\n        attn_weights = self.attn_dropout(attn_weights)\n        # Mask heads if we want to\n        if head_mask is not None:\n            attn_weights = attn_weights * head_mask\n        attn_output = torch.matmul(attn_weights, value)\n        attn_output = attn_output.transpose(1, 2)\n        return attn_output, attn_weights\n\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        # 如果传入了encoder_hidden_states(编码器输出),表示是交叉注意力,这时必须有q_attn属性\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n            # 设定跨注意力时的q线性输出,k,v线性输出,这时的注意力掩码为编码器填充掩码\n            query_states = self.q_attn(hidden_states)\n            key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else: # 自注意力 先投影到3d之后拆分\n            query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)\n        # -->(b,q_len,h,hd)\n        shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n        shape_kv = (*key_states.shape[:-1], -1, self.head_dim) # -->(b,k_len,h,hd)\n        query_states = query_states.view(shape_q).transpose(1, 2) # -->(b,h,q_len,hd)\n        key_states = key_states.view(shape_kv).transpose(1, 2)\n        value_states = value_states.view(shape_kv).transpose(1, 2)\n        if layer_past is not None: # 如果有缓存\n            past_key, past_value = layer_past # 缓存的k,v states\n            key_states = torch.cat((past_key, key_states), dim=-2) # 在序列维度合并\n            value_states = torch.cat((past_value, value_states), dim=-2)\n        if use_cache is True: # 如果设置使用缓存\n            present = (key_states, value_states) # 当前的k,v states\n        else: \n            present = None\n        is_cross_attention = encoder_hidden_states is not None # 是否是跨注意力\n        # 判断是否是解码器自注意力 query_states.shape[-2]= 1的话是只有当前token传入\n        is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n        # 是否使用传统的eager注意力机制\n        using_eager = self.config._attn_implementation == \"eager\"\n        attention_interface: Callable = eager_attention_forward # 具体的注意力方法\n        if self.config._attn_implementation != \"eager\":\n            # 对不能使用sdpa的情况,回退到eager模式 sdpa不支持输出权重矩阵和头掩码\n            if self.config._attn_implementation == \"sdpa\" and (output_attentions or head_mask is not None):\n                using_eager = True\n                logger.warning_once(\n                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                )\n            else: \n                # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n                # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n                # not necessarily to eager (if mentionned options are provided).\n                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n        # 如果使用eager,并且reorder_and_upcast_attn为True\n        if using_eager and self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(\n                query_states, key_states, value_states, attention_mask, head_mask\n            )\n        else: # 这种是使用具体的注意力方法获取注意力输出和权重矩阵\n            attn_output, attn_weights = attention_interface(\n                self,\n                query_states,\n                key_states,\n                value_states,\n                attention_mask,\n                head_mask=head_mask,\n                dropout=self.attn_dropout.p if self.training else 0.0,\n                is_causal=is_causal,\n                **kwargs,\n            )\n        # -->(b,q_len,d)\n        attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous()\n        attn_output = self.c_proj(attn_output) \n        attn_output = self.resid_dropout(attn_output) \n        outputs = (attn_output, present) # 注意力输出+缓存\n        if output_attentions:\n            outputs += (attn_weights,)\n        return outputs  # a, present, (attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:35.368251Z","iopub.execute_input":"2025-05-31T02:44:35.368564Z","iopub.status.idle":"2025-05-31T02:44:35.393933Z","shell.execute_reply.started":"2025-05-31T02:44:35.368542Z","shell.execute_reply":"2025-05-31T02:44:35.393082Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"config._attn_implementation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:45.632474Z","iopub.execute_input":"2025-05-31T02:44:45.633050Z","iopub.status.idle":"2025-05-31T02:44:45.638382Z","shell.execute_reply.started":"2025-05-31T02:44:45.633024Z","shell.execute_reply":"2025-05-31T02:44:45.637580Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'eager'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"config.activation_function","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:47.705109Z","iopub.execute_input":"2025-05-31T02:44:47.705414Z","iopub.status.idle":"2025-05-31T02:44:47.710274Z","shell.execute_reply.started":"2025-05-31T02:44:47.705389Z","shell.execute_reply":"2025-05-31T02:44:47.709733Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'gelu_new'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"ACT2FN[config.activation_function]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:50.157331Z","iopub.execute_input":"2025-05-31T02:44:50.157651Z","iopub.status.idle":"2025-05-31T02:44:50.163510Z","shell.execute_reply.started":"2025-05-31T02:44:50.157629Z","shell.execute_reply":"2025-05-31T02:44:50.162925Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"NewGELUActivation()"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# 前馈层\nclass GPT2MLP(nn.Module):\n    def __init__(self, intermediate_size, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        self.c_fc = Conv1D(intermediate_size, embed_dim)\n        self.c_proj = Conv1D(embed_dim, intermediate_size)\n        self.act = ACT2FN[config.activation_function]\n        self.dropout = nn.Dropout(config.resid_pdrop)\n\n    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n        hidden_states = self.c_fc(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.c_proj(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:55.918561Z","iopub.execute_input":"2025-05-31T02:44:55.918912Z","iopub.status.idle":"2025-05-31T02:44:55.924708Z","shell.execute_reply.started":"2025-05-31T02:44:55.918883Z","shell.execute_reply":"2025-05-31T02:44:55.923923Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print(config.n_inner)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:44:59.526053Z","iopub.execute_input":"2025-05-31T02:44:59.526345Z","iopub.status.idle":"2025-05-31T02:44:59.530407Z","shell.execute_reply.started":"2025-05-31T02:44:59.526325Z","shell.execute_reply":"2025-05-31T02:44:59.529731Z"}},"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"config.add_cross_attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:45:01.811853Z","iopub.execute_input":"2025-05-31T02:45:01.812159Z","iopub.status.idle":"2025-05-31T02:45:01.817784Z","shell.execute_reply.started":"2025-05-31T02:45:01.812139Z","shell.execute_reply":"2025-05-31T02:45:01.817050Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"class GPT2Block(nn.Module):\n    def __init__(self, config, layer_idx=None):\n        super().__init__()\n        hidden_size = config.hidden_size # 隐藏表示大小\n        # 前馈中间层维度\n        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        self.attn = GPT2Attention(config=config, layer_idx=layer_idx) # 注意力\n        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        # 交叉注意力的情况 add_cross_attention为True\n        if config.add_cross_attention:\n            self.crossattention = GPT2Attention(config=config, is_cross_attention=True, layer_idx=layer_idx)\n            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        self.mlp = GPT2MLP(inner_dim, config)\n\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n        residual = hidden_states # 残差\n        hidden_states = self.ln_1(hidden_states) # norm\n        attn_outputs = self.attn( # 自注意力\n            hidden_states,\n            layer_past=layer_past,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n        )\n        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n        outputs = attn_outputs[1:]\n        # 残差连接 residual:上一层的解码器输入或者最开始的嵌入\n        hidden_states = attn_output + residual\n        if encoder_hidden_states is not None: # 跨注意力的情况\n            # 这种情况必须有crossattention模块,表示使用交叉注意力\n            if not hasattr(self, \"crossattention\"):\n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n                )\n            residual = hidden_states # 残差是解码器自注意力的输出\n            hidden_states = self.ln_cross_attn(hidden_states)\n            cross_attn_outputs = self.crossattention( # 交叉注意力\n                hidden_states,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states, # 编码器输出\n                encoder_attention_mask=encoder_attention_mask,\n                output_attentions=output_attentions,\n            )\n            attn_output = cross_attn_outputs[0] # 交叉注意力的输出\n            # 残差\n            hidden_states = residual + attn_output\n            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states) # norm\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        # 前馈前后残差\n        hidden_states = residual + feed_forward_hidden_states\n        if use_cache: # 如果使用缓存 带上缓存 自注意力的缓存\n            outputs = (hidden_states,) + outputs\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n        return outputs  # hidden_states, present, (attentions, cross_attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:45:34.868986Z","iopub.execute_input":"2025-05-31T02:45:34.869527Z","iopub.status.idle":"2025-05-31T02:45:34.879912Z","shell.execute_reply.started":"2025-05-31T02:45:34.869506Z","shell.execute_reply":"2025-05-31T02:45:34.879265Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class GPT2PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = GPT2Config # 设定配置类\n    load_tf_weights = load_tf_weights_in_gpt2 # 加载tf本地检查点的方法\n    base_model_prefix = \"transformer\" # 基础模型前缀\n    is_parallelizable = True  # 可并行化为True\n    supports_gradient_checkpointing = True #支持梯度检查点\n    _no_split_modules = [\"GPT2Block\"] # 静态结构中不可拆分的模块\n    # 分布设备放置中跳过的配置中的键\n    _skip_keys_device_placement = \"past_key_values\"\n    _supports_flash_attn_2 = True # 是否支持flash_attn\n    _supports_sdpa = True \n\n    def __init__(self, *inputs, **kwargs):\n        super().__init__(*inputs, **kwargs) # 调用父类的初始化方法\n    # 初始化权重\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, Conv1D)):\n            # 与使用 truncated_normal 进行初始化的 TF 版本略有不同\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_() # 设置填充对应的表示为全0\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        # 这来自 GPT-2 原论文 Appendix A：为了解决深层网络中的梯度消失或爆炸问题，他们对输出投影矩阵 c_proj.weight \n        # 使用了缩小的标准差。\n        # 原因是：Transformer 深度为 n_layer，每层有两个残差路径（对应两个 LayerNorm + Add），所以总共有 2 * n_layer \n        # 个残差路径，初始化时需考虑这一点进行缩放。\n        for name, p in module.named_parameters():\n            if name == \"c_proj.weight\":\n                # 特殊缩放初始化 --> 每个 Transformer 块有 2 个层范数\n                p.data.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:45:45.516099Z","iopub.execute_input":"2025-05-31T02:45:45.516708Z","iopub.status.idle":"2025-05-31T02:45:45.524312Z","shell.execute_reply.started":"2025-05-31T02:45:45.516667Z","shell.execute_reply":"2025-05-31T02:45:45.523224Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"config.n_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:45:47.687603Z","iopub.execute_input":"2025-05-31T02:45:47.688163Z","iopub.status.idle":"2025-05-31T02:45:47.693174Z","shell.execute_reply.started":"2025-05-31T02:45:47.688138Z","shell.execute_reply":"2025-05-31T02:45:47.692306Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"math.sqrt(2 * config.n_layer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:45:50.071880Z","iopub.execute_input":"2025-05-31T02:45:50.072180Z","iopub.status.idle":"2025-05-31T02:45:50.077790Z","shell.execute_reply.started":"2025-05-31T02:45:50.072158Z","shell.execute_reply":"2025-05-31T02:45:50.077074Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"4.898979485566356"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# 双头模型输出\n@dataclass # 数据类\nclass GPT2DoubleHeadsModelOutput(ModelOutput):\n    \"\"\"\n    Base class for outputs of models predicting if two sentences are consecutive or not.\n\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss.\n        mc_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels` is provided):\n            Multiple choice classification loss.\n        logits (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        mc_logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n            Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n        past_key_values (`Tuple[Tuple[torch.Tensor]]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of length `config.n_layers`, containing tuples of tensors of shape `(batch_size, num_heads,\n            sequence_length, embed_size_per_head)`).\n\n            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n            shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            GPT2Attentions weights after the attention softmax, used to compute the weighted average in the\n            self-attention heads.\n    \"\"\"\n    loss: Optional[torch.FloatTensor] = None \n    mc_loss: Optional[torch.FloatTensor] = None\n    logits: Optional[torch.FloatTensor] = None\n    mc_logits: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None # 缓存\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:04:06.440065Z","iopub.execute_input":"2025-05-31T05:04:06.440354Z","iopub.status.idle":"2025-05-31T05:04:06.447295Z","shell.execute_reply.started":"2025-05-31T05:04:06.440332Z","shell.execute_reply":"2025-05-31T05:04:06.446544Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"GPT2_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`GPT2Config`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nGPT2_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n            sequence tokens in the vocabulary.\n\n            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n            `input_ids`.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        past_key_values (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`):\n            Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n            their past given to this model should not be passed as `input_ids` as they have already been computed.\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            If `past_key_values` is used, `attention_mask` needs to contain the masking strategy that was used for\n            `past_key_values`. In other words, the `attention_mask` always has to have the length:\n            `len(past_key_values) + len(input_ids)`\n\n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n\n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n\n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n\n            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n            `past_key_values`).\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\nPARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n\n    Args:\n        device_map (`Dict[int, list]`, *optional*):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the gpt2 models have the\n            following number of attention modules:\n\n                - openai-community/gpt2: 12\n                - openai-community/gpt2-medium: 24\n                - openai-community/gpt2-large: 36\n                - openai-community/gpt2-xl: 48\n\n    Example:\n\n    ```python\n    # Here is an example of a device map on a machine with 4 GPUs using gpt2-xl, which has a total of 48 attention modules:\n    model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-xl\")\n    device_map = {\n        0: [0, 1, 2, 3, 4, 5, 6, 7, 8],\n        1: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n        2: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n        3: [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n    }\n    model.parallelize(device_map)\n    ```\n\"\"\"\nDEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n\n    Example:\n\n    ```python\n    # On a 4 GPU machine with openai-community/gpt2-large:\n    model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")\n    device_map = {\n        0: [0, 1, 2, 3, 4, 5, 6, 7],\n        1: [8, 9, 10, 11, 12, 13, 14, 15],\n        2: [16, 17, 18, 19, 20, 21, 22, 23],\n        3: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n    }\n    model.parallelize(device_map)  # 将模型拆分到多个设备上\n    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n    ```\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T02:46:24.551000Z","iopub.execute_input":"2025-05-31T02:46:24.551541Z","iopub.status.idle":"2025-05-31T02:46:24.558191Z","shell.execute_reply.started":"2025-05-31T02:46:24.551518Z","shell.execute_reply":"2025-05-31T02:46:24.557462Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# model.parallelize(device_map) 的作用是：\n# 将 模型结构（Module） 拆分到多个设备上运行\n# 同时自动将每个层的 参数（权重） 移动到对应设备上\n# 所以权重也是跟着结构被分布的\n# 注意\n# 嵌入层（embedding）和输出层（LMHead）始终在第一个设备（GPU 0）\n# 所以为了负载均衡，第0号设备应少分配几层（比如这里只分了9层，其他是13层）\n# 当你使用：\n# model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-xl\")\n# model.parallelize(device_map)\n# 即使 from_pretrained 加载的是 单个文件（未分片）的权重，parallelize() 也会：\n# 自动将权重按层分发到对应设备\n# 具体过程如下：\n# from_pretrained 会在 主设备（通常是 CPU 或 GPU:0） 上加载完整模型权重\n# 调用 model.parallelize(device_map) 时：\n# 会将每一层的模块（nn.Module）迁移到 device_map 指定的设备上\n# 同时这些模块内部的权重（即 .weight, .bias）也随模块一起移动到对应设备","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ model = GPT2LMHeadModel.from_pretrained(...) 发生了什么？\n# 这一步确实：\n# 在 CPU 上构建了完整的模型结构\n# 并加载了完整的所有权重（即使你有多个 GPU，默认仍是在 CPU 上完成）\n# ✅ model.parallelize(device_map) 做了什么？\n# 这一步会：\n# 将 模型中的每一层（包括其权重） 移动到你提供的 device_map 指定的 GPU 上\n# 同时会把这些层从 CPU 移除（即调用 .to(device)），不再占用 CPU 内存\n# ❓ 那 CPU 上还保留原始结构或权重吗？\n# 不会保留。\n# 一旦你调用 parallelize()，以下都会发生：\n# 模型各层被转移到目标 GPU（包括结构和权重）\n# CPU 上的模型层和权重会 释放掉，不再占用内存\n# 这是通过模块内部的 .to(device) 逐层调用完成的，PyTorch 本身会将原先设备上的数据清除。\n# 调用 from_pretrained() 后，模型和权重最初在 CPU；调用 parallelize() 后，结构和权重都被分布到多个 GPU，\n# CPU 上不会再保留原始结构和参数。这是一个转移，而不是复制。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ 在 PyTorch 中使用 from_pretrained(..., device_map=...) 加载 sharded checkpoint：\n# 🧩 权重文件已分片（sharded checkpoint）：\n# HuggingFace 的大模型（如 GPT2-XL、OPT、BLOOM 等）通常提供 .bin, .bin.1, .bin.2 等多个文件组成的权重（即分片）。\n# 配套的 pytorch_model.bin.index.json 文件记录了每个权重张量在哪个文件。\n# ⚙️ 配置文件中或代码中指定 device_map：\n# device_map 决定了 哪些模块（如某些层）分布在哪个 GPU 上。\n# 你可以手动指定，也可以自动推理（device_map=\"auto\"）。\n# ✅ 模型结构本身并没有显式分片逻辑：\n# 结构是完整统一的（例如一个 GPT2LMHeadModel 对象），只是 每个子模块的 .to(device) 被设置为不同的 GPU。\n# 换句话说：结构是被动地“被搬运”到各 GPU 的，而不是像 Flax 那样在结构中硬编码分片规则。\n# ✅ from_pretrained(...) 做了哪些事：\n# 1. 实例化模型结构（Module 树）\n# 是的，会实例化模型结构，即构建一个完整的 nn.Module 对象。\n\n# 这一步使用的是模型的 config 信息，如 hidden size、层数等。\n\n# 此时会触发默认的 PyTorch 初始化逻辑（如 nn.Linear 用 Xavier 初始化）。\n\n# 2. 初始化的权重在哪里？\n# 默认初始化是在 CPU 上完成。\n\n# 结构定义 + 初始化权重 → 初始模型对象。\n\n# 如果你用 device_map=\"auto\"，HuggingFace 会临时构造空权重模型并跳过初始化（详见下文）。\n\n# 3. 什么时候加载权重？\n# 紧接着，from_pretrained() 会加载检查点权重（可能是 sharded 的），用检查点中的值覆盖初始化的权重。\n\n# 🧠 更高效的情况（device_map=\"auto\"）：\n# 当你指定 device_map=\"auto\" 并使用支持的模型时（如 bloom-7b1），会走 内存优化路径：\n\n# 利用 init_empty_weights() 创建 结构但无参数值的模型（即参数 .data 都是 empty tensor）。\n\n# 然后用 load_checkpoint_and_dispatch() 直接将 checkpoint 中的分片权重加载到指定的 GPU 上。\n\n# ✅ 此时 不会先初始化全模型的权重再替换，而是直接将 checkpoint 的值加载进空结构里。\n\n# ✅ 总结回答\n# 问题\t回答\n# from_pretrained(...) 会实例化模型吗？\t✅ 会实例化 nn.Module 结构\n# 实例化时的权重会初始化吗？\t✅ 会，除非用了 memory-efficient 路径（如 init_empty_weights()）\n# 初始化权重是在 CPU 还是 GPU？\t默认是在 CPU（后续 .to() 分配）；memory-efficient 情况会直接指定 GPU\n# 使用 sharded 时，加载的检查点权重是怎么处理的？\t✅ 覆盖/替代初始化权重，或直接加载进 empty 权重中（更高效)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config.num_hidden_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:30:00.012383Z","iopub.execute_input":"2025-05-31T03:30:00.012867Z","iopub.status.idle":"2025-05-31T03:30:00.017501Z","shell.execute_reply.started":"2025-05-31T03:30:00.012842Z","shell.execute_reply":"2025-05-31T03:30:00.016945Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"range(torch.cuda.device_count())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:37:44.368950Z","iopub.execute_input":"2025-05-31T03:37:44.369655Z","iopub.status.idle":"2025-05-31T03:37:44.374696Z","shell.execute_reply.started":"2025-05-31T03:37:44.369628Z","shell.execute_reply":"2025-05-31T03:37:44.374109Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"range(0, 2)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"get_device_map(12, range(torch.cuda.device_count()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:38:50.148835Z","iopub.execute_input":"2025-05-31T03:38:50.149108Z","iopub.status.idle":"2025-05-31T03:38:50.154793Z","shell.execute_reply.started":"2025-05-31T03:38:50.149088Z","shell.execute_reply":"2025-05-31T03:38:50.154075Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{0: [0, 1, 2, 3, 4, 5], 1: [6, 7, 8, 9, 10, 11]}"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# del some_tensor\n# torch.cuda.empty_cache()\n# 用于清理不再需要的中间变量之后，释放显存，防止 OOM。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tuple([None] * 12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:59:02.020289Z","iopub.execute_input":"2025-05-31T03:59:02.020626Z","iopub.status.idle":"2025-05-31T03:59:02.026197Z","shell.execute_reply.started":"2025-05-31T03:59:02.020602Z","shell.execute_reply":"2025-05-31T03:59:02.025342Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(None, None, None, None, None, None, None, None, None, None, None, None)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"torch.arange(2,1 + 2, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T04:02:03.666265Z","iopub.execute_input":"2025-05-31T04:02:03.666572Z","iopub.status.idle":"2025-05-31T04:02:03.673594Z","shell.execute_reply.started":"2025-05-31T04:02:03.666550Z","shell.execute_reply":"2025-05-31T04:02:03.672727Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([2])"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"config.add_cross_attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T04:11:58.443420Z","iopub.execute_input":"2025-05-31T04:11:58.443758Z","iopub.status.idle":"2025-05-31T04:11:58.449047Z","shell.execute_reply.started":"2025-05-31T04:11:58.443736Z","shell.execute_reply":"2025-05-31T04:11:58.448458Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"@add_start_docstrings(\n    \"The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top.\",\n    GPT2_START_DOCSTRING,\n)\nclass GPT2Model(GPT2PreTrainedModel):\n    _supports_param_buffer_assignment = False\n    def __init__(self, config):\n        super().__init__(config)\n        self.embed_dim = config.hidden_size\n        self.wte = nn.Embedding(config.vocab_size, self.embed_dim) # 词嵌入\n        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim) # 位置嵌入\n        self.drop = nn.Dropout(config.embd_pdrop) # dropout\n        # 模块列表\n        self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon) # norm\n        # Model parallel\n        self.model_parallel = False # 是否并行\n        self.device_map = None # 设备映射\n        self.gradient_checkpointing = False  # 是否使用梯度检查点\n        self._attn_implementation = config._attn_implementation # 注意力机制实现\n        # 初始化权重并应用最终处理\n        self.post_init()\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None): # 并行化\n        # Check validity of device_map\n        warnings.warn(\n            \"`GPT2Model.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your\"\n            \" model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'h.0': 0, 'h.1': 1,\"\n            \" ...}\",\n            FutureWarning,\n        )\n        # 设备映射\n        self.device_map = (\n            get_device_map(len(self.h), range(torch.cuda.device_count())) if device_map is None else device_map\n        )\n        assert_device_map(self.device_map, len(self.h))\n        self.model_parallel = True # 设置为并行状态\n        # 第一个设备\n        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n        self.last_device = \"cuda:\" + str(max(self.device_map.keys())) # 最后一个设备\n        self.wte = self.wte.to(self.first_device) # 词嵌入和位置嵌入放到第一个设备上\n        self.wpe = self.wpe.to(self.first_device)\n        # 把各个层放置到设备\n        for k, v in self.device_map.items():\n            for block in v: # 遍历需要放到同一个设备上的各个层索引\n                cuda_device = \"cuda:\" + str(k) # 当前层需要放置到的设备\n                self.h[block] = self.h[block].to(cuda_device) # 把层放到对应的设备上\n        # ln_f to last\n        self.ln_f = self.ln_f.to(self.last_device)\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self): # 去并行化\n        warnings.warn(\n            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n            FutureWarning,\n        )\n        self.model_parallel = False # 设置并行状态:False\n        self.device_map = None # 清空映射字典\n        self.first_device = \"cpu\"  # 去并行,就是移到cpu内存\n        self.last_device = \"cpu\"\n        self.wte = self.wte.to(\"cpu\")\n        self.wpe = self.wpe.to(\"cpu\")\n        for index in range(len(self.h)): # 每一层也移到cpu\n            self.h[index] = self.h[index].to(\"cpu\")\n        self.ln_f = self.ln_f.to(\"cpu\")\n        # 清除 PyTorch 内部的 显存缓存池（caching allocator）中未使用的内存块；\n        # 释放这些缓存回 CUDA 驱动，使得 nvidia-smi 看到的显存占用下降；\n        # 适用于：你手动释放了变量（如 del tensor）后，希望显存立即释放。\n        torch.cuda.empty_cache() # 清空 GPU 缓存内存\n    # 获取词嵌入\n    def get_input_embeddings(self):\n        return self.wte\n    def set_input_embeddings(self, new_embeddings):\n        self.wte = new_embeddings\n    def _prune_heads(self, heads_to_prune):\n        # 对每一层进行修剪头\n        for layer, heads in heads_to_prune.items(): # 层索引,当前层对应的头索引列表\n            self.h[layer].attn.prune_heads(heads)\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPastAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n        # 是否输出注意力矩阵\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = ( # 是否输出每一层的隐藏状态\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache # 是否使用缓存\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if input_ids is not None and inputs_embeds is not None: # 不允许同时传入input_ids和inputs_embeds\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None: # 这种情况是有input_ids,无inputs_embeds\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size() # 输入形状:(b,s)\n            input_ids = input_ids.view(-1, input_shape[-1])\n            batch_size = input_ids.shape[0] # b\n        elif inputs_embeds is not None: # 有inputs_embeds,无input_ids\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size = inputs_embeds.shape[0]\n        else: # 这种是两者都没有传入,抛出值错误\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n        # 获取输入数据所在的设备\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        # token type ids 对应每个token所在的句子编号\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n        # 如果没传入缓存\n        if past_key_values is None:\n            past_length = 0 # 设定默认的缓存长度为0\n            past_key_values = tuple([None] * len(self.h)) # 初始化每层的缓存都是None\n        else: # 如果传入了缓存\n            past_length = past_key_values[0][0].size(-2) # 获取缓存中key序列长度\n        if position_ids is None: # 设置位置ids\n            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n            position_ids = position_ids.unsqueeze(0)\n        # 如果没传入inputs_embeds,这时根据input_ids获取词嵌入\n        if inputs_embeds is None:\n            inputs_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids) \n        # 获取层输入 carry\n        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n        # 设置是否使用sdpa\n        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None\n        attention_mask = attention_mask.view(batch_size, -1) if attention_mask is not None else None\n        if self._attn_implementation == \"flash_attention_2\":\n            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n        elif _use_sdpa:\n            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa( # 准备4维因果掩码\n                attention_mask=attention_mask,\n                input_shape=(batch_size, input_shape[-1]),\n                inputs_embeds=inputs_embeds,\n                past_key_values_length=past_length,\n            )\n        else: # 使用eager机制\n            if attention_mask is not None:\n                # We create a 4D attention mask from a 2D tensor mask.\n                # Sizes are [batch_size, 1, 1, to_seq_length]\n                # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n                # this attention mask is more simple than the triangular masking of causal attention\n                # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n                attention_mask = attention_mask[:, None, None, :]\n                # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n                # masked positions, this operation will create a tensor which is 0.0 for\n                # positions we want to attend and the dtype's smallest value for masked positions.\n                # Since we are adding it to the raw scores before the softmax, this is\n                # effectively the same as removing these entirely.\n                attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n                # 注意力bias 遮挡位置会是很大的负数\n                attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n        # 如果配置中指定跨注意力可用,并且传入了编码器输出\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.add_cross_attention and encoder_hidden_states is not None:\n            # 编码器输出对应的批次和序列长度\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size() \n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None: # 设定默认的编码器填充掩码\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            if _use_sdpa: # 如果sdpa可用,准备4d的编码器填充掩码\n                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                    mask=encoder_attention_mask, dtype=inputs_embeds.dtype, tgt_len=input_shape[-1]\n                )\n            elif not self._attn_implementation == \"flash_attention_2\": # 如果不是flash_attention\n                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else: # 这种是自注意力的情况,这时设定编码器填充掩码为None,因为不需要\n            encoder_attention_mask = None\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # head_mask has shape n_layer x batch x n_heads x N x N\n        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n        if token_type_ids is not None: # 带上token type嵌入\n            token_type_embeds = self.wte(token_type_ids)\n            hidden_states = hidden_states + token_type_embeds\n        hidden_states = self.drop(hidden_states) # dropout\n        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n        # 如果设置了使用梯度检查点,并且是训练模式\n        if self.gradient_checkpointing and self.training:\n            if use_cache: # 使用梯度检查点就不能使用缓存\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n        \n        presents = () if use_cache else None # 缓存\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n        all_hidden_states = () if output_hidden_states else None\n        for i in range(len(self.h)): # 遍历每一层索引\n            block, layer_past = self.h[i], past_key_values[i] # 每一层,每一层对应的缓存k,v states\n            # 如果设定了并行化\n            if self.model_parallel:\n                torch.cuda.set_device(hidden_states.device) # 设置当前线程的默认 GPU 设备为 hidden_states所在device\n                # 确保 layer_past 与 hidden_states 位于同一设备上\n                if layer_past is not None:\n                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n                # 确保 attention_mask 始终与 hidden_states 位于同一设备上\n                if attention_mask is not None:\n                    attention_mask = attention_mask.to(hidden_states.device)\n                if isinstance(head_mask, torch.Tensor):\n                    head_mask = head_mask.to(hidden_states.device)\n            if output_hidden_states: # 每一层的隐藏状态输入\n                all_hidden_states = all_hidden_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n                outputs = self._gradient_checkpointing_func(\n                    block.__call__, # 调用每一层的call方法\n                    hidden_states,\n                    None, # 使用gradient_checkpointing时,缓存不能用,所以这里是None\n                    attention_mask,\n                    head_mask[i],\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    use_cache,\n                    output_attentions,\n                )\n            else:\n                outputs = block(\n                    hidden_states,\n                    layer_past=layer_past,\n                    attention_mask=attention_mask,\n                    head_mask=head_mask[i],\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = outputs[0] # 每一层最后的输出\n            if use_cache is True: # 每一层都有缓存(past_key,past_value)\n                presents = presents + (outputs[1],)\n\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n                if self.config.add_cross_attention:\n                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n            # 如果模型并行\n            if self.model_parallel:\n                for k, v in self.device_map.items():\n                    # 如果当前层是当前设备对应的层列表的最后一个,并且当前设备不是最后一个设备\n                    # 这时要移动hidden_states到下个设备,因为下一层的结构和权重都在下个设备上\n                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n        hidden_states = self.ln_f(hidden_states) # 经过所有层之后,隐藏状态标准化\n        # 变形\n        hidden_states = hidden_states.view(output_shape)\n        # Add last hidden state\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n                if v is not None\n            )\n        # 带缓存和交叉注意力权重矩阵的结构化输出\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=presents,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T04:38:27.968240Z","iopub.execute_input":"2025-05-31T04:38:27.968935Z","iopub.status.idle":"2025-05-31T04:38:28.004602Z","shell.execute_reply.started":"2025-05-31T04:38:27.968910Z","shell.execute_reply":"2025-05-31T04:38:28.003991Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input\n    embeddings).\n    \"\"\",\n    GPT2_START_DOCSTRING,\n)\nclass GPT2LMHeadModel(GPT2PreTrainedModel, GenerationMixin):\n    # 指定要权重共享的键 \n    _tied_weights_keys = [\"lm_head.weight\"]\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = GPT2Model(config) # gpt2主模块\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # 语言模型头\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None): # 结构和权重会移到并行的gpu设备\n        warnings.warn(\n            \"`GPT2LMHeadModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should load\"\n            \" your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'transformer.h.0':\"\n            \" 0, 'transformer.h.1': 1, ...}\",\n            FutureWarning,\n        )\n        self.device_map = ( # 设备映射\n            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.transformer.h))\n        self.transformer.parallelize(self.device_map) # 主模块并行化\n        self.lm_head = self.lm_head.to(self.transformer.first_device)\n        self.model_parallel = True # 设置并行标记\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self): # 去并行\n        warnings.warn(\n            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n            FutureWarning,\n        )\n        self.transformer.deparallelize() # 调用主模块的去并行\n        self.transformer = self.transformer.to(\"cpu\") \n        self.lm_head = self.lm_head.to(\"cpu\")\n        self.model_parallel = False\n        torch.cuda.empty_cache()\n    # 获取输出嵌入:(b,s,v)\n    def get_output_embeddings(self):\n        return self.lm_head\n    \n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=CausalLMOutputWithCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        # 如果并行\n        if self.model_parallel:\n            # 把hidden_states移动到第一个设备\n            torch.cuda.set_device(self.transformer.first_device) \n            hidden_states = hidden_states.to(self.lm_head.weight.device)\n        lm_logits = self.lm_head(hidden_states) # (b,s,v)\n        loss = None\n        if labels is not None: # 如果传入了标签\n            # 计算损失\n            loss = self.loss_function(\n                lm_logits,\n                labels,\n                vocab_size=self.config.vocab_size,\n                **kwargs,\n            )\n\n        if not return_dict:\n            output = (lm_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=loss,\n            logits=lm_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n            cross_attentions=transformer_outputs.cross_attentions,\n        )\n\n    @staticmethod # 静态方法\n    def _reorder_cache(\n        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n    ) -> Tuple[Tuple[torch.Tensor]]:\n        \"\"\"\n        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n        beam_idx at every generation step.\n        \"\"\"\n        return tuple(\n            # 0表示批次维度,在这个上下文中是不变的维度 \n            # layer_past 是某层的 (past_key, past_value)，shape 通常为：\n            # (batch_size * num_beams, num_heads, seq_len, head_dim)\n            # past_state.index_select(0, beam_idx.to(past_state.device))\n            # 这一行表示按 beam_idx 重排第 0 个维度（即 batch 维度），确保在 beam search 重排序后，缓存的顺序也对得上。\n            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n            for layer_past in past_key_values # 遍历每一层的缓存\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T04:50:39.677860Z","iopub.execute_input":"2025-05-31T04:50:39.678175Z","iopub.status.idle":"2025-05-31T04:50:39.695716Z","shell.execute_reply.started":"2025-05-31T04:50:39.678154Z","shell.execute_reply":"2025-05-31T04:50:39.694881Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\nThe GPT2 Model transformer with a language modeling and a multiple-choice classification head on top e.g. for\nRocStories/SWAG tasks. The two heads are two linear layers. The language modeling head has its weights tied to the\ninput embeddings, the classification head takes as input the input of a specified classification token index in the\ninput sequence).\n\"\"\",\n    GPT2_START_DOCSTRING,\n)\nclass GPT2DoubleHeadsModel(GPT2PreTrainedModel, GenerationMixin):\n    _tied_weights_keys = [\"lm_head.weight\"] # 指定权重字典中权重共享的键\n    def __init__(self, config):\n        super().__init__(config)\n        config.num_labels = 1\n        self.transformer = GPT2Model(config) # 主模块\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.multiple_choice_head = SequenceSummary(config) # 多选头\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        warnings.warn(\n            \"`GPT2DoubleHeadsModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should\"\n            \" load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your\"\n            \" own `device_map` but it needs to be a dictionary module_name to device, so for instance\"\n            \" {'transformer.h.0': 0, 'transformer.h.1': 1, ...}\",\n            FutureWarning,\n        )\n        self.device_map = (\n            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.transformer.h))\n        self.transformer.parallelize(self.device_map) \n        self.lm_head = self.lm_head.to(self.transformer.first_device)\n        self.multiple_choice_head = self.multiple_choice_head.to(self.transformer.first_device)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        warnings.warn(\n            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n            FutureWarning,\n        )\n        self.transformer.deparallelize()\n        self.transformer = self.transformer.to(\"cpu\")\n        self.lm_head = self.lm_head.to(\"cpu\")\n        self.multiple_choice_head = self.multiple_choice_head.to(\"cpu\")\n        self.model_parallel = False\n        torch.cuda.empty_cache()\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=GPT2DoubleHeadsModelOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        mc_token_ids: Optional[torch.LongTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        mc_labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Tuple, GPT2DoubleHeadsModelOutput]:\n        r\"\"\"\n        mc_token_ids (`torch.LongTensor` of shape `(batch_size, num_choices)`, *optional*, default to index of the last token of the input):\n            Index of the classification token in each input sequence. Selected in the range `[0, input_ids.size(-1) -\n            1]`.\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n            `labels = input_ids`. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to\n            `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size - 1]`\n        mc_labels (`torch.LongTensor` of shape `(batch_size)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n            where *num_choices* is the size of the second dimension of the input tensors. (see *input_ids* above)\n\n        Return:\n\n        Example:\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoTokenizer, GPT2DoubleHeadsModel\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        >>> model = GPT2DoubleHeadsModel.from_pretrained(\"openai-community/gpt2\")\n\n        >>> # Add a [CLS] to the vocabulary (we should train it also!)\n        >>> num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n        >>> # Update the model embeddings with the new vocabulary size\n        >>> embedding_layer = model.resize_token_embeddings(len(tokenizer))\n\n        >>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n        >>> encoded_choices = [tokenizer.encode(s) for s in choices]\n        >>> cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n\n        >>> input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n        >>> mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n\n        >>> outputs = model(input_ids, mc_token_ids=mc_token_ids)\n        >>> lm_logits = outputs.logits\n        >>> mc_logits = outputs.mc_logits\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = transformer_outputs[0]\n        # 如果设定模型并行\n        if self.model_parallel: # 移动hidden_states到lm_head所在设备\n            torch.cuda.set_device(self.transformer.first_device)\n            hidden_states = hidden_states.to(self.lm_head.weight.device)\n        lm_logits = self.lm_head(hidden_states)\n        # 多选logits\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n        mc_loss = None\n        if mc_labels is not None: # 如果传入了mc_labels\n            loss_fct = CrossEntropyLoss()\n            mc_loss = loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))\n        lm_loss = None\n        if labels is not None: # 如果传入了labels\n            labels = labels.to(lm_logits.device) \n            shift_logits = lm_logits[..., :-1, :].contiguous() # 因为输入是labels[:,:-1]\n            shift_labels = labels[..., 1:].contiguous() # 标签 labels[:,1:]\n            loss_fct = CrossEntropyLoss() # 交叉熵损失\n            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)) # 语言模型损失\n        if not return_dict:\n            output = (lm_logits, mc_logits) + transformer_outputs[1:]\n            if mc_loss is not None:\n                output = (mc_loss,) + output\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return GPT2DoubleHeadsModelOutput(\n            loss=lm_loss,\n            mc_loss=mc_loss,\n            logits=lm_logits,\n            mc_logits=mc_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n    @staticmethod # 静态方法\n    def _reorder_cache(\n        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n    ) -> Tuple[Tuple[torch.Tensor]]:\n        \"\"\"\n        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n        beam_idx at every generation step.\n        \"\"\"\n        return tuple(\n            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n            for layer_past in past_key_values # 遍历出每一层缓存\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:04:21.873246Z","iopub.execute_input":"2025-05-31T05:04:21.873535Z","iopub.status.idle":"2025-05-31T05:04:21.892548Z","shell.execute_reply.started":"2025-05-31T05:04:21.873513Z","shell.execute_reply":"2025-05-31T05:04:21.891663Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"help(GPT2DoubleHeadsModel.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2DoubleHeadsModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:05:29.849711Z","iopub.execute_input":"2025-05-31T05:05:29.850132Z","iopub.status.idle":"2025-05-31T05:05:29.939774Z","shell.execute_reply.started":"2025-05-31T05:05:29.850111Z","shell.execute_reply":"2025-05-31T05:05:29.938934Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\nmodel = GPT2DoubleHeadsModel.from_pretrained(\"openai-community/gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:05:42.106255Z","iopub.execute_input":"2025-05-31T05:05:42.106781Z","iopub.status.idle":"2025-05-31T05:05:46.468639Z","shell.execute_reply.started":"2025-05-31T05:05:42.106754Z","shell.execute_reply":"2025-05-31T05:05:46.467930Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0536e84d32441d3a62c285061c7a9f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda4c1e004af43e98e11bb3f417fd7a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04d15780e0e48fb8f61038c47a9d9bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"685114562a694b36a8ad9c35963d7705"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723de7b7680b4190960daa546388060e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970b100e50d54ad9895d293622aa7e39"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c594007ea12a42a991d1f8eacb97dcde"}},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"model.parallelize()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:06:55.489659Z","iopub.execute_input":"2025-05-31T05:06:55.489972Z","iopub.status.idle":"2025-05-31T05:06:56.041533Z","shell.execute_reply.started":"2025-05-31T05:06:55.489952Z","shell.execute_reply":"2025-05-31T05:06:56.040967Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1152: FutureWarning: `GPT2DoubleHeadsModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'transformer.h.0': 0, 'transformer.h.1': 1, ...}\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:708: FutureWarning: `GPT2Model.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'h.0': 0, 'h.1': 1, ...}\n  warnings.warn(\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Add a [CLS] to the vocabulary (we should train it also!)\nnum_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:07:23.997246Z","iopub.execute_input":"2025-05-31T05:07:23.997896Z","iopub.status.idle":"2025-05-31T05:07:24.001366Z","shell.execute_reply.started":"2025-05-31T05:07:23.997869Z","shell.execute_reply":"2025-05-31T05:07:24.000737Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"num_added_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:07:31.361119Z","iopub.execute_input":"2025-05-31T05:07:31.361731Z","iopub.status.idle":"2025-05-31T05:07:31.366513Z","shell.execute_reply.started":"2025-05-31T05:07:31.361706Z","shell.execute_reply":"2025-05-31T05:07:31.365727Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"# Update the model embeddings with the new vocabulary size\nembedding_layer = model.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:07:47.306847Z","iopub.execute_input":"2025-05-31T05:07:47.307149Z","iopub.status.idle":"2025-05-31T05:07:48.875626Z","shell.execute_reply.started":"2025-05-31T05:07:47.307126Z","shell.execute_reply":"2025-05-31T05:07:48.874988Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"embedding_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:07:59.570199Z","iopub.execute_input":"2025-05-31T05:07:59.570878Z","iopub.status.idle":"2025-05-31T05:07:59.575210Z","shell.execute_reply.started":"2025-05-31T05:07:59.570856Z","shell.execute_reply":"2025-05-31T05:07:59.574606Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"Embedding(50258, 768)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\nencoded_choices = [tokenizer.encode(s) for s in choices]\ncls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:08:44.306969Z","iopub.execute_input":"2025-05-31T05:08:44.307584Z","iopub.status.idle":"2025-05-31T05:08:44.323153Z","shell.execute_reply.started":"2025-05-31T05:08:44.307559Z","shell.execute_reply":"2025-05-31T05:08:44.322520Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"encoded_choices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:09:00.347111Z","iopub.execute_input":"2025-05-31T05:09:00.347378Z","iopub.status.idle":"2025-05-31T05:09:00.352473Z","shell.execute_reply.started":"2025-05-31T05:09:00.347360Z","shell.execute_reply":"2025-05-31T05:09:00.351730Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"[[15496, 11, 616, 3290, 318, 13779, 220, 50257],\n [15496, 11, 616, 3797, 318, 13779, 220, 50257]]"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"cls_token_location","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:09:10.446802Z","iopub.execute_input":"2025-05-31T05:09:10.447639Z","iopub.status.idle":"2025-05-31T05:09:10.452354Z","shell.execute_reply.started":"2025-05-31T05:09:10.447586Z","shell.execute_reply":"2025-05-31T05:09:10.451663Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"[7, 7]"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\nmc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:09:39.542789Z","iopub.execute_input":"2025-05-31T05:09:39.543507Z","iopub.status.idle":"2025-05-31T05:09:39.547826Z","shell.execute_reply.started":"2025-05-31T05:09:39.543486Z","shell.execute_reply":"2025-05-31T05:09:39.547071Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"outputs = model(input_ids.to(\"cuda:0\"), mc_token_ids=mc_token_ids.to(\"cuda:0\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:10:40.234630Z","iopub.execute_input":"2025-05-31T05:10:40.234950Z","iopub.status.idle":"2025-05-31T05:10:40.861191Z","shell.execute_reply.started":"2025-05-31T05:10:40.234928Z","shell.execute_reply":"2025-05-31T05:10:40.860606Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"lm_logits = outputs.logits\nmc_logits = outputs.mc_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:10:53.766708Z","iopub.execute_input":"2025-05-31T05:10:53.767615Z","iopub.status.idle":"2025-05-31T05:10:53.771255Z","shell.execute_reply.started":"2025-05-31T05:10:53.767578Z","shell.execute_reply":"2025-05-31T05:10:53.770596Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"print(lm_logits.shape,mc_logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:11:32.964020Z","iopub.execute_input":"2025-05-31T05:11:32.964298Z","iopub.status.idle":"2025-05-31T05:11:32.968989Z","shell.execute_reply.started":"2025-05-31T05:11:32.964280Z","shell.execute_reply":"2025-05-31T05:11:32.968247Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 2, 8, 50258]) torch.Size([1, 2])\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"config.num_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T05:13:48.063995Z","iopub.execute_input":"2025-05-31T05:13:48.064873Z","iopub.status.idle":"2025-05-31T05:13:48.069830Z","shell.execute_reply.started":"2025-05-31T05:13:48.064842Z","shell.execute_reply":"2025-05-31T05:13:48.069142Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    The GPT2 Model transformer with a sequence classification head on top (linear layer).\n\n    [`GPT2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-1) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    GPT2_START_DOCSTRING,\n)\nclass GPT2ForSequenceClassification(GPT2PreTrainedModel): # 序列分类\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.transformer = GPT2Model(config) # 主模块\n        self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=\"microsoft/DialogRPT-updown\",\n        output_type=SequenceClassifierOutputWithPast,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states) # (b,s,num_labels)\n        if input_ids is not None:\n            batch_size, sequence_length = input_ids.shape[:2]\n        else:\n            batch_size, sequence_length = inputs_embeds.shape[:2]\n        # 多批次时,需要配置中设定填充token id\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            last_non_pad_token = -1\n        elif input_ids is not None: # elif中的是pad_token_id有设置的情况\n            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n        else:\n            last_non_pad_token = -1\n            # 当你传入的是 inputs_embeds 而不是 input_ids 时，模型将无法检测 padding token，因此如果你用了 padding token，\n            # 结果可能不符合预期。\n            # 背景说明：\n            # 一般来说，模型接收 input_ids，它会自动查找哪些是 padding token（例如 token id 为 0），然后通过 \n            # attention_mask 或 tokenizer 的 pad token id 去处理。\n            # 但如果你直接传入 inputs_embeds（嵌入后的张量），模型已经看不到原始的 token id 了，它无法判断哪些是 padding。\n            # 因此，无法自动屏蔽 padding 部分的注意力或处理逻辑，从而可能导致错误的 attention、loss 等。\n            logger.warning_once(\n                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n            )\n        # 对每个样本，从它的 logit 序列中选出最后一个非 pad token 的 logit 向量（大小为 vocab_size）\n        # logits 形状是 [batch_size, seq_len, vocab_size]\n        # last_non_pad_token 是一个形状为 [batch_size] 的张量，记录每个样本中最后一个非 pad token 的位置（即对应的 token index）\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T05:28:23.334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logits = torch.tensor([\n    [  # batch 0\n        [0.1, 0.2, 0.3, 0.4],  # token 0\n        [0.5, 0.6, 0.7, 0.8],  # token 1\n        [0.9, 1.0, 1.1, 1.2],  # token 2\n        [1.3, 1.4, 1.5, 1.6],  # token 3\n        [1.7, 1.8, 1.9, 2.0],  # token 4\n    ],\n    [  # batch 1\n        [2.1, 2.2, 2.3, 2.4],\n        [2.5, 2.6, 2.7, 2.8],\n        [2.9, 3.0, 3.1, 3.2],\n        [3.3, 3.4, 3.5, 3.6],\n        [3.7, 3.8, 3.9, 4.0],\n    ],\n    [  # batch 2\n        [4.1, 4.2, 4.3, 4.4],\n        [4.5, 4.6, 4.7, 4.8],\n        [4.9, 5.0, 5.1, 5.2],\n        [5.3, 5.4, 5.5, 5.6],\n        [5.7, 5.8, 5.9, 6.0],\n    ],\n])  # shape: (3, 5, 4)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T05:28:23.334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"last_non_pad_token = torch.tensor([2, 4, 1])  # 每个样本的最后有效 token 的索引","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T05:28:23.335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pooled_logits = logits[torch.arange(3), last_non_pad_token]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T05:28:23.335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 等价于：\n# pooled_logits[0] = logits[0, 2] = [0.9, 1.0, 1.1, 1.2]\n# pooled_logits[1] = logits[1, 4] = [3.7, 3.8, 3.9, 4.0]\n# pooled_logits[2] = logits[2, 1] = [4.5, 4.6, 4.7, 4.8]\n# 最终 pooled_logits 的值：\n# tensor([\n#     [0.9, 1.0, 1.1, 1.2],\n#     [3.7, 3.8, 3.9, 4.0],\n#     [4.5, 4.6, 4.7, 4.8],\n# ])  # shape: (3, 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"help()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    GPT2 Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    GPT2_START_DOCSTRING,\n)\nclass GPT2ForTokenClassification(GPT2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = GPT2Model(config)\n        if hasattr(config, \"classifier_dropout\") and config.classifier_dropout is not None:\n            classifier_dropout = config.classifier_dropout\n        elif hasattr(config, \"hidden_dropout\") and config.hidden_dropout is not None:\n            classifier_dropout = config.hidden_dropout\n        else:\n            classifier_dropout = 0.1\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n    # fmt: off\n    @add_code_sample_docstrings(\n        checkpoint=\"brad1141/gpt2-finetuned-comp2\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_loss=0.25,\n        expected_output=[\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n            \"Position\",\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n            \"Lead\",\n        ],\n    )\n    # fmt: on\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = transformer_outputs[0]\n        hidden_states = self.dropout(hidden_states)\n        logits = self.classifier(hidden_states)\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + transformer_outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    The GPT-2 Model transformer with a span classification head on top for extractive question-answering tasks like\n    SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    GPT2_START_DOCSTRING,\n)\nclass GPT2ForQuestionAnswering(GPT2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.transformer = GPT2Model(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        real_checkpoint=_CHECKPOINT_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"__all__ = [\n    \"GPT2DoubleHeadsModel\",\n    \"GPT2ForQuestionAnswering\",\n    \"GPT2ForSequenceClassification\",\n    \"GPT2ForTokenClassification\",\n    \"GPT2LMHeadModel\",\n    \"GPT2Model\",\n    \"GPT2PreTrainedModel\",\n    \"load_tf_weights_in_gpt2\",\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}