{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import TYPE_CHECKING\nfrom transformers.utils import _LazyModule\nfrom transformers.utils.import_utils import define_import_structure\nif TYPE_CHECKING:  # 静态检查工具检测时\n    from transformers.models.gpt2.configuration_gpt2 import *\n    from transformers.models.gpt2.modeling_flax_gpt2 import *\n    from transformers.models.gpt2.modeling_gpt2 import *\n    from transformers.models.gpt2.modeling_tf_gpt2 import *\n    from transformers.models.gpt2.tokenization_gpt2 import *\n    from transformers.models.gpt2.tokenization_gpt2_fast import *\n    from transformers.models.gpt2.tokenization_gpt2_tf import *\nelse: # 否则,懒加载\n    import sys\n    # _file = globals()[\"__file__\"]\n    # sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T13:28:26.896957Z","iopub.execute_input":"2025-05-30T13:28:26.897582Z","iopub.status.idle":"2025-05-30T13:28:26.902178Z","shell.execute_reply.started":"2025-05-30T13:28:26.897561Z","shell.execute_reply":"2025-05-30T13:28:26.901476Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from collections import OrderedDict\nfrom typing import Any, List, Mapping, Optional\nfrom transformers import PreTrainedTokenizer, TensorType, is_torch_available\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.onnx import OnnxConfigWithPast, PatchingSpec\nfrom transformers.utils import logging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T13:31:02.208301Z","iopub.execute_input":"2025-05-30T13:31:02.208604Z","iopub.status.idle":"2025-05-30T13:31:02.425637Z","shell.execute_reply.started":"2025-05-30T13:31:02.208581Z","shell.execute_reply":"2025-05-30T13:31:02.425077Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"logger = logging.get_logger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T13:31:11.885158Z","iopub.execute_input":"2025-05-30T13:31:11.885461Z","iopub.status.idle":"2025-05-30T13:31:11.889243Z","shell.execute_reply.started":"2025-05-30T13:31:11.885438Z","shell.execute_reply":"2025-05-30T13:31:11.888669Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 场景\t会参考 keys_to_ignore_at_inference 吗？\t说明\n# model(**inputs)\t❌ 不会参考\t直接返回所有输出，包括 past_key_values\n# model.generate(...)\t✅ 会参考\t自动剔除非必要字段，生成结果中可能会排除这些键\n# transformers.pipeline(...)\t✅ 会参考\t自动过滤掉这些键，保证输出简洁\n# Trainer.predict(...)\t✅ 会参考\t只返回用户关心的主输出（如 logits、labels）\n# 这两个设置作用时间点不同：\n# 设置\t生效时间点\t控制什么\n# use_cache\tforward 前\t是否生成缓存\n# keys_to_ignore_at_inference\tforward 后\t推理时是否过滤掉缓存字段\n# 如果你是自己调用 model(input_ids)，不会有任何影响，past_key_values 总会返回（除非你手动 use_cache=False）\n# ；但如果你用的是 .generate() 或 pipeline，则可能自动帮你裁剪掉这些键。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 默认的GPT2Config 返回openai-community/gpt2相似的配置\nclass GPT2Config(PretrainedConfig):\n    \"\"\"\n    ```python\n    >>> from transformers import GPT2Config, GPT2Model\n\n    >>> # Initializing a GPT2 configuration\n    >>> configuration = GPT2Config()\n\n    >>> # Initializing a model (with random weights) from the configuration\n    >>> model = GPT2Model(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"gpt2\"   # 指定模型类型\n    # 模型在推理输出中可能会包含 past_key_values（即用于加速的缓存）。\n    # 但对于某些任务（比如只获取最终 logits），这个缓存在后处理时可以忽略。\n    # 并不是“加载配置时忽略”，而是“模型 forward 输出时，这些 key 可以选择性忽略”。\n    # 是否输出缓存，取决于 use_cache 参数；是否忽略缓存，取决于 keys_to_ignore_at_inference。两者作用不同。\n    # 模型在 forward 时依然会输出 past_key_values，只是告诉某些 API（如管道、Trainer 或序列生成等）在后处理或使用时可以忽略它。\n    keys_to_ignore_at_inference = [\"past_key_values\"] \n    # 属性映射\n    # 如果某些用户或框架代码使用标准化名称（如 hidden_size）来访问或设置配置，内部会自动映射到对应的实际字段（如 n_embd）。\n    # 这在如下两种情况中会用到：\n    # 用户调用 config.hidden_size，实际上访问的是 config.n_embd。\n    # 使用 from_pretrained 加载配置时传入了 hidden_size=1024，会自动设置 n_embd=1024。\n    # 这保证了兼容性与通用接口，即使底层实现字段名是非通用的（如 n_embd），也可以使用标准名字访问\n    attribute_map = {\n        \"hidden_size\": \"n_embd\",\n        \"max_position_embeddings\": \"n_positions\",\n        \"num_attention_heads\": \"n_head\",\n        \"num_hidden_layers\": \"n_layer\",\n    }\n\n    def __init__(\n        self,\n        vocab_size=50257, # 词汇表大小\n        n_positions=1024,  # 支持的最大序列长度\n        n_embd=768, # 模型隐藏维度\n        n_layer=12,  # Transformer 层数\n        n_head=12,  # 多头注意力中 head 的数量\n        n_inner=None, # 前馈网络的中间层维度（默认为 None，表示使用 n_embd）\n        activation_function=\"gelu_new\", # 前馈网络中使用的激活函数\n        resid_pdrop=0.1, # 残差连接的 dropout 概率\n        embd_pdrop=0.1, # 嵌入层的 dropout 概率\n        attn_pdrop=0.1,  # 注意力权重的 dropout 概率\n        layer_norm_epsilon=1e-5,  # LayerNorm 中的 epsilon，避免除以 0\n        initializer_range=0.02, # 权重初始化时的标准差范围\n        summary_type=\"cls_index\",   # 下游任务中 summary 的类型（多用于分类）\n        summary_use_proj=True,  # 是否对 summary 输出做线性变换\n        summary_activation=None, # summary 的激活函数\n        summary_proj_to_labels=True,   # 是否投影到标签维度（用于分类）\n        summary_first_dropout=0.1,  # summary dropout 概率\n        scale_attn_weights=True,   # 是否对 attention 权重进行缩放（默认 True）\n        use_cache=True,  # 是否启用缓存，用于加速生成任务\n        bos_token_id=50256,  # 起始 token ID（Beginning of Sequence）\n        eos_token_id=50256,\n        scale_attn_by_inverse_layer_idx=False, # 是否按层数反比缩放注意力（通常用于一些改进变种）\n        reorder_and_upcast_attn=False,  # 是否在 attention 中重排序并上提精度（float32 计算）\n        **kwargs,  # 其他额外参数\n    ):\n        self.vocab_size = vocab_size\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.n_inner = n_inner\n        self.activation_function = activation_function\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.summary_type = summary_type\n        self.summary_use_proj = summary_use_proj\n        self.summary_activation = summary_activation\n        self.summary_first_dropout = summary_first_dropout\n        self.summary_proj_to_labels = summary_proj_to_labels\n        self.scale_attn_weights = scale_attn_weights\n        self.use_cache = use_cache\n        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n        self.reorder_and_upcast_attn = reorder_and_upcast_attn\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs) # 调用父类的初始化","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T13:56:38.725419Z","iopub.execute_input":"2025-05-30T13:56:38.725713Z","iopub.status.idle":"2025-05-30T13:56:38.733992Z","shell.execute_reply.started":"2025-05-30T13:56:38.725695Z","shell.execute_reply":"2025-05-30T13:56:38.733415Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"configuration = GPT2Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:03:06.396709Z","iopub.execute_input":"2025-05-30T14:03:06.396982Z","iopub.status.idle":"2025-05-30T14:03:06.400599Z","shell.execute_reply.started":"2025-05-30T14:03:06.396964Z","shell.execute_reply":"2025-05-30T14:03:06.399838Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"configuration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:03:11.662764Z","iopub.execute_input":"2025-05-30T14:03:11.663057Z","iopub.status.idle":"2025-05-30T14:03:11.669955Z","shell.execute_reply.started":"2025-05-30T14:03:11.663038Z","shell.execute_reply":"2025-05-30T14:03:11.668941Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"GPT2Config {\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"class GPT2OnnxConfig(OnnxConfigWithPast):\n    def __init__(\n        self,\n        config: PretrainedConfig,\n        task: str = \"default\",\n        patching_specs: List[PatchingSpec] = None,\n        use_past: bool = False,\n    ):\n        super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)\n        # 设置默认的填充id\n        if not getattr(self._config, \"pad_token_id\", None): \n            # TODO: how to do that better?\n            self._config.pad_token_id = 0\n    # 这种装饰器叫做 @property 属性装饰器，它将一个方法转化为一个只读属性\n    # 这个 inputs() 方法返回的是 ONNX 导出时模型输入的结构定义，也叫做 输入张量的动态维度映射说明，主要用于告诉 ONNX：\n    # 每个输入的维度含义，比如哪个维度是 batch，哪个是 sequence，哪个是缓存长度（past_sequence）等。\n    @property \n    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n        if self.use_past: # 如果使用缓存\n            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n        else:\n            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n\n        return common_inputs\n\n    @property \n    def num_layers(self) -> int:\n        return self._config.n_layer\n\n    @property\n    def num_attention_heads(self) -> int:\n        return self._config.n_head\n\n    def generate_dummy_inputs(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        batch_size: int = -1,\n        seq_length: int = -1,\n        is_pair: bool = False,\n        framework: Optional[TensorType] = None,\n    ) -> Mapping[str, Any]:\n        # 调用父类生成基本输入（input_ids 和 attention_mask），即正常推理用的输入。\n        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n            tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n        )\n        # 按 forward() 函数参数顺序构建输入字典，从 input_ids 开始。\n        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n        # 如果启用缓存（KV缓存），则需要额外添加 past_key_values 输入。\n        if self.use_past:\n            if not is_torch_available():\n                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n            else:\n                import torch\n                batch, seqlen = common_inputs[\"input_ids\"].shape # 获取 batch 和当前序列长度\n                # 设置 past 序列长度比当前序列长，模拟真实缓存场景\n                past_key_values_length = seqlen + 2\n                past_shape = ( # (b,h,s,hd)\n                    batch,\n                    self.num_attention_heads,\n                    past_key_values_length,\n                    self._config.hidden_size // self.num_attention_heads,\n                )\n                # 构造每层 KV 缓存的形状：[batch, num_heads, past_seq_len, head_dim]\n                ordered_inputs[\"past_key_values\"] = [\n                    (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(self.num_layers)\n                ]\n        # 添加当前序列的 attention mask。\n        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n        # 如果使用 past，需将 attention_mask 补齐为 [batch, past_seq_len + cur_seq_len]，前面是 1 表示已有上下文。\n        if self.use_past:\n            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n            ordered_inputs[\"attention_mask\"] = torch.cat(\n                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n            )\n        # 返回构建好的 dummy 输入，包括 input_ids、attention_mask 和 past_key_values（如果启用）。\n        return ordered_inputs\n    # 指定默认导出 ONNX 模型时使用的 ONNX opset 版本\n    # default_onnx_opset = 13 是 GPT2 导出为 ONNX 时推荐使用的操作符版本号，确保兼容性和功能完整性。\n    @property\n    def default_onnx_opset(self) -> int:\n        return 13\n__all__ = [\"GPT2Config\", \"GPT2OnnxConfig\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:19:25.789919Z","iopub.execute_input":"2025-05-30T14:19:25.790186Z","iopub.status.idle":"2025-05-30T14:19:25.800894Z","shell.execute_reply.started":"2025-05-30T14:19:25.790166Z","shell.execute_reply":"2025-05-30T14:19:25.800129Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"onnxConfig=GPT2OnnxConfig(configuration)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:03:56.515130Z","iopub.execute_input":"2025-05-30T14:03:56.515674Z","iopub.status.idle":"2025-05-30T14:03:56.518966Z","shell.execute_reply.started":"2025-05-30T14:03:56.515654Z","shell.execute_reply":"2025-05-30T14:03:56.518131Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"onnxConfig.num_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:05:48.238478Z","iopub.execute_input":"2025-05-30T14:05:48.239147Z","iopub.status.idle":"2025-05-30T14:05:48.243782Z","shell.execute_reply.started":"2025-05-30T14:05:48.239125Z","shell.execute_reply":"2025-05-30T14:05:48.243160Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"onnxConfig.inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:06:03.512061Z","iopub.execute_input":"2025-05-30T14:06:03.512310Z","iopub.status.idle":"2025-05-30T14:06:03.517173Z","shell.execute_reply.started":"2025-05-30T14:06:03.512293Z","shell.execute_reply":"2025-05-30T14:06:03.516457Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('input_ids', {0: 'batch', 1: 'sequence'}),\n             ('attention_mask', {0: 'batch', 1: 'sequence'})])"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"onnxConfig=GPT2OnnxConfig(configuration,use_past=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:06:46.638989Z","iopub.execute_input":"2025-05-30T14:06:46.639269Z","iopub.status.idle":"2025-05-30T14:06:46.643214Z","shell.execute_reply.started":"2025-05-30T14:06:46.639249Z","shell.execute_reply":"2025-05-30T14:06:46.642470Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"onnxConfig.inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:06:52.813467Z","iopub.execute_input":"2025-05-30T14:06:52.813796Z","iopub.status.idle":"2025-05-30T14:06:52.820684Z","shell.execute_reply.started":"2025-05-30T14:06:52.813776Z","shell.execute_reply":"2025-05-30T14:06:52.820041Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('input_ids', {0: 'batch', 1: 'sequence'}),\n             ('past_key_values.0.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.0.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.1.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.1.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.2.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.2.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.3.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.3.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.4.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.4.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.5.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.5.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.6.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.6.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.7.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.7.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.8.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.8.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.9.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.9.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.10.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.10.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.11.key',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('past_key_values.11.value',\n              {0: 'batch', 2: 'past_sequence + sequence'}),\n             ('attention_mask', {0: 'batch', 1: 'past_sequence + sequence'})])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"onnxConfig.num_attention_heads","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:07:44.552888Z","iopub.execute_input":"2025-05-30T14:07:44.553576Z","iopub.status.idle":"2025-05-30T14:07:44.557668Z","shell.execute_reply.started":"2025-05-30T14:07:44.553553Z","shell.execute_reply":"2025-05-30T14:07:44.557068Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"onnxConfig.default_onnx_opset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:09:01.888004Z","iopub.execute_input":"2025-05-30T14:09:01.888256Z","iopub.status.idle":"2025-05-30T14:09:01.892887Z","shell.execute_reply.started":"2025-05-30T14:09:01.888238Z","shell.execute_reply":"2025-05-30T14:09:01.892225Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"13"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('openai-community/gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:24:05.310600Z","iopub.execute_input":"2025-05-30T14:24:05.310839Z","iopub.status.idle":"2025-05-30T14:24:14.585277Z","shell.execute_reply.started":"2025-05-30T14:24:05.310825Z","shell.execute_reply":"2025-05-30T14:24:14.584429Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02d1dce668d4b959e77977a6bcc80a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80c0ab983454f1fa3f902312d1da528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564e96da2bf7409da2d029a02f490713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd1f95498424551b425434e5b923b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d835024765481eb0d3e7d288690201"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"onnxConfig.generate_dummy_inputs(tokenizer,framework=\"pt\").keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:26:48.949038Z","iopub.execute_input":"2025-05-30T14:26:48.949715Z","iopub.status.idle":"2025-05-30T14:26:48.957150Z","shell.execute_reply.started":"2025-05-30T14:26:48.949694Z","shell.execute_reply":"2025-05-30T14:26:48.956661Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"odict_keys(['input_ids', 'past_key_values', 'attention_mask'])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"len([i for i in onnxConfig.generate_dummy_inputs(tokenizer,framework=\"pt\").values()][1]) # past_key_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:27:56.705981Z","iopub.execute_input":"2025-05-30T14:27:56.706214Z","iopub.status.idle":"2025-05-30T14:27:56.713199Z","shell.execute_reply.started":"2025-05-30T14:27:56.706199Z","shell.execute_reply":"2025-05-30T14:27:56.712659Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}