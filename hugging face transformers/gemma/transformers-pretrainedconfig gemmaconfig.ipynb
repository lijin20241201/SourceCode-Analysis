{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers.configuration_utils import PretrainedConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T03:30:42.033553Z","iopub.execute_input":"2025-10-03T03:30:42.033883Z","iopub.status.idle":"2025-10-03T03:30:42.039283Z","shell.execute_reply.started":"2025-10-03T03:30:42.033860Z","shell.execute_reply":"2025-10-03T03:30:42.038189Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 1. attribute_map 的作用\n# key：外部使用的属性名（“别名”）。\n# value：Hugging Face 内部配置里使用的“规范化名称”（唯一属性）。\n# 这样做的好处是：\n# 兼容不同模型代码里习惯用的不同名字。\n# 在统一的 API（例如 AutoConfig、from_pretrained）里，始终能映射到 Hugging Face 自己的标准字段。\n# 因为 Hugging Face 想统一：\n# 内部保存和导出 checkpoint 时，用 标准字段（如 hidden_size）。\n# 外部用户或者不同论文/框架的命名习惯，也能无缝兼容。\n# 这样 Hugging Face 的 AutoModel / AutoConfig 就能加载不同来源的模型，而不需要用户手动修改配置。\n# 1. base_config_key 在 Hugging Face 里的作用\n# 场景：很多模型是“组合模型”（composite models），比如：\n# EncoderDecoderModel → 由一个 encoder 配置 + 一个 decoder 配置组合而成。\n# VisionTextDualEncoder → 一个 vision 模型配置 + 一个 text 模型配置。\n# 问题：这种组合模型保存的时候，config.json 里会有多个子配置。那怎么区分哪个是“主”配置呢？\n# 这时就用到 base_config_key，它标识 哪个子配置是基础配置，也就是整个模型的核心。\n# 3. 为什么要有 base_config_key\n# 因为 Hugging Face 里很多工具（比如 AutoTokenizer, AutoConfig）需要知道：\n# 当我加载这个组合模型时，哪个子配置才是我应该默认用的？\n# 比如 encoder-decoder 模型，它可能需要用 encoder 的 vocab 来初始化 tokenizer。\n# 所以 base_config_key 就是一个“指路标”。\n# ✅ 总结\n# base_config_key = 在组合模型配置里，标识“主配置”是谁。\n# 它和推理阶段的“小模型过滤候选 token → 大模型精排”完全不是一回事。\n# multi_label_classification 多标签分类\n# 每个样本可以同时属于 多个类别。\n# 比如新闻分类：一条新闻可能既是 体育，又涉及 娱乐。\n# 普通赋值\n# torch_dtype = kwargs.pop(\"torch_dtype\", None)\n# if torch_dtype is not None:\n#     ...\n# 赋值操作和条件判断是 两步。\n# 不能写成\n# if (torch_dtype = kwargs.pop(\"torch_dtype\", None)) is not None:  # ❌\n# 因为 = 是语句，不是表达式，Python 不允许在 if 条件里直接使用。\n# 海象赋值 :=\n# if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n# torch_dtype := ... 是一个 表达式，会先把值赋给 torch_dtype，\n# 同时整个表达式的值就是 kwargs.pop(...) 的返回值，\n# 因此可以直接在 if 条件里判断。\n# 正确做法是：先初始化模型，再调用方法启用梯度检查点：\n# model = MyModel(config)\n# model.gradient_checkpointing_enable()\n# 如果用 Trainer API 训练模型，可以在 TrainingArguments 里直接设置，不需要在 config 里传：\n# training_args = TrainingArguments(\n#     gradient_checkpointing=True,\n#     ...\n# )\n# 1. 不能直接实例化 PretrainedConfig\n# PretrainedConfig 是基类，不能直接创建实例。\n# 示例用 BertConfig（继承自 PretrainedConfig）来演示。\n# 2. 从预训练配置加载模型配置\n# config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\")\n# 从 Hugging Face Hub 下载 BERT base uncased 的配置，并缓存到本地。\n# config = BertConfig.from_pretrained(\"./test/saved_model/\")\n# 从本地保存的目录加载配置（以前用 save_pretrained 保存的配置）。\n# config = BertConfig.from_pretrained(\"./test/saved_model/my_configuration.json\")\n# 也可以直接从具体的 JSON 文件加载配置。\n# . 使用额外参数覆盖配置\n# config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True, foo=False)\n# # assert config.output_attentions == True\n# 获取未使用的参数\n# config, unused_kwargs = BertConfig.from_pretrained(\n#     \"google-bert/bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\n# )\n# assert config.output_attentions == True\n# assert unused_kwargs == {\"foo\": False}\n# return_unused_kwargs=True 时，返回一个元组 (config, unused_kwargs)。\n# 任何 BertConfig 不认识的参数（这里是 foo=False）会收集到 unused_kwargs。\n# BertConfig() 默认就是 base 模型的默认配置（hidden_size=768，num_attention_heads=12 等）。\n# 如果你用 BertConfig.from_pretrained(\"bert-large-uncased\")：\n# 得到的配置是 large 模型的参数（hidden_size=1024，num_attention_heads=16 等）。\n# 与 BertConfig() 的 base 默认值相比，自然有差异。\n# 但是 use_diff=True 的机制只会考虑：\n# 用户在实例上修改过的字段\n# 它不会自动把 from_pretrained 加载的不同预训练模型参数也视作“差异”，除非这些字段被显式修改过。\n# 换句话说：\n# 用户修改的字段\t✅ 会保存\n# 从 from_pretrained(\"large\") 加载的字段\t❌ 不算差异，除非再手动修改\n# 总结：use_diff=True 关注的是用户显式修改过的字段，而不是不同预训练模型本身的配置差异。\n# 用户修改配置通常发生在以下几种情况：\n# 1. 改变模型结构\n\n# 用户希望在原始预训练模型基础上调整层数、隐藏维度、注意力头数等：\n# config = BertConfig.from_pretrained(\"bert-base-uncased\")\n# config.hidden_size = 1024         # 改大隐藏层维度\n# config.num_attention_heads = 16   # 改变注意力头数\n# 2. 改变训练行为相关参数\n\n# 控制输出或行为的参数，如是否输出中间隐藏状态、注意力权重：\n# config.output_hidden_states = True\n# config.output_attentions = True\n# 3. 改变任务相关设置\n\n# 对于下游任务，修改标签数量或问题类型：\n# config.num_labels = 5\n# config.problem_type = \"multi_label_classification\"\n# 4. 自定义 tokenizer 或特殊 token\n\n# 指定起始、结束、填充或分隔符 token id：\n# config.pad_token_id = 0\n# config.bos_token_id = 101\n# config.eos_token_id = 102\n# 5. 实验性调整或微调\n\n# 例如修改 tie_word_embeddings、chunk_size_feed_forward 等优化训练速度或内存的参数。\n\n# ✅ 总结：\n\n# 用户修改配置 = 显式改变了原本从 from_pretrained 或默认配置加载的字段\n\n# 这些修改才会在 use_diff=True 时被保存到 JSON 文件\n\n# 否则，加载时依然会使用模型默认值或从预训练模型加载的值\n# use_diff\t保存内容\n# False（默认）\t保存全部字段：包括默认值和用户修改的字段。JSON 是完整配置，加载时不用依赖默认值。\n# True\t只保存与默认值不同的字段：只写入用户修改的字段或与类默认值不同的字段，加载时会用默认值补全未保存字段。\n# 默认保存全部字段 → 兼容性最强\n\n# use_diff=True → 文件更小，只保存“差异”，方便追踪用户修改\n# 在 from_pretrained 方法里，实现“差异保存 JSON 也能完整加载”的机制主要体现在这几步：\n# 获取配置字典\n# config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n# get_config_dict 会读取 JSON 文件（无论是完整配置还是差异 JSON）\n\n# 如果是差异 JSON，只返回修改过的字段\n# 实例化配置\n# return cls.from_dict(config_dict, **kwargs)\n# from_dict 会创建一个 默认配置实例\n\n# 然后用 config_dict 中的字段 覆盖默认值\n\n# 这一步就完成了“差异 JSON 补全未保存字段”的逻辑\n# 关键点：__init__ 里有 所有字段的默认值：\n# def __init__(self, hidden_size=768, num_attention_heads=12, ...):\n#     self.hidden_size = hidden_size\n#     ...\n# 因此 cls(**{\"hidden_size\":1024}) 会：\n# 用 JSON 中提供的值覆盖默认值（hidden_size=1024）。\n# 其他未修改的字段使用 __init__ 中的默认值（比如 num_attention_heads=12）。\n# cls(**config_dict) 这一行代码执行时，cls 是 PretrainedConfig 的 具体子类，比如 BertConfig、GPT2Config 等。\n\n# config_dict 是从模型的配置文件加载的字典，这个字典会包含加载时的所有配置（可能是默认配置，也可能是修改过的配置）。\n\n# 默认参数的设置：\n\n# BertConfig（或其他具体模型类）会在 __init__ 方法中定义模型的默认参数（比如 hidden_size=768、num_attention_heads=12）。\n\n# 如果 config_dict 里没有这些参数，BertConfig 的构造函数会使用 默认值（即 __init__ 中定义的默认值）。\n\n# use_diff=True 保存的差异：\n\n# 如果在保存配置时设置了 use_diff=True，那么只会保存与 默认配置 的差异部分，比如 {\"hidden_size\": 1024}。\n\n# 加载配置时（即 cls(**config_dict)），config_dict 中没有差异的部分会使用 子类 __init__ 方法中定义的默认值。例如，如果 config_dict 中没有 num_attention_heads，它会使用 BertConfig 中的默认值 num_attention_heads=12。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PretrainedConfig(PushToHubMixin):\n    model_type: str = \"\" # 模型类型（例如 \"bert\", \"gpt2\"），子类通常会覆盖\n    base_config_key: str = \"\"  # 用于标识基础配置的 key（一般在组合模型中使用）\n    sub_configs: dict[str, type[\"PretrainedConfig\"]] = {} # 子配置的注册表，key 为名字，value 为 PretrainedConfig 类型\n    has_no_defaults_at_init: bool = False # 初始化时是否没有默认值（控制初始化逻辑）\n    attribute_map: dict[str, str] = {}  # 属性映射表，用于将外部访问的属性名映射到内部实际属性名\n    base_model_tp_plan: Optional[dict[str, Any]] = None # tensor parallel 的规划（张量并行），通常在大模型分布式时使用\n    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None # pipeline parallel 的规划（流水线并行）\n    base_model_ep_plan: Optional[dict[str, tuple[list[str]]]] = None  # expert parallel 的规划（专家并行，MoE场景）\n    _auto_class: Optional[str] = None # 自动类名（用于 AutoConfig 自动加载）\n    def __setattr__(self, key, value):\n        # 当设置属性时，检查 key 是否在 attribute_map 中\n        if key in super().__getattribute__(\"attribute_map\"):\n            key = super().__getattribute__(\"attribute_map\")[key] # 如果存在映射关系，则替换为内部真正的 key\n        super().__setattr__(key, value) # 调用父类方法实际设置属性\n    def __getattribute__(self, key): \n        # 获取属性时，避免递归调用 attribute_map 本身\n        if key != \"attribute_map\" and key in super().__getattribute__(\"attribute_map\"):\n            key = super().__getattribute__(\"attribute_map\")[key] # 如果 key 在映射表中，取对应的内部属性名\n        return super().__getattribute__(key)  # 返回实际属性值\n    def __init__(\n        self,\n        *,\n        # All models common arguments\n        output_hidden_states: bool = False,   # 是否返回所有层的 hidden states\n        output_attentions: bool = False,   # 是否返回 attention 权重\n        return_dict: bool = True,     # 模型输出是否使用字典格式\n        torchscript: bool = False,  # 是否启用 TorchScript\n        dtype: Optional[Union[str, \"torch.dtype\"]] = None # 模型参数的数据类型 (float32, float16等)\n        # Common arguments\n        pruned_heads: Optional[dict[int, list[int]]] = None,   # 剪枝的 attention head 记录\n        tie_word_embeddings: bool = True,  # 是否共享输入和输出 embedding\n        chunk_size_feed_forward: int = 0,  # FFN 层的分块大小（大模型可节省内存）\n        is_encoder_decoder: bool = False, # 是否为 encoder-decoder 模型\n        is_decoder: bool = False,  # 是否是 decoder（在 encoder-decoder 模型中区分）\n        cross_attention_hidden_size: Optional[int] = None,  # cross-attn 的 hidden size\n        add_cross_attention: bool = False,  # 是否在 decoder 里加 cross-attn\n        tie_encoder_decoder: bool = False,   # encoder/decoder 是否共享权重\n        # Fine-tuning task arguments\n        architectures: Optional[list[str]] = None,  # 模型架构名（如 [\"BertForSequenceClassification\"]）\n        finetuning_task: Optional[str] = None, # 微调任务名称（如 \"mrpc\", \"sst2\"）\n        id2label: Optional[dict[int, str]] = None,  # 分类任务：id → label 名称\n        label2id: Optional[dict[str, int]] = None,  # 分类任务：label → id\n        num_labels: Optional[int] = None,  # 分类任务的类别数\n        task_specific_params: Optional[dict[str, Any]] = None, # 任务特定的配置\n        problem_type: Optional[str] = None,   # 任务类型: regression / single_label_classification / multi_label_classification\n        # Tokenizer kwargs\n        tokenizer_class: Optional[str] = None,  # 指定 tokenizer 类名\n        prefix: Optional[str] = None,   # 输入前缀（seq2seq 模型用）\n        bos_token_id: Optional[int] = None,   # 句子开始 token id\n        pad_token_id: Optional[int] = None,  # pad token id\n        eos_token_id: Optional[int] = None,  # 句子结束 token id\n        sep_token_id: Optional[int] = None,  # 分隔符 token id\n        decoder_start_token_id: Optional[int] = None, # decoder 起始 token id\n        **kwargs, # 允许扩展参数，保证向后兼容\n    ):\n        # 参数合法性检查\n        if label2id is not None and not isinstance(label2id, dict):\n            raise ValueError(\"Argument label2id should be a dictionary.\")\n        if id2label is not None and not isinstance(id2label, dict):\n            raise ValueError(\"Argument id2label should be a dictionary.\")\n        if num_labels is not None and id2label is not None and len(id2label) != num_labels:\n            logger.warning(\n                f\"You passed `num_labels={num_labels}` which is incompatible to \"\n                f\"the `id2label` map of length `{len(id2label)}`.\"\n            )\n        if problem_type is not None and problem_type not in (\n            \"regression\",\n            \"single_label_classification\",\n            \"multi_label_classification\",\n        ):\n            raise ValueError(\n                f\"The config parameter `problem_type` was not understood: received {problem_type} \"\n                \"but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid.\"\n            )\n        # 为了兼容旧版 `torch_dtype` 参数，而不是使用更简单的 `dtype`\n        # 不发出警告，否则会一直触发，因为大多数 hub 上的配置都有 `torch_dtype`\n        # 向后兼容 torch_dtype 参数（hub 上很多 config.json 用的 torch_dtype）\n        # 先执行 kwargs.pop(\"torch_dtype\", None)，得到一个值（可能是 None 或实际 dtype）。\n        # 将这个值赋给 torch_dtype。\n        # 将这个值用于 is not None 判断。\n        # 如果条件成立，就进入 if 语句块。\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            dtype = dtype if dtype is not None else torch_dtype # 如果 dtype 和 torch_dtype 同时存在，优先 dtype\n        if dtype is not None and isinstance(dtype, str) and is_torch_available():\n            # we will start using self.dtype in v5, but to be consistent with\n            # from_pretrained's dtype arg convert it to an actual torch.dtype object\n            # 如果 dtype 是字符串（如 \"float32\"），转成 torch.dtype 对象\n            import torch\n            dtype = getattr(torch, dtype)\n        # Attributes common for all models\n        self.return_dict = return_dict\n        self.output_hidden_states = output_hidden_states\n        self.torchscript = torchscript\n        self.dtype = dtype\n        self._output_attentions = output_attentions  # has public property\n        # Less common kwargs, only used by some models\n        self.pruned_heads = pruned_heads if pruned_heads is not None else {}\n        self.tie_word_embeddings = tie_word_embeddings\n        self.chunk_size_feed_forward = chunk_size_feed_forward\n        # Encoder-decoder models attributes\n        self.is_encoder_decoder = is_encoder_decoder\n        self.is_decoder = is_decoder  # used in encoder-decoder models to differentiate encoder from decoder\n        self.cross_attention_hidden_size = cross_attention_hidden_size\n        self.add_cross_attention = add_cross_attention\n        self.tie_encoder_decoder = tie_encoder_decoder\n        # Fine-tuning task attributes\n        self.architectures = architectures\n        self.finetuning_task = finetuning_task\n        self.id2label = id2label\n        self.label2id = label2id\n        self.task_specific_params = task_specific_params\n        self.problem_type = problem_type\n        if self.id2label is None:\n            self._create_id_label_maps(num_labels if num_labels is not None else 2)\n        else:\n            # Keys are always strings in JSON so convert ids to int here.\n            self.id2label = {int(key): value for key, value in self.id2label.items()}\n        # Tokenizer attributes\n        self.tokenizer_class = tokenizer_class\n        self.prefix = prefix\n        self.bos_token_id = bos_token_id\n        self.pad_token_id = pad_token_id\n        self.eos_token_id = eos_token_id\n        self.sep_token_id = sep_token_id\n        self.decoder_start_token_id = decoder_start_token_id\n        # 序列生成参数（兼容旧版本） 这些参数未来会弃用，但现在依然可以加载\n        for parameter_name, default_value in self._get_global_generation_defaults().items():\n            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n        # Name or path to the pretrained checkpoint\n        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))  # checkpoint 名或路径\n        self._commit_hash = kwargs.pop(\"_commit_hash\", None)  # Git commit hash\n        # Attention 实现方式\n        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n        # Transformers 版本号\n        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n        # 梯度检查点参数（将被弃用）\n        if kwargs.get(\"gradient_checkpointing\", False):\n            warnings.warn(\n                \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n                \"Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the \"\n                \"`Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\"\n            )\n        # Additional attributes without default values\n        for key, value in kwargs.items():\n            try:\n                setattr(self, key, value)\n            except AttributeError as err:\n                logger.error(f\"Can't set {key} with value {value} for {self}\")\n                raise err\n        # TODO: remove later, deprecated arguments for TF models\n        self.tf_legacy_loss = kwargs.pop(\"tf_legacy_loss\", False)\n        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n    def _create_id_label_maps(self, num_labels: int):\n        self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n    @property\n    def name_or_path(self) -> Optional[str]:\n        return getattr(self, \"_name_or_path\", None)\n    @name_or_path.setter\n    def name_or_path(self, value):\n        self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n    @property\n    def output_attentions(self):\n        \"\"\"\n        `bool`: Whether or not the model should returns all attentions.\n        \"\"\"\n        return self._output_attentions\n    @output_attentions.setter\n    def output_attentions(self, value: bool):\n        # If we set `output_attentions` explicitly before the attn implementation, dispatch eager\n        if value and self._attn_implementation is None:\n            self._attn_implementation = \"eager\"\n        if value and self._attn_implementation != \"eager\":\n            raise ValueError(\n                \"The `output_attentions` attribute is not supported when using the `attn_implementation` set to \"\n                f\"{self._attn_implementation}. Please set it to 'eager' instead.\"\n            )\n        self._output_attentions = value\n    @property\n    def use_return_dict(self) -> bool:\n        \"\"\"\n        `bool`: Whether or not return [`~utils.ModelOutput`] instead of tuples.\n        \"\"\"\n        # If torchscript is set, force `return_dict=False` to avoid jit errors\n        return self.return_dict and not self.torchscript\n    @property\n    def num_labels(self) -> int:\n        \"\"\"\n        `int`: The number of labels for classification models.\n        \"\"\"\n        return len(self.id2label)\n    @num_labels.setter\n    def num_labels(self, num_labels: int):\n        # we do not store `num_labels` attribute in config, but instead\n        # compute it based on the length of the `id2label` map\n        if self.id2label is None or self.num_labels != num_labels:\n            self._create_id_label_maps(num_labels)\n\n    @property\n    def _attn_implementation(self):\n        return self._attn_implementation_internal\n\n    @_attn_implementation.setter\n    def _attn_implementation(self, value: Optional[Union[str, dict]]):\n        \"\"\"We set it recursively on the sub-configs as well\"\"\"\n        # Set if for current config\n        current_attn = getattr(self, \"_attn_implementation\", None)\n        attn_implementation = value if not isinstance(value, dict) else value.get(\"\", current_attn)\n        self._attn_implementation_internal = attn_implementation\n\n        # Set it recursively on the subconfigs\n        for subconfig_key in self.sub_configs:\n            subconfig = getattr(self, subconfig_key, None)\n            if subconfig is not None:\n                current_subconfig_attn = getattr(subconfig, \"_attn_implementation\", None)\n                sub_implementation = (\n                    value if not isinstance(value, dict) else value.get(subconfig_key, current_subconfig_attn)\n                )\n                subconfig._attn_implementation = sub_implementation\n    @property\n    def torch_dtype(self):\n        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        return self.dtype\n\n    @torch_dtype.setter\n    def torch_dtype(self, value):\n        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        self.dtype = value\n\n    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n        # 设置 Hugging Face hub token，如果 kwargs 中有 token，会处理\n        self._set_token_in_kwargs(kwargs)\n         # 确保传入的路径是目录而不是文件\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n        # 获取非默认的生成参数（generation 参数），比如修改过的 decoding 设置\n        non_default_generation_parameters = self._get_non_default_generation_parameters()\n        if len(non_default_generation_parameters) > 0:\n            # TODO: 将来如果用户修改了加载的 config，这里应该抛异常（现在只给警告）\n            warnings.warn(\n                \"Some non-default generation parameters are set in the model config. These should go into either a) \"\n                \"`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file \"\n                \"(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).\"\n                \"This warning will become an exception in the future.\"\n                f\"\\nNon-default generation parameters: {str(non_default_generation_parameters)}\",\n                UserWarning,\n            )\n        # 创建保存目录，如果目录不存在就创建\n        os.makedirs(save_directory, exist_ok=True)\n        # 如果要 push 到 Hugging Face hub\n        if push_to_hub:\n            commit_message = kwargs.pop(\"commit_message\", None) # 获取提交信息\n            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])  # 获取 repo 名称\n            repo_id = self._create_repo(repo_id, **kwargs) # 创建远程 repo\n            files_timestamps = self._get_files_timestamps(save_directory) # 记录当前文件时间戳，用于增量上传\n        # transformers_weights 属性不需要序列化，保存前删除\n        if \"transformers_weights\" in self:\n            delattr(self, \"transformers_weights\")\n        # 如果是自定义配置类，保存自定义对象文件，以便从 Hub 加载\n        if self._auto_class is not None:\n            custom_object_save(self, save_directory, config=self)\n        # 定义输出配置文件路径（使用预定义名字 CONFIG_NAME，例如 config.json）\n        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n        # 将配置保存为 JSON 文件，可选只保存有差异的字段（use_diff=True）\n        # 这里的“差异”指的是 当前配置实例 (self) 与该配置类的默认配置 (PretrainedConfig 或派生类的默认值) 的差异。\n        self.to_json_file(output_config_file, use_diff=True)\n        logger.info(f\"Configuration saved in {output_config_file}\")\n        # 如果要 push 到 Hub，上传修改过的文件\n        if push_to_hub:\n            self._upload_modified_files(\n                save_directory,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=kwargs.get(\"token\"),\n            )\n\n    @staticmethod\n    def _set_token_in_kwargs(kwargs, token=None):\n       \n        # Some model config classes like CLIP define their own `from_pretrained` without the new argument `token` yet.\n        if token is None:\n            token = kwargs.pop(\"token\", None)\n        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n\n        if use_auth_token is not None:\n            warnings.warn(\n                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                FutureWarning,\n            )\n            if token is not None:\n                raise ValueError(\n                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                )\n            token = use_auth_token\n\n        if token is not None:\n            kwargs[\"token\"] = token\n\n    @classmethod\n    def from_pretrained(\n        cls: type[SpecificPretrainedConfigType],\n        pretrained_model_name_or_path: Union[str, os.PathLike],\n        cache_dir: Optional[Union[str, os.PathLike]] = None,\n        force_download: bool = False,\n        local_files_only: bool = False,\n        token: Optional[Union[str, bool]] = None,\n        revision: str = \"main\",\n        **kwargs,\n    ) -> SpecificPretrainedConfigType:\n        # 文档示例：\n        # - 不能直接实例化 PretrainedConfig，所以用 BertConfig 演示\n        # - 支持从 Hugging Face Hub 或本地目录 / JSON 文件加载配置\n        # - 可以用额外参数覆盖默认字段\n        # - 可返回未使用的 kwargs\n        kwargs[\"cache_dir\"] = cache_dir # 将常用参数放入 kwargs，以便后续统一处理\n        kwargs[\"force_download\"] = force_download\n        kwargs[\"local_files_only\"] = local_files_only\n        kwargs[\"revision\"] = revision\n        # 处理 Hugging Face token（如果需要访问私有模型）\n        cls._set_token_in_kwargs(kwargs, token)\n        # 从预训练模型路径或 Hub 获取配置字典，同时返回未使用的 kwargs\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n        # 如果存在 base_config_key（组合模型场景），提取基础配置\n        if cls.base_config_key and cls.base_config_key in config_dict:\n            config_dict = config_dict[cls.base_config_key]\n        # 检查配置的 model_type 是否和当前类匹配\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            # 有些组合模型可能没有 base_config_key，例如 LlamaConfig\n            # 遍历 config_dict 的子字典，看看有没有匹配的 model_type\n            for v in config_dict.values():\n                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n                    config_dict = v\n            # 如果仍然不匹配，发出警告\n            # raise warning only if we still can't see a match in `model_type`\n            if config_dict[\"model_type\"] != cls.model_type:\n                logger.warning(\n                    f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n                    f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n                )\n\n        return cls.from_dict(config_dict, **kwargs) # 最后通过 from_dict 将字典转换为配置实例\n    @classmethod\n    def get_config_dict(\n        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n    ) -> tuple[dict[str, Any], dict[str, Any]]:\n        cls._set_token_in_kwargs(kwargs)\n        original_kwargs = copy.deepcopy(kwargs)\n        # Get config dict associated with the base config file\n        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n        if config_dict is None:\n            return {}, kwargs\n        if \"_commit_hash\" in config_dict:\n            original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n        # That config file may point us toward another config file to use.\n        if \"configuration_files\" in config_dict:\n            configuration_file = get_configuration_file(config_dict[\"configuration_files\"])\n            config_dict, kwargs = cls._get_config_dict(\n                pretrained_model_name_or_path, _configuration_file=configuration_file, **original_kwargs\n            )\n\n        return config_dict, kwargs\n\n    @classmethod\n    def _get_config_dict(\n        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n    ) -> tuple[dict[str, Any], dict[str, Any]]:\n        # 指定缓存目录，存放下载的模型/配置文件。默认使用 transformers 的全局缓存路径。\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        # 是否强制重新下载，即使缓存中已有文件也覆盖下载。默认 False。\n        force_download = kwargs.pop(\"force_download\", False)\n        # 断点续传下载的参数，允许恢复未完成的下载。\n        resume_download = kwargs.pop(\"resume_download\", None)\n        # 下载时使用的代理设置，例如 {\"http\": \"http://127.0.0.1:8080\"}。\n        proxies = kwargs.pop(\"proxies\", None)\n        # Hugging Face Hub 的访问令牌，用于私有模型下载或推送。\n        token = kwargs.pop(\"token\", None)\n        # 是否只使用本地文件，不从 Hugging Face Hub 下载。默认 False。\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        # 模型的版本或分支，例如 \"main\" 或 commit hash。用于指定下载特定版本。\n        revision = kwargs.pop(\"revision\", None)\n        # 是否信任远程模型的自定义代码（仅在 Auto 类使用时有效）。用于安全检查。\n        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n        subfolder = kwargs.pop(\"subfolder\", \"\") # 指定子文件夹路径，如果配置文件在模型目录下的子文件夹中。\n        # 内部参数，用于标记是否通过 pipeline 调用获取配置。用于用户代理记录\n        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n        from_auto_class = kwargs.pop(\"_from_auto\", False) # 内部参数，用于标记是否由 Auto 类调用。用于用户代理记录。\n        commit_hash = kwargs.pop(\"_commit_hash\", None) # 指定模型或配置的 Hub commit hash，用于版本控制和 reproducibility。\n        # 特殊参数，指定 GGUF 格式 checkpoint 文件（一些大模型使用 GGUF 存储配置）。\n        gguf_file = kwargs.get(\"gguf_file\")\n        # trust_remote_code 参数仅对 Auto 类有效，这里不生效\n        if trust_remote_code is True:\n            logger.warning(\n                \"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\"\n                \" ignored.\"\n            )\n        # 用于记录用户代理信息，传给缓存或下载函数\n        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n        if from_pipeline is not None:\n            user_agent[\"using_pipeline\"] = from_pipeline\n\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n         # 判断路径是否为本地目录\n        is_local = os.path.isdir(pretrained_model_name_or_path)\n        if os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n            # 特殊情况：直接提供了本地文件\n            resolved_config_file = pretrained_model_name_or_path\n            is_local = True\n        elif is_remote_url(pretrained_model_name_or_path): # 远程 URL 下载\n            configuration_file = pretrained_model_name_or_path if gguf_file is None else gguf_file\n            resolved_config_file = download_url(pretrained_model_name_or_path)\n        else: # 本地文件夹或缓存路径\n            configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n             # 从本地文件夹 / 缓存 / Hub 下载配置\n            try:\n                # Load from local folder or from cache or download from model Hub and cache\n                resolved_config_file = cached_file(\n                    pretrained_model_name_or_path,\n                    configuration_file,\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    proxies=proxies,\n                    resume_download=resume_download,\n                    local_files_only=local_files_only,\n                    token=token,\n                    user_agent=user_agent,\n                    revision=revision,\n                    subfolder=subfolder,\n                    _commit_hash=commit_hash,\n                )\n                if resolved_config_file is None:\n                    return None, kwargs\n                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)  # 提取 commit hash，用于记录 Hub 版本\n            except OSError: # 缓存文件读取错误，抛出环境异常\n                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                # the original exception.\n                raise\n            except Exception:   # 其他异常，给出通用错误提示\n                # For any other exception, we throw a generic error.\n                raise OSError(\n                    f\"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it\"\n                    \" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\n                    f\" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory\"\n                    f\" containing a {configuration_file} file\"\n                )\n\n        try:\n            if gguf_file:  # 支持 GGUF 格式 checkpoint（某些 LLM 模型）\n                config_dict = load_gguf_checkpoint(resolved_config_file, return_tensors=False)[\"config\"]\n            else:\n                # 从 JSON 文件读取配置字典\n                config_dict = cls._dict_from_json_file(resolved_config_file)\n            config_dict[\"_commit_hash\"] = commit_hash # 保存 commit hash\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            raise OSError(f\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\")\n\n        if is_local:\n            logger.info(f\"loading configuration file {resolved_config_file}\")\n        else:\n            logger.info(f\"loading configuration file {configuration_file} from cache at {resolved_config_file}\")\n\n        # timm 模型没有保存 model_type，手动补上\n        if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n            config_dict[\"model_type\"] = \"timm_wrapper\"\n\n        return config_dict, kwargs # 返回配置字典和剩余未使用的 kwargs\n\n    @classmethod\n    def from_dict(\n        cls: type[SpecificPretrainedConfigType], config_dict: dict[str, Any], **kwargs\n    ) -> SpecificPretrainedConfigType:\n        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n        # Those arguments may be passed along for our internal telemetry.\n        # We remove them so they don't appear in `return_unused_kwargs`.\n        kwargs.pop(\"_from_auto\", None)\n        kwargs.pop(\"_from_pipeline\", None)\n        # The commit hash might have been updated in the `config_dict`, we don't want the kwargs to erase that update.\n        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n        # For BC on the old `torch_dtype`\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n            # If both are present, use `dtype`\n            kwargs[\"dtype\"] = kwargs.get(\"dtype\", torch_dtype)\n\n        # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\n        config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n        # 具体的子类会设置 hidden_size=1024 这些参数 所以可以用use_diff=True 只保存差异\n        config = cls(**config_dict)\n\n        if hasattr(config, \"pruned_heads\"):\n            config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}\n\n        # Update config with kwargs if needed\n        if \"num_labels\" in kwargs and \"id2label\" in kwargs:\n            num_labels = kwargs[\"num_labels\"]\n            id2label = kwargs[\"id2label\"] if kwargs[\"id2label\"] is not None else []\n            if len(id2label) != num_labels:\n                raise ValueError(\n                    f\"You passed along `num_labels={num_labels}` with an incompatible id to label map: \"\n                    f\"{kwargs['id2label']}. Since those arguments are inconsistent with each other, you should remove \"\n                    \"one of them.\"\n                )\n        to_remove = []\n        # kwargs 的更新\n        # 用额外的 kwargs 覆盖实例字段\n        # 这也是一种用户修改字段的机制，但不是默认值补全\n        for key, value in kwargs.items(): \n            if hasattr(config, key):\n                current_attr = getattr(config, key)\n                # To authorize passing a custom subconfig as kwarg in models that have nested configs.\n                # We need to update only custom kwarg values instead and keep other attributes in subconfig.\n                if isinstance(current_attr, PretrainedConfig) and isinstance(value, dict):\n                    current_attr_updated = current_attr.to_dict()\n                    current_attr_updated.update(value)\n                    value = current_attr.__class__(**current_attr_updated)\n                setattr(config, key, value)\n                if key != \"dtype\":\n                    to_remove.append(key)\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        logger.info(f\"Model config {config}\")\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    @classmethod\n    def from_json_file(\n        cls: type[SpecificPretrainedConfigType], json_file: Union[str, os.PathLike]\n    ) -> SpecificPretrainedConfigType:\n        \n        config_dict = cls._dict_from_json_file(json_file)\n        return cls(**config_dict)\n\n    @classmethod\n    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n        with open(json_file, encoding=\"utf-8\") as reader:\n            text = reader.read()\n        return json.loads(text)\n\n    def __eq__(self, other):\n        return isinstance(other, PretrainedConfig) and (self.__dict__ == other.__dict__)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n\n    def __iter__(self):\n        yield from self.__dict__\n\n    def to_diff_dict(self) -> dict[str, Any]:\n        config_dict = self.to_dict()\n        # Get the default config dict (from a fresh PreTrainedConfig instance)\n        default_config_dict = PretrainedConfig().to_dict()\n        # get class specific config dict\n        class_config_dict = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n\n        serializable_config_dict = {}\n\n        # Only serialize values that differ from the default config,\n        # except always keep the 'config' attribute.\n        for key, value in config_dict.items():\n            if (\n                isinstance(getattr(self, key, None), PretrainedConfig)\n                and key in class_config_dict\n                and isinstance(class_config_dict[key], dict)\n                or key in self.sub_configs\n            ):\n                # For nested configs we need to clean the diff recursively\n                diff = recursive_diff_dict(value, default_config_dict, config_obj=getattr(self, key, None))\n                if \"model_type\" in value:\n                    # Needs to be set even if it's not in the diff\n                    diff[\"model_type\"] = value[\"model_type\"]\n\n                serializable_config_dict[key] = diff\n            elif (\n                key not in default_config_dict\n                or key == \"transformers_version\"\n                or key == \"vocab_file\"\n                or value != default_config_dict[key]\n                or (key in default_config_dict and value != class_config_dict.get(key, value))\n            ):\n                serializable_config_dict[key] = value\n\n        self._remove_keys_not_serialized(serializable_config_dict)\n\n        # Key removed only in diff dict\n        if \"_name_or_path\" in serializable_config_dict:\n            del serializable_config_dict[\"_name_or_path\"]\n\n        if hasattr(self, \"quantization_config\"):\n            serializable_config_dict[\"quantization_config\"] = (\n                self.quantization_config.to_dict()\n                if not isinstance(self.quantization_config, dict)\n                else self.quantization_config\n            )\n        self.dict_dtype_to_str(serializable_config_dict)\n\n        return serializable_config_dict\n\n    def to_dict(self) -> dict[str, Any]:\n        \n        output = copy.deepcopy(self.__dict__)\n        if hasattr(self.__class__, \"model_type\"):\n            output[\"model_type\"] = self.__class__.model_type\n\n        # Transformers version when serializing the model\n        output[\"transformers_version\"] = __version__\n\n        for key, value in output.items():\n            # Deal with nested configs like CLIP\n            if isinstance(value, PretrainedConfig):\n                value = value.to_dict()\n                del value[\"transformers_version\"]\n\n            output[key] = value\n\n        self._remove_keys_not_serialized(output)\n\n        if hasattr(self, \"quantization_config\"):\n            output[\"quantization_config\"] = (\n                self.quantization_config.to_dict()\n                if not isinstance(self.quantization_config, dict)\n                else self.quantization_config\n            )\n        self.dict_dtype_to_str(output)\n\n        return output\n\n    def to_json_string(self, use_diff: bool = True) -> str:\n        \n        if use_diff is True:\n            config_dict = self.to_diff_dict()\n        else:\n            config_dict = self.to_dict()\n        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n\n    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n       \n        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n            writer.write(self.to_json_string(use_diff=use_diff))\n\n    def update(self, config_dict: dict[str, Any]):\n        \n        for key, value in config_dict.items():\n            setattr(self, key, value)\n\n    def update_from_string(self, update_str: str):\n        d = dict(x.split(\"=\") for x in update_str.split(\",\"))\n        for k, v in d.items():\n            if not hasattr(self, k):\n                raise ValueError(f\"key {k} isn't in the original config dict\")\n\n            old_v = getattr(self, k)\n            if isinstance(old_v, bool):\n                if v.lower() in [\"true\", \"1\", \"y\", \"yes\"]:\n                    v = True\n                elif v.lower() in [\"false\", \"0\", \"n\", \"no\"]:\n                    v = False\n                else:\n                    raise ValueError(f\"can't derive true or false from {v} (key {k})\")\n            elif isinstance(old_v, int):\n                v = int(v)\n            elif isinstance(old_v, float):\n                v = float(v)\n            elif not isinstance(old_v, str):\n                raise TypeError(\n                    f\"You can only update int, float, bool or string values in the config, got {v} for key {k}\"\n                )\n\n            setattr(self, k, v)\n\n    def dict_dtype_to_str(self, d: dict[str, Any]) -> None:\n       \n        if d.get(\"dtype\") is not None:\n            if isinstance(d[\"dtype\"], dict):\n                d[\"dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"dtype\"].items()}\n            # models like Emu3 can have \"dtype\" as token in config's vocabulary map,\n            # so we also exclude int type here to avoid error in this special case.\n            elif not isinstance(d[\"dtype\"], (str, int)):\n                d[\"dtype\"] = str(d[\"dtype\"]).split(\".\")[1]\n        for value in d.values():\n            if isinstance(value, dict):\n                self.dict_dtype_to_str(value)\n\n    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n       \n        if hasattr(self, \"quantization_config\"):\n            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n            _ = d.pop(\"_pre_quantization_dtype\", None)\n\n        if \"_auto_class\" in d:\n            del d[\"_auto_class\"]\n        if \"_output_attentions\" in d:\n            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n        if \"_commit_hash\" in d:\n            del d[\"_commit_hash\"]\n        if \"_attn_implementation_internal\" in d:\n            del d[\"_attn_implementation_internal\"]\n        # Do not serialize `base_model_tp_plan` for now\n        if \"base_model_tp_plan\" in d:\n            del d[\"base_model_tp_plan\"]\n        # Do not serialize `base_model_pp_plan` for now\n        if \"base_model_pp_plan\" in d:\n            del d[\"base_model_pp_plan\"]\n        for value in d.values():\n            if isinstance(value, dict):\n                self._remove_keys_not_serialized(value)\n\n    @classmethod\n    def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n        \n        if not isinstance(auto_class, str):\n            auto_class = auto_class.__name__\n\n        import transformers.models.auto as auto_module\n\n        if not hasattr(auto_module, auto_class):\n            raise ValueError(f\"{auto_class} is not a valid auto class.\")\n\n        cls._auto_class = auto_class\n\n    @staticmethod\n    def _get_global_generation_defaults() -> dict[str, Any]:\n        return {\n            \"max_length\": 20,\n            \"min_length\": 0,\n            \"do_sample\": False,\n            \"early_stopping\": False,\n            \"num_beams\": 1,\n            \"num_beam_groups\": 1,\n            \"diversity_penalty\": 0.0,\n            \"temperature\": 1.0,\n            \"top_k\": 50,\n            \"top_p\": 1.0,\n            \"typical_p\": 1.0,\n            \"repetition_penalty\": 1.0,\n            \"length_penalty\": 1.0,\n            \"no_repeat_ngram_size\": 0,\n            \"encoder_no_repeat_ngram_size\": 0,\n            \"bad_words_ids\": None,\n            \"num_return_sequences\": 1,\n            \"output_scores\": False,\n            \"return_dict_in_generate\": False,\n            \"forced_bos_token_id\": None,\n            \"forced_eos_token_id\": None,\n            \"remove_invalid_values\": False,\n            \"exponential_decay_length_penalty\": None,\n            \"suppress_tokens\": None,\n            \"begin_suppress_tokens\": None,\n        }\n\n    def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n       \n        non_default_generation_parameters = {}\n        decoder_attribute_name = None\n\n        # Composite models don't have a default config, use their decoder config as a fallback for default values\n        # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n        try:\n            default_config = self.__class__()\n        except ValueError:\n            decoder_config = self.get_text_config(decoder=True)\n            if decoder_config is not self:\n                default_config = decoder_config.__class__()\n            else:\n                default_config = None\n\n        # If it is a composite model, we want to check the subconfig that will be used for generation\n        self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n\n        for parameter_name, default_global_value in self._get_global_generation_defaults().items():\n            if hasattr(self_decoder_config, parameter_name):\n                is_default_in_config = is_default_generation_value = None\n                parameter_value = getattr(self_decoder_config, parameter_name)\n                # Three cases in which is okay for the model config to hold generation config parameters:\n                # 1. The parameter is set to `None`, effectively delegating its value to the generation config\n                if parameter_value is None:\n                    continue\n                # 2. If we have a default config, then the instance should hold the same generation defaults\n                if default_config is not None:\n                    is_default_in_config = parameter_value == getattr(default_config, parameter_name)\n                # 3. if we don't have a default config, then the instance should hold the global generation defaults\n                else:\n                    is_default_generation_value = parameter_value == default_global_value\n\n                is_non_default = (is_default_in_config is False) or (\n                    is_default_in_config is None and is_default_generation_value is False\n                )\n                if is_non_default:\n                    non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n\n        return non_default_generation_parameters\n\n    def get_text_config(self, decoder=None, encoder=None) -> \"PretrainedConfig\":\n       \n        return_both = decoder == encoder  # both unset or both set -> search all possible names\n\n        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n        encoder_possible_text_config_names = (\"text_encoder\",)\n        if return_both:\n            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n        elif decoder:\n            possible_text_config_names = decoder_possible_text_config_names\n        else:\n            possible_text_config_names = encoder_possible_text_config_names\n\n        valid_text_config_names = []\n        for text_config_name in possible_text_config_names:\n            if hasattr(self, text_config_name):\n                text_config = getattr(self, text_config_name, None)\n                if text_config is not None:\n                    valid_text_config_names += [text_config_name]\n\n        if len(valid_text_config_names) > 1:\n            raise ValueError(\n                f\"Multiple valid text configs were found in the model config: {valid_text_config_names}. In this \"\n                \"case, using `get_text_config()` would be ambiguous. Please specify the desired text config directly, \"\n                \"e.g. `text_config = config.sub_config_name`\"\n            )\n        elif len(valid_text_config_names) == 1:\n            config_to_return = getattr(self, valid_text_config_names[0])\n        else:\n            config_to_return = self\n\n        # handle legacy models with flat config structure, when we only want one of the configs\n        if not return_both and len(valid_text_config_names) == 0 and config_to_return.is_encoder_decoder:\n            config_to_return = copy.deepcopy(config_to_return)\n            prefix_to_discard = \"encoder\" if decoder else \"decoder\"\n            for key in config_to_return.to_dict():\n                if key.startswith(prefix_to_discard):\n                    delattr(config_to_return, key)\n            # old encoder/decoder models may use \"encoder_layers\"/\"decoder_layers\" instead of \"num_hidden_layers\"\n            if decoder and hasattr(config_to_return, \"decoder_layers\"):\n                config_to_return.num_hidden_layers = config_to_return.decoder_layers\n            elif encoder and hasattr(config_to_return, \"encoder_layers\"):\n                config_to_return.num_hidden_layers = config_to_return.encoder_layers\n\n        return config_to_return\n\n    @classmethod\n    def from_text_vision_configs(cls, text_config, vision_config, **kwargs):\n       \n        warnings.warn(\n            \"The `from_text_vision_configs` method is deprecated and will be removed in v4.60 of Transformers. Please instantiate \"\n            \"the config class directly with `MyConfig(text_config=text_config, vision_config=vision_config, **kwargs)` instead.\",\n            FutureWarning,\n        )\n\n        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n\n    @classmethod\n    def from_text_audio_configs(cls, text_config, audio_config, **kwargs):\n       \n        warnings.warn(\n            \"The `from_text_audio_configs` method is deprecated and will be removed in v4.60 of Transformers. Please instantiate \"\n            \"the config class directly with `MyConfig(text_config=text_config, audio_config=audio_config, **kwargs)` instead.\",\n            FutureWarning,\n        )\n\n        return cls(text_config=text_config.to_dict(), audio_config=audio_config.to_dict(), **kwargs)\n\n\ndef get_configuration_file(configuration_files: list[str]) -> str:\n    \n    configuration_files_map = {}\n    for file_name in configuration_files:\n        if file_name.startswith(\"config.\") and file_name.endswith(\".json\") and file_name != \"config.json\":\n            v = file_name.removeprefix(\"config.\").removesuffix(\".json\")\n            configuration_files_map[v] = file_name\n    available_versions = sorted(configuration_files_map.keys())\n\n    # Defaults to FULL_CONFIGURATION_FILE and then try to look at some newer versions.\n    configuration_file = CONFIG_NAME\n    transformers_version = version.parse(__version__)\n    for v in available_versions:\n        if version.parse(v) <= transformers_version:\n            configuration_file = configuration_files_map[v]\n        else:\n            # No point going further since the versions are sorted.\n            break\n\n    return configuration_file\n\n\ndef recursive_diff_dict(dict_a, dict_b, config_obj=None):\n    \n    diff = {}\n    default = config_obj.__class__().to_dict() if config_obj is not None else {}\n    for key, value in dict_a.items():\n        obj_value = getattr(config_obj, str(key), None)\n        if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n            diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n            diff[key] = diff_value\n        elif key not in dict_b or (value != default[key]):\n            diff[key] = value\n    return diff\n\n\nPretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)\nif PretrainedConfig.push_to_hub.__doc__ is not None:\n    PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n        object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n    )\nALLOWED_LAYER_TYPES = (\n    \"full_attention\",\n    \"sliding_attention\",\n    \"chunked_attention\",\n    \"linear_attention\",  # used in minimax\n)\n\ndef layer_type_validation(layer_types: list[str]):\n    \"\"\"Check that each entry in `layer_types` are allowed.\"\"\"\n    if not all(layer_type in ALLOWED_LAYER_TYPES for layer_type in layer_types):\n        raise ValueError(f\"The `layer_types` entries must be in {ALLOWED_LAYER_TYPES}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GemmaConfig 类 是继承自 PretrainedConfig 的，用于初始化一个新的 Gemma 配置。\n# model_type = \"gemma\" 该属性指定了模型的类型（在这个例子中是 gemma），这通常用于在模型加载时标识特定模型。\n# keys_to_ignore_at_inference = [\"past_key_values\"] 这个列表指定了推理阶段需要忽略的字段。在推理过程中，\n# 这些字段不会被保存或用于计算。\n# base_model_tp_plan 和 base_model_pp_plan：这些是与模型分布式训练（例如模型并行）和推理优化（\n# 例如参数并行）相关的配置项。它们为不同的层和操作指定了并行处理方式和参数拆分方式。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"layers.*.self_attn.q_proj\"、\"layers.*.self_attn.k_proj\"、\"layers.*.self_attn.v_proj\"：这些项对应于\n# 自注意力机制中的查询（Q）、键（K）和值（V）投影矩阵。根据 colwise 设置，这些投影矩阵将在列方向上进行并行化，意味\n# 着每个设备只处理矩阵的某一列。\n# \"layers.*.self_attn.o_proj\"：这是自注意力机制的输出投影矩阵，根据 rowwise 设置，它将在行方向上进行并行化，意\n# 味着每个设备只处理矩阵的某一行。\n# \"layers.*.mlp.gate_proj\"、\"layers.*.mlp.up_proj\"、\"layers.*.mlp.down_proj\"：这些项对应于多层感知机（MLP\n# ）中的投影矩阵。colwise 表示这些矩阵将在列方向上进行并行化，而 rowwise 表示某些矩阵将在行方向上进行并行化。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"embed_tokens\"：模型的嵌入层，负责将 input_ids 转换为 inputs_embeds。这里可能是在设备 1 上处理该层，\n# input_ids 作为输入，inputs_embeds 作为输出传递给后续层。\n# \"layers\"：这里是模型的多层（transformer 层）。这些层可能分布在不同的设备上。在第一台设备上，hidden_states\n# 和 attention_mask 会被传递给后续设备，最终在各自的设备上处理，并且只传递 hidden_states，以减少数据传输的开销。\n# \"norm\"：归一化层，这里处理的是 hidden_states，并且将结果传递给下游设备继续计算。\n# 键值对的结构\n# 键： 这里的键是模型中的某些层或操作的名称，例如 embed_tokens 表示嵌入层（Embedding），layers 表示模型\n# 的 Transformer 层，norm 是归一化层。\n# 值： 每个键对应的值是一个元组，元组包含两个元素：\n# 第一个列表（输入）： 表示这一层/操作需要的输入数据（可能是其他层的输出）。\n# 第二个列表（输出）： 表示这一层/操作的输出数据，通常会被传递给下一个操作或层。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GemmaConfig(PretrainedConfig):\n    r\"\"\"\n    ```python\n    >>> from transformers import GemmaModel, GemmaConfig\n    >>> # Initializing a Gemma gemma-7b style configuration\n    >>> configuration = GemmaConfig()\n    >>> # Initializing a model from the gemma-7b style configuration\n    >>> model = GemmaModel(configuration)\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"gemma\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    # Tensor Parallelism (TP) 是一种分布式训练策略，其中模型的各个层被切分并并行化到多个设备上\n    base_model_tp_plan = {\n        \"layers.*.self_attn.q_proj\": \"colwise\",\n        \"layers.*.self_attn.k_proj\": \"colwise\",\n        \"layers.*.self_attn.v_proj\": \"colwise\",\n        \"layers.*.self_attn.o_proj\": \"rowwise\",\n        \"layers.*.mlp.gate_proj\": \"colwise\",\n        \"layers.*.mlp.up_proj\": \"colwise\",\n        \"layers.*.mlp.down_proj\": \"rowwise\",\n    }\n    base_model_pp_plan = {\n        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n    }\n\n    def __init__(\n        self,\n        vocab_size=256000,\n        hidden_size=3072,\n        intermediate_size=24576,\n        num_hidden_layers=28,\n        num_attention_heads=16,\n        num_key_value_heads=16,\n        head_dim=256,\n        hidden_act=\"gelu_pytorch_tanh\",\n        hidden_activation=None,\n        max_position_embeddings=8192,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        eos_token_id=1,\n        bos_token_id=2,\n        tie_word_embeddings=True,\n        rope_theta=10000.0,\n        attention_bias=False,\n        attention_dropout=0.0,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.head_dim = head_dim\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.hidden_activation = hidden_activation\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n\n__all__ = [\"GemmaConfig\"]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}