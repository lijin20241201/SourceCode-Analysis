{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from typing import TYPE_CHECKING\n# from ...utils import _LazyModule\n# from ...utils.import_utils import define_import_structure\n# if TYPE_CHECKING:\n#     from .configuration_t5 import *\n#     from .modeling_flax_t5 import *\n#     from .modeling_t5 import *\n#     from .modeling_tf_t5 import *\n#     from .tokenization_t5 import *\n#     from .tokenization_t5_fast import *\n# else:\n#     import sys\n#     _file = globals()[\"__file__\"]\n#     sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Mapping\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.onnx import OnnxSeq2SeqConfigWithPast\nfrom transformers.utils import logging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T04:31:18.333292Z","iopub.execute_input":"2025-06-03T04:31:18.333632Z","iopub.status.idle":"2025-06-03T04:31:24.512823Z","shell.execute_reply.started":"2025-06-03T04:31:18.333604Z","shell.execute_reply":"2025-06-03T04:31:24.511555Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"logger = logging.get_logger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T04:31:31.319754Z","iopub.execute_input":"2025-06-03T04:31:31.320161Z","iopub.status.idle":"2025-06-03T04:31:31.324468Z","shell.execute_reply.started":"2025-06-03T04:31:31.320138Z","shell.execute_reply":"2025-06-03T04:31:31.323584Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class T5Config(PretrainedConfig):\n    model_type = \"t5\" # 指定模型类型\n    # 如果你是自己调用 model(input_ids)，不会有任何影响，past_key_values 总会返回（除非你手动 use_cache=False）\n    # 但如果你用的是 .generate() 或 pipeline，则可能自动帮你裁剪掉past_key_values。\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    attribute_map = {\n        \"hidden_size\": \"d_model\",\n        \"num_attention_heads\": \"num_heads\",\n        \"num_hidden_layers\": \"num_layers\",\n        \"head_dim\": \"d_kv\",\n    }\n\n    def __init__(\n        self,\n        vocab_size=32128, # 词汇表大小\n        d_model=512,  # 模型隐藏维度\n        d_kv=64, # 每个注意力头的维度大小\n        d_ff=2048, # 中间前馈层的维度大小\n        num_layers=6,  # Transformer 编码器中的隐藏层数量\n        num_decoder_layers=None, # Transformer 解码器中的隐藏层数量。若未设置，将使用 `num_layers` 的值\n        num_heads=8,  # 多头注意力中 head 的数量\n        relative_attention_num_buckets=32, # 每个注意力层使用的相对位置编码的桶（bucket）数量。\n        relative_attention_max_distance=128, # 为 bucket 分隔设置的最大序列距离。\n        dropout_rate=0.1, # 所有 dropout 层的比率。\n        layer_norm_epsilon=1e-6,  # LayerNorm 层中使用的 epsilon 值。\n        initializer_factor=1.0, # 所有权重矩阵初始化时的因子\n        feed_forward_proj=\"relu\",  # 前馈网络中使用的激活函数\n        is_encoder_decoder=True, # 是否是编码器-解码器架构\n        use_cache=True, # 是否启用缓存，用于加速生成任务\n        pad_token_id=0,\n        eos_token_id=1,\n        classifier_dropout=0.0,# 分类器dropout\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.d_kv = d_kv\n        self.d_ff = d_ff\n        self.num_layers = num_layers\n        self.num_decoder_layers = (\n            num_decoder_layers if num_decoder_layers is not None else self.num_layers\n        )  # default = symmetry\n        self.num_heads = num_heads\n        self.relative_attention_num_buckets = relative_attention_num_buckets\n        self.relative_attention_max_distance = relative_attention_max_distance\n        self.dropout_rate = dropout_rate\n        self.classifier_dropout = classifier_dropout\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_factor = initializer_factor\n        self.feed_forward_proj = feed_forward_proj\n        self.use_cache = use_cache\n        act_info = self.feed_forward_proj.split(\"-\")\n        self.dense_act_fn = act_info[-1] # 前馈中间层的激活函数\n        self.is_gated_act = act_info[0] == \"gated\" # 是否是门控型的\n        if len(act_info) > 1 and act_info[0] != \"gated\" or len(act_info) > 2:\n            raise ValueError(\n                f\"`feed_forward_proj`: {feed_forward_proj} is not a valid activation function of the dense layer. \"\n                \"Please make sure `feed_forward_proj` is of the format `gated-{ACT_FN}` or `{ACT_FN}`, e.g. \"\n                \"'gated-gelu' or 'relu'\"\n            )\n\n        # 向后兼容\n        if feed_forward_proj == \"gated-gelu\":\n            self.dense_act_fn = \"gelu_new\"\n        super().__init__(\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            is_encoder_decoder=is_encoder_decoder,\n            **kwargs,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T04:55:23.321454Z","iopub.execute_input":"2025-06-03T04:55:23.321856Z","iopub.status.idle":"2025-06-03T04:55:23.332403Z","shell.execute_reply.started":"2025-06-03T04:55:23.321830Z","shell.execute_reply":"2025-06-03T04:55:23.331468Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"config = T5Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T04:55:46.028754Z","iopub.execute_input":"2025-06-03T04:55:46.029047Z","iopub.status.idle":"2025-06-03T04:55:46.034061Z","shell.execute_reply.started":"2025-06-03T04:55:46.029025Z","shell.execute_reply":"2025-06-03T04:55:46.032962Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T04:55:52.615858Z","iopub.execute_input":"2025-06-03T04:55:52.616160Z","iopub.status.idle":"2025-06-03T04:55:52.623843Z","shell.execute_reply.started":"2025-06-03T04:55:52.616138Z","shell.execute_reply":"2025-06-03T04:55:52.622904Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"T5Config {\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class T5OnnxConfig(OnnxSeq2SeqConfigWithPast):\n    # 这种装饰器叫做属性装饰器，它将一个方法转化为一个只读属性\n    # 返回一个字典，定义了模型在导出为 ONNX 时的输入张量维度含义（即 dynamic axes），\n    # 用于指示哪些维度是可变的，并在 ONNX 导出时进行动态 shape 处理。\n    @property\n    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n        # 对于 encoder 输入：\n        #   - \"input_ids\": shape 为 [batch_size, encoder_seq_len]\n        #   - \"attention_mask\": shape 为 [batch_size, encoder_seq_len]\n        common_inputs = {\n            \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n            \"attention_mask\": {0: \"batch\", 1: \"encoder_sequence\"},\n        }\n        # 对于 decoder 输入：\n        #   - 如果启用 use_past\n        #       - decoder_input_ids 是仅包含最后一个 token，形状为 [batch_size]，因为是增量推理\n        #       - decoder_attention_mask 对应的是 [batch_size, past_decoder_seq_len + current_token]\n        #       - encoder 侧的 attention_mask 也需要扩展成 past_encoder_sequence + current_sequence\n        #       - 还需要注入 past key/value 相关的 dynamic axes（通过 fill_with_past_key_values_）\n        if self.use_past:\n            # 增量推理时 encoder 侧输入可视为 past + 当前输入，便于兼容长序列拼接推理\n            common_inputs[\"attention_mask\"][1] = \"past_encoder_sequence + sequence\"\n            # decoder 只输入一个当前 token，而不是整个序列\n            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n            # decoder_attention_mask 也要反映出当前 token 在拼接后的 decoder 序列中位置\n            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n        # 如果未启用 use_past（即一次性完整解码）：\n            # - decoder_input_ids: [batch_size, decoder_seq_len]\n            # - decoder_attention_mask: [batch_size, decoder_seq_len]\n        else: # 非增量推理时 decoder 需要整个输入序列\n            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n        # 如果启用了 use_past，则还要加入 past_key_values 的 dynamic axes 映射\n        if self.use_past:\n            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n        return common_inputs\n    # 指定导出为 ONNX 时使用的默认 opset 版本。opset 决定了导出模型可使用的 ONNX 算子版本。 \n    @property\n    def default_onnx_opset(self) -> int:\n        return 13\n\n\n__all__ = [\"T5Config\", \"T5OnnxConfig\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:25:34.983117Z","iopub.execute_input":"2025-06-03T05:25:34.983522Z","iopub.status.idle":"2025-06-03T05:25:34.991852Z","shell.execute_reply.started":"2025-06-03T05:25:34.983485Z","shell.execute_reply":"2025-06-03T05:25:34.990885Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"onnxConfig=T5OnnxConfig(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:25:58.295976Z","iopub.execute_input":"2025-06-03T05:25:58.296308Z","iopub.status.idle":"2025-06-03T05:25:58.301810Z","shell.execute_reply.started":"2025-06-03T05:25:58.296279Z","shell.execute_reply":"2025-06-03T05:25:58.300880Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"onnxConfig.num_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:26:05.655242Z","iopub.execute_input":"2025-06-03T05:26:05.655548Z","iopub.status.idle":"2025-06-03T05:26:05.662043Z","shell.execute_reply.started":"2025-06-03T05:26:05.655525Z","shell.execute_reply":"2025-06-03T05:26:05.661137Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(6, 6)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"onnxConfig.inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:26:27.123353Z","iopub.execute_input":"2025-06-03T05:26:27.123746Z","iopub.status.idle":"2025-06-03T05:26:27.130110Z","shell.execute_reply.started":"2025-06-03T05:26:27.123709Z","shell.execute_reply":"2025-06-03T05:26:27.129130Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'input_ids': {0: 'batch', 1: 'encoder_sequence'},\n 'attention_mask': {0: 'batch', 1: 'encoder_sequence'},\n 'decoder_input_ids': {0: 'batch', 1: 'decoder_sequence'},\n 'decoder_attention_mask': {0: 'batch', 1: 'decoder_sequence'}}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"onnxConfig=T5OnnxConfig(config,use_past=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:27:54.979437Z","iopub.execute_input":"2025-06-03T05:27:54.979878Z","iopub.status.idle":"2025-06-03T05:27:54.984295Z","shell.execute_reply.started":"2025-06-03T05:27:54.979852Z","shell.execute_reply":"2025-06-03T05:27:54.983259Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"onnxConfig.inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:28:02.989201Z","iopub.execute_input":"2025-06-03T05:28:02.989522Z","iopub.status.idle":"2025-06-03T05:28:02.997006Z","shell.execute_reply.started":"2025-06-03T05:28:02.989493Z","shell.execute_reply":"2025-06-03T05:28:02.995954Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'input_ids': {0: 'batch', 1: 'encoder_sequence'},\n 'attention_mask': {0: 'batch', 1: 'past_encoder_sequence + sequence'},\n 'decoder_input_ids': {0: 'batch'},\n 'decoder_attention_mask': {0: 'batch', 1: 'past_decoder_sequence + sequence'},\n 'past_key_values.0.decoder.key': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.0.decoder.value': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.0.encoder.key': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.0.encoder.value': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.1.decoder.key': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.1.decoder.value': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.1.encoder.key': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.1.encoder.value': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.2.decoder.key': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.2.decoder.value': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.2.encoder.key': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.2.encoder.value': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.3.decoder.key': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.3.decoder.value': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.3.encoder.key': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.3.encoder.value': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.4.decoder.key': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.4.decoder.value': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.4.encoder.key': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.4.encoder.value': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.5.decoder.key': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.5.decoder.value': {0: 'batch', 2: 'past_decoder_sequence'},\n 'past_key_values.5.encoder.key': {0: 'batch', 2: 'past_encoder_sequence'},\n 'past_key_values.5.encoder.value': {0: 'batch', 2: 'past_encoder_sequence'}}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"onnxConfig.num_attention_heads","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:29:54.174662Z","iopub.execute_input":"2025-06-03T05:29:54.175594Z","iopub.status.idle":"2025-06-03T05:29:54.182037Z","shell.execute_reply.started":"2025-06-03T05:29:54.175491Z","shell.execute_reply":"2025-06-03T05:29:54.180542Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(8, 8)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"onnxConfig.default_onnx_opset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T05:30:01.045376Z","iopub.execute_input":"2025-06-03T05:30:01.045723Z","iopub.status.idle":"2025-06-03T05:30:01.052089Z","shell.execute_reply.started":"2025-06-03T05:30:01.045674Z","shell.execute_reply":"2025-06-03T05:30:01.051220Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"13"},"metadata":{}}],"execution_count":16}]}