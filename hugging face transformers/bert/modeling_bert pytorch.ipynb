{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport os\nimport warnings\nfrom dataclasses import dataclass  # 简化数据类定义\nfrom typing import List, Optional, Tuple, Union # 类型注解支持\n\nimport torch\nimport torch.utils.checkpoint\nfrom packaging import version\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.generation import GenerationMixin\nfrom transformers.modeling_attn_mask_utils import (\n    _prepare_4d_attention_mask_for_sdpa,\n    _prepare_4d_causal_attention_mask_for_sdpa,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    get_torch_version,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.bert.configuration_bert import BertConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:00.060068Z","iopub.execute_input":"2025-05-30T03:50:00.060865Z","iopub.status.idle":"2025-05-30T03:50:38.050062Z","shell.execute_reply.started":"2025-05-30T03:50:00.060828Z","shell.execute_reply":"2025-05-30T03:50:38.049478Z"}},"outputs":[{"name":"stderr","text":"2025-05-30 03:50:19.065519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748577019.475976      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748577019.585776      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"logger = logging.get_logger(__name__)  # 日志对象","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:38.051336Z","iopub.execute_input":"2025-05-30T03:50:38.051857Z","iopub.status.idle":"2025-05-30T03:50:38.055479Z","shell.execute_reply.started":"2025-05-30T03:50:38.051838Z","shell.execute_reply":"2025-05-30T03:50:38.054712Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"_CHECKPOINT_FOR_DOC = \"google-bert/bert-base-uncased\"\n_CONFIG_FOR_DOC = \"BertConfig\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:38.056255Z","iopub.execute_input":"2025-05-30T03:50:38.056506Z","iopub.status.idle":"2025-05-30T03:50:38.099853Z","shell.execute_reply.started":"2025-05-30T03:50:38.056480Z","shell.execute_reply":"2025-05-30T03:50:38.099355Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Token分类\n_CHECKPOINT_FOR_TOKEN_CLASSIFICATION = \"dbmdz/bert-large-cased-finetuned-conll03-english\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:41.287686Z","iopub.execute_input":"2025-05-30T03:50:41.287953Z","iopub.status.idle":"2025-05-30T03:50:41.291696Z","shell.execute_reply.started":"2025-05-30T03:50:41.287933Z","shell.execute_reply":"2025-05-30T03:50:41.290974Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# token分类预期输出\n_TOKEN_CLASS_EXPECTED_OUTPUT = (\n    \"['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] \"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:43.504952Z","iopub.execute_input":"2025-05-30T03:50:43.505286Z","iopub.status.idle":"2025-05-30T03:50:43.509268Z","shell.execute_reply.started":"2025-05-30T03:50:43.505263Z","shell.execute_reply":"2025-05-30T03:50:43.508488Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"_TOKEN_CLASS_EXPECTED_LOSS = 0.01 # token分类预期损失","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:45.739084Z","iopub.execute_input":"2025-05-30T03:50:45.739360Z","iopub.status.idle":"2025-05-30T03:50:45.743120Z","shell.execute_reply.started":"2025-05-30T03:50:45.739336Z","shell.execute_reply":"2025-05-30T03:50:45.742340Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 问答\n_CHECKPOINT_FOR_QA = \"deepset/bert-base-cased-squad2\"\n_QA_EXPECTED_OUTPUT = \"'a nice puppet'\"\n_QA_EXPECTED_LOSS = 7.41\n_QA_TARGET_START_INDEX = 14\n_QA_TARGET_END_INDEX = 15","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:47.989380Z","iopub.execute_input":"2025-05-30T03:50:47.989684Z","iopub.status.idle":"2025-05-30T03:50:47.993675Z","shell.execute_reply.started":"2025-05-30T03:50:47.989660Z","shell.execute_reply":"2025-05-30T03:50:47.993103Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 序列分类\n_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"textattack/bert-base-uncased-yelp-polarity\"\n_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_1'\" \n_SEQ_CLASS_EXPECTED_LOSS = 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:50:52.018642Z","iopub.execute_input":"2025-05-30T03:50:52.019132Z","iopub.status.idle":"2025-05-30T03:50:52.022531Z","shell.execute_reply.started":"2025-05-30T03:50:52.019106Z","shell.execute_reply":"2025-05-30T03:50:52.021954Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 在 pytorch 模型中加载 tf 检查点\n# 当前这段代码中，config 没有被使用。\n# 它可能是为了兼容某些接口或者未来扩展保留的参数。\n# model 是指初始化的模型实例吗？\n# 是的，model 是已经初始化的 PyTorch 模型实例（通常是空权重结构），\n# 这个函数的作用是：将 TensorFlow checkpoint 中的权重加载并覆盖到该模型实例中。\n# tf_checkpoint_path 是什么样子的？\n# 它应是指向 TensorFlow checkpoint 的本地文件前缀路径，例如：\n# ./bert_model.ckpt\n# TensorFlow checkpoint 通常由以下三个文件组成：\n# bert_model.ckpt.index\n# bert_model.ckpt.meta\n# bert_model.ckpt.data-00000-of-00001\n# 其中 tf_checkpoint_path 是不带 .index、.meta、.data-xxxx 后缀的路径前缀。\n# 是否支持类似 deepset/bert-base-cased-squad2 这样的路径？\n# 不支持直接使用像 deepset/bert-base-cased-squad2 这样的HuggingFace模型名。\n# 原因：tf_path = os.path.abspath(tf_checkpoint_path)\n# 此语句期望你传入的是本地路径，它会转成绝对路径。如果传的是 HuggingFace 模型名，会被当作本地路径处理，导致报错。\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    try: # 尝试导入\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError: # 捕获导入错误,打印错误日志,之后抛出\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path) # 获取TF checkpoint的绝对路径\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\") # 打印转换tf检查点的日志\n    # 获取TF checkpoint中所有变量（名称及形状）\n    init_vars = tf.train.list_variables(tf_path)\n    names = [] # 用来存储参数的名称\n    arrays = [] # 存储具体的参数值\n    for name, shape in init_vars:\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name)  # 加载变量的值\n        names.append(name) \n        arrays.append(array)\n    # 遍历对应的参数名和值\n    # 假设 TensorFlow 变量名是：bert/encoder/layer_0/attention/self/query/kernel\n    # 拆分后是：[\"bert\", \"encoder\", \"layer_0\", \"attention\", \"self\", \"query\", \"kernel\"]\n    # PyTorch 模型中路径大致为：model.bert.encoder.layer[0].attention.self.query.weight\n    # 通过 pointer，代码逐步执行如下：\n    # pointer = model\n    # pointer = pointer.bert\n    # pointer = pointer.encoder\n    # pointer = pointer.layer[0]\n    # pointer = pointer.attention\n    # pointer = pointer.self\n    # pointer = pointer.query\n    # pointer = pointer.weight\n    # 这样最终定位到了 PyTorch 中的 query.weight 参数，便可将 TF 权重 array 赋值给它。\n    # 注意事项：\n    # 中途若某个模块在 PyTorch 中不存在，会触发 AttributeError，被跳过。\n    # 如果最终 pointer.shape != array.shape，则报错防止权重错配。\n    # pointer.data = torch.from_numpy(array) 是核心赋值语句。\n    for name, array in zip(names, arrays):\n        name = name.split(\"/\") # 将变量名按 '/' 拆分，得到模块层级名列表\n        # 忽略Adam优化器的中间变量（不用于推理或训练初始化）\n        if any(\n            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n            for n in name\n        ):\n            logger.info(f\"Skipping {'/'.join(name)}\") \n            continue # 跳过优化器之类的变量\n        pointer = model # 从模型顶层开始逐层定位对应子模块\n        # 在这段代码中，pointer 是一个动态指针变量，用于从 PyTorch 模型的顶层出发，逐层定位到目标权重的具体位置\n        # 。整个核心逻辑依赖它去精确找到与 TensorFlow 中某个权重对应的 PyTorch 模块/参数。\n        for m_name in name: # 遍历 TF 变量路径中的每个模块名片段\n            # 如果模块名是带下标的形式（如 'layer_0'），则将其拆分成 ['layer', '0']\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name) # 例：'layer_0' -> ['layer', '0']\n            else: # 普通模块名不拆分\n                scope_names = [m_name]\n            # 将 TF 权重变量名映射到 PyTorch 模型的属性\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                # TF 的 kernel 对应 PyTorch 的 weight，gamma 是 LayerNorm 的 weight\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\") # TF 的 bias 和 beta 对应 PyTorch 的 bias\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")  # 某些输出层权重直接映射为 weight\n            elif scope_names[0] == \"squad\": # 特殊命名的 classifier 层\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])  # 常规情况，按属性名获取下一层模块\n                except AttributeError: # 如果模块在 PyTorch 模型中不存在，跳过该变量\n                    logger.info(f\"Skipping {'/'.join(name)}\")\n                    continue\n            # 如果是带索引的模块（如 transformer 层列表），进入指定下标的子模块\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])  # 提取数字索引\n                pointer = pointer[num] # 进入子模块 如果是序列模块\n        # 特殊情况：embedding 权重，获取其 weight 属性\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        # 对于 kernel 权重（通常是全连接层），进行转置匹配 PyTorch 的布局\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n        # 校验 shape 是否匹配，避免因维度不一致导致错误加载\n        try:\n            if pointer.shape != array.shape:\n                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n        except ValueError as e:  # 将维度信息附加到异常中，并重新抛出\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(f\"Initialize PyTorch weight {name}\") # 打印成功加载的变量日志\n        pointer.data = torch.from_numpy(array) # 将 numpy 数组转为 PyTorch tensor，并复制到目标权重变量中\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:51:01.044109Z","iopub.execute_input":"2025-05-30T03:51:01.044749Z","iopub.status.idle":"2025-05-30T03:51:01.056180Z","shell.execute_reply.started":"2025-05-30T03:51:01.044727Z","shell.execute_reply":"2025-05-30T03:51:01.055389Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"config=BertConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:51:03.488583Z","iopub.execute_input":"2025-05-30T03:51:03.489084Z","iopub.status.idle":"2025-05-30T03:51:03.492623Z","shell.execute_reply.started":"2025-05-30T03:51:03.489061Z","shell.execute_reply":"2025-05-30T03:51:03.491827Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"config.max_position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:51:05.541040Z","iopub.execute_input":"2025-05-30T03:51:05.541311Z","iopub.status.idle":"2025-05-30T03:51:05.547098Z","shell.execute_reply.started":"2025-05-30T03:51:05.541291Z","shell.execute_reply":"2025-05-30T03:51:05.546368Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"config.type_vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:51:07.661749Z","iopub.execute_input":"2025-05-30T03:51:07.662079Z","iopub.status.idle":"2025-05-30T03:51:07.667067Z","shell.execute_reply.started":"2025-05-30T03:51:07.662044Z","shell.execute_reply":"2025-05-30T03:51:07.666444Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"aa=torch.arange(8).expand((2, -1))\naa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:57:15.774086Z","iopub.execute_input":"2025-05-30T03:57:15.774365Z","iopub.status.idle":"2025-05-30T03:57:15.780177Z","shell.execute_reply.started":"2025-05-30T03:57:15.774347Z","shell.execute_reply":"2025-05-30T03:57:15.779569Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n        [0, 1, 2, 3, 4, 5, 6, 7]])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"torch.zeros(aa.size(), dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:51:24.370804Z","iopub.execute_input":"2025-05-30T03:51:24.371614Z","iopub.status.idle":"2025-05-30T03:51:24.378521Z","shell.execute_reply.started":"2025-05-30T03:51:24.371588Z","shell.execute_reply":"2025-05-30T03:51:24.377835Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 0, 0, 0, 0, 0, 0, 0]])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# 从单词、位置和 token_type 嵌入构建嵌入\nclass BertEmbeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__() # 调用父类的初始化\n        self.word_embeddings = nn.Embedding( # 词嵌入\n            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding( # 位置嵌入\n            config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding( # token类型嵌入\n            config.type_vocab_size, config.hidden_size)\n        # 在 Python 中，按 PEP8 规范，变量名应为 layer_norm 形式。但 TensorFlow 模型保存变量时，通常变量名\n        # 是 LayerNorm（驼峰命名）。\n        # 为了权重自动对齐（如 LayerNorm/gamma → PyTorch 中的 LayerNorm.weight），必须让 PyTorch 模型中变\n        # 量名也叫 LayerNorm，否则无法正确映射\n        # 与 BatchNorm 不同，LayerNorm 是逐样本归一化，不会维护任何 running_mean 或 running_var。\n        # 所以它只有两个参数：weight 和 bias，均为可训练的。\n        # LayerNorm 是单样本级别的统计，每个样本独立计算均值与方差，然后进行归一化。\n        # 假设输入是 (batch_size, seq_len, hidden_dim)\n        # LayerNorm 通常在最后一个维度（hidden_dim）上进行归一化\n        # LayerNorm 没有 running_mean / running_var，只有 weight 和 bias。\n        # 是 逐样本归一化，不同于 BatchNorm 的批次统计。\n        # 优点是对 batch size 不敏感，适合 NLP、Transformers 等序列建模任务。\n        # LayerNorm 是逐样本统计，不维护全局均值和方差，因此根本没有 running_mean 和 running_var。\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob) # dropout\n        # 位置嵌入类型,默认是绝对\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        # PyTorch 中的 register_buffer 方法用于注册非参数性张量（不会作为 model.parameters() 返回，但会和模型\n        # 一起保存和加载，比如 .to(device) 时也会自动迁移）\n        # persistent=False 不希望保存，如：推理中可以重新生成、与权重无关的缓存型 buffer。\n        # 本例中的 position_ids 虽然会跟随模型迁移设备，但因为 persistent=False，所以不会随着模型保存和加载，\n        # 通常用于推理阶段可重构的辅助张量，如位置索引、掩码模板等。\n        # register_buffer：将张量注册为 buffer（非参数）\n        # persistent=False：表示不希望它被保存到模型的 state_dict 中\n        self.register_buffer(\n            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n        )\n        self.register_buffer(\n            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n        )\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        past_key_values_length: int = 0,\n    ) -> torch.Tensor:\n        # 获取输入的形状\n        if input_ids is not None: \n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1] # 序列长度\n        # 设置默认的位置ids  只切出本次传入的那段\n        if position_ids is None: \n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        # “issue #5664” 是 Hugging Face Transformers 仓库中的一个问题，涉及在使用 TorchScript 对模型进行追踪（tracing）时，\n        # 如果未显式传入 token_type_ids，可能会导致模型在推理阶段出现错误。\n        # 为了解决这个问题，开发者在模型的构造函数中注册了一个全零的 token_type_ids 缓冲区\n        if token_type_ids is None: # 设置默认的token type ids\n            if hasattr(self, \"token_type_ids\"):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n        # 设置词嵌入\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\": \n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings \n        embeddings = self.LayerNorm(embeddings) # 层标准化\n        embeddings = self.dropout(embeddings) # dropout\n        return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:01:34.257058Z","iopub.execute_input":"2025-05-30T04:01:34.258006Z","iopub.status.idle":"2025-05-30T04:01:34.267371Z","shell.execute_reply.started":"2025-05-30T04:01:34.257976Z","shell.execute_reply":"2025-05-30T04:01:34.266770Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"position_ids_l = torch.arange(3, dtype=torch.long).view(-1, 1) # (3,1) query_length\nposition_ids_l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:49:14.500374Z","iopub.execute_input":"2025-05-30T04:49:14.500903Z","iopub.status.idle":"2025-05-30T04:49:14.506458Z","shell.execute_reply.started":"2025-05-30T04:49:14.500880Z","shell.execute_reply":"2025-05-30T04:49:14.505801Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([[0],\n        [1],\n        [2]])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"position_ids_r=torch.arange(5, dtype=torch.long).view(1,-1) # key_length\nposition_ids_r","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:47:49.990186Z","iopub.execute_input":"2025-05-30T04:47:49.990886Z","iopub.status.idle":"2025-05-30T04:47:49.996425Z","shell.execute_reply.started":"2025-05-30T04:47:49.990865Z","shell.execute_reply":"2025-05-30T04:47:49.995722Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 1, 2, 3, 4]])"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"distance = position_ids_l - position_ids_r\ndistance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:50:19.540398Z","iopub.execute_input":"2025-05-30T04:50:19.540931Z","iopub.status.idle":"2025-05-30T04:50:19.546459Z","shell.execute_reply.started":"2025-05-30T04:50:19.540908Z","shell.execute_reply":"2025-05-30T04:50:19.545894Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0, -1, -2, -3, -4],\n        [ 1,  0, -1, -2, -3],\n        [ 2,  1,  0, -1, -2]])"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# 有缓存的情况\nposition_ids_l = torch.tensor(5 - 1, dtype=torch.long).view(\n                    -1, 1\n                )\nposition_ids_l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:53:46.195811Z","iopub.execute_input":"2025-05-30T04:53:46.196585Z","iopub.status.idle":"2025-05-30T04:53:46.202794Z","shell.execute_reply.started":"2025-05-30T04:53:46.196560Z","shell.execute_reply":"2025-05-30T04:53:46.202013Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([[4]])"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"distance = position_ids_l - position_ids_r\ndistance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:54:15.077449Z","iopub.execute_input":"2025-05-30T04:54:15.078296Z","iopub.status.idle":"2025-05-30T04:54:15.083397Z","shell.execute_reply.started":"2025-05-30T04:54:15.078265Z","shell.execute_reply":"2025-05-30T04:54:15.082695Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"tensor([[4, 3, 2, 1, 0]])"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"class BertSelfAttention(nn.Module): # 注意力\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.num_attention_heads = config.num_attention_heads # 头数\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads) # 头大小\n        self.all_head_size = self.num_attention_heads * self.attention_head_size \n        self.query = nn.Linear(config.hidden_size, self.all_head_size) # q,k,v投影\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        # 位置嵌入类型,默认绝对\n        self.position_embedding_type = position_embedding_type or getattr(\n            config, \"position_embedding_type\", \"absolute\"\n        )\n        # 相对位置嵌入\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings # 最大位置嵌入\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n        self.is_decoder = config.is_decoder # 是否是解码器结构\n\n    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(new_x_shape) # -->(b,s,h,hd)\n        return x.permute(0, 2, 1, 3) # (b,h,s,hd)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        #  hidden_states是初次的嵌入表示或者上一次编码器或解码器的输出\n        mixed_query_layer = self.query(hidden_states) \n\n        # 如果这个模块是作为交叉注意力模块实例化的，那么 key 和 value 来自编码器；此时的 \n        # attention mask 需要确保不会对编码器中的 padding token 进行注意力计算\n        # 如果传入了编码器输出,这里就是True \n        is_cross_attention = encoder_hidden_states is not None \n        # 如果是交叉注意力,并且有缓存,对应是解码器交叉注意力,有缓存的情况\n        if is_cross_attention and past_key_value is not None:\n            key_layer = past_key_value[0] # 从缓存中获取k,v\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask # 编码器的填充掩码\n        elif is_cross_attention: # 交叉注意力,没缓存\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) # (b,h,s,hd)\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask # 编码器的填充掩码\n        elif past_key_value is not None: # 自注意力,有缓存\n            # 获取当前传入的目标序列的k,v state,对应是解码器自注意力的情况\n            key_layer = self.transpose_for_scores(self.key(hidden_states)) # (b,h,s,hd)\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            # 之后在序列维度拼接 \n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else: # 这个是自注意力,并且没缓存的情况,直接计算,对应是编码器的情况\n            key_layer = self.transpose_for_scores(self.key(hidden_states)) # (b,h,s,hd)\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n        \n        query_layer = self.transpose_for_scores(mixed_query_layer) # (b,h,s,hd)\n        # 是否使用缓存 在past_key_value存在时才使用缓存\n        use_cache = past_key_value is not None \n        if self.is_decoder: # 如果是解码器架构,总是设置缓存\n            # 如果是交叉注意力（cross_attention），则保存所有交叉注意力的键/值状态（Tuple(torch.Tensor, torch.Tensor)）。\n            # 后续对交叉注意力层的调用可以复用这些键/值状态（对应第一个“if”分支）。\n            # 如果是单向自注意力（即解码器中的 self-attention），则保存解码器中所有先前的键/值状态（也是 Tuple(\n            # torch.Tensor, torch.Tensor)）。\n            # 后续对单向自注意力的调用可以将之前的键/值状态与当前投影后的键/值状态拼接（对应第三个 “elif” 分支）。\n            # 如果是编码器中的双向自注意力，past_key_value 总是为 None。\n            past_key_value = (key_layer, value_layer)\n        \n        # q@k (b,h,s_q,hd)@(b,h,hd,s_k)-->(b,h,s_q,s_K)\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        \n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            # 如果使用缓存,s_q=1,s_K是包括缓存的序列\n            query_length, key_length = query_layer.shape[2], key_layer.shape[2] # 序列长度 s_q,s_K\n            if use_cache: # 如果使用缓存 这里的position_ids_l是query中当前token对应的key中的相对位置\n                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                    -1, 1\n                )\n            else:# 如果不使用缓存 (s_q,1)\n                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            # (1,s_k)\n            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            # 如果使用缓存,这里就是query中当前token和key中所有token之间的相对位置偏移\n            # 如果不用缓存,每一行是query中每个token与key中所有token的相对位置偏移\n            distance = position_ids_l - position_ids_r\n            # 位置嵌入 这里转换成正数形式\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n            \n            if self.position_embedding_type == \"relative_key\":\n                # 相对位置分数 (b,h,s_q,hd)@(s_q,s_k,hd)-->(b,h,s_q,s_k)\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                # (b,h,s_q,s_k)+(b,h,s_q,s_k)-->(b,h,s_q,s_k)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                # query相对key (b,h,s_q,hd)@(s_q,s_k,hd)-->(b,h,s_q,s_k)\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                # key相对query (b,h,s_k,hd)@(s_q,s_k,hd)-->(b,h,s_q,s_k)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                # 带上相对位置的注意力得分\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n        # 缩放\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None: # 如果传入了注意力掩码\n            # 应用注意力掩码\n            attention_scores = attention_scores + attention_mask\n        # 对注意力分数在s_k上归一化\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n        # 之后对注意力矩阵dropout\n        attention_probs = self.dropout(attention_probs)\n        # 如果头掩码存在\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask \n        # 加权求和得到上下文表示 (b,h,s_q,s_k)@(b,h,s_k,hd)-->(b,h,s_q,hd)\n        context_layer = torch.matmul(attention_probs, value_layer)\n        # -->(b,s_q,h,hd)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        # -->(b,s_q,d)\n        context_layer = context_layer.view(new_context_layer_shape)\n        # 是否输出当前层的注意力权重矩阵\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n        if self.is_decoder: # 如果是解码器架构\n            outputs = outputs + (past_key_value,) # 带上缓存\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:19:00.994531Z","iopub.execute_input":"2025-05-30T05:19:00.995043Z","iopub.status.idle":"2025-05-30T05:19:01.011582Z","shell.execute_reply.started":"2025-05-30T05:19:00.995019Z","shell.execute_reply":"2025-05-30T05:19:01.010865Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"get_torch_version()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:20:29.503552Z","iopub.execute_input":"2025-05-30T05:20:29.504315Z","iopub.status.idle":"2025-05-30T05:20:29.508889Z","shell.execute_reply.started":"2025-05-30T05:20:29.504282Z","shell.execute_reply":"2025-05-30T05:20:29.508396Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'2.6.0+cu124'"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"class BertSdpaSelfAttention(BertSelfAttention): # sdpa\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__(config, position_embedding_type=position_embedding_type)\n        self.dropout_prob = config.attention_probs_dropout_prob # dropout比率\n        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n    # Adapted from BertSelfAttention\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # 如果不是绝对位置嵌入,或者设定输出attentions,或者head_mask存在\n        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n            # 上面的情况不支持,回退到BertSelfAttention\n            logger.warning_once(\n                \"BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n                '`attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                past_key_value,\n                output_attentions,\n            )\n\n        bsz, tgt_len, _ = hidden_states.size() # 批次大小,目标序列长度\n        # -->(b,h,s_q,hd)\n        query_layer = self.transpose_for_scores(self.query(hidden_states))\n        # 如果有编码器输出encoder_hidden_states,那么就是交叉注意力\n        is_cross_attention = encoder_hidden_states is not None\n        # 这里设定当前的k,v states\n        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n        # 设定注意力掩码 如果是交叉注意力,就是编码器填充掩码,否则就是传入的attention_mask(填充或因果)\n        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n        # 如果是交叉注意力并且有缓存,并且缓存的key的序列长度==当前的传入的序列长度\n        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n            key_layer, value_layer = past_key_value # 缓存的key,value states\n        else: # 是交叉，但是没缓存,或者是自注意力\n            key_layer = self.transpose_for_scores(self.key(current_states)) # (b,h,s_k,hd)\n            value_layer = self.transpose_for_scores(self.value(current_states)) # (b,h,s_v,hd)\n            # 如果有缓存,解码器自注意力 \n            if past_key_value is not None and not is_cross_attention:\n                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)  # 拼接缓存和当前\n                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n\n        if self.is_decoder: # 如果是解码器架构\n            # 解码器才设定缓存,编码器不设定\n            past_key_value = (key_layer, value_layer)\n        # 设定连续\n        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n            query_layer = query_layer.contiguous()\n            key_layer = key_layer.contiguous()\n            value_layer = value_layer.contiguous()\n\n        # 我们通过这个 is_causal 的 if 语句来调用 SDPA 的 Flash Attention 或 Efficient kernel，而不是在 SDPA 中使用内联的条件赋值。\n        # 这样做是为了支持 torch.compile 的动态形状（dynamic shapes）和完整计算图（full graph）选项。\n        # 如果使用内联条件，会导致动态形状无法编译。\n        # 如果是解码器架构,并且当前注意力机制不是交叉注意力,并且没有传入attention_mask,并且传入的tgt_len > 1\n        # tgt_len > 1是没有使用缓存k,v的情况,这时设定is_causal =True\n        is_causal = (\n            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n        )\n        # 使用sdpa 缩放点积注意力 -->(b,h,s_q,hd)\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_layer,\n            key_layer,\n            value_layer,\n            attn_mask=attention_mask,\n            dropout_p=self.dropout_prob if self.training else 0.0,\n            is_causal=is_causal,\n        )\n        \n        attn_output = attn_output.transpose(1, 2) # (b,s_q,h,hd)\n        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size) # (b,s,d)\n        outputs = (attn_output,)\n        if self.is_decoder: # 如果是解码器架构,返回输出+缓存\n            outputs = outputs + (past_key_value,)\n        return outputs ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:45:49.329698Z","iopub.execute_input":"2025-05-30T05:45:49.330366Z","iopub.status.idle":"2025-05-30T05:45:49.340273Z","shell.execute_reply.started":"2025-05-30T05:45:49.330343Z","shell.execute_reply":"2025-05-30T05:45:49.339698Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states) # 注意力机制最后的线性层\n        hidden_states = self.dropout(hidden_states)\n        # 残差+norm\n        hidden_states = self.LayerNorm(hidden_states + input_tensor) \n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:48:40.736198Z","iopub.execute_input":"2025-05-30T05:48:40.736772Z","iopub.status.idle":"2025-05-30T05:48:40.741501Z","shell.execute_reply.started":"2025-05-30T05:48:40.736750Z","shell.execute_reply":"2025-05-30T05:48:40.740821Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"BERT_SELF_ATTENTION_CLASSES = {\n    \"eager\": BertSelfAttention,\n    \"sdpa\": BertSdpaSelfAttention,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:48:52.396902Z","iopub.execute_input":"2025-05-30T05:48:52.397629Z","iopub.status.idle":"2025-05-30T05:48:52.400849Z","shell.execute_reply.started":"2025-05-30T05:48:52.397606Z","shell.execute_reply":"2025-05-30T05:48:52.400252Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"config._attn_implementation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:49:26.929644Z","iopub.execute_input":"2025-05-30T05:49:26.929907Z","iopub.status.idle":"2025-05-30T05:49:26.934749Z","shell.execute_reply.started":"2025-05-30T05:49:26.929889Z","shell.execute_reply":"2025-05-30T05:49:26.934256Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'eager'"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"class BertAttention(nn.Module): \n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n        self.self = BERT_SELF_ATTENTION_CLASSES[config._attn_implementation]( # 注意力机制\n            config, position_embedding_type=position_embedding_type\n        )\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set() # 存放已修剪头的集合\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        # 返回已经修剪过的头的索引,index是标记每个嵌入元素位置是否是修剪过的\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n        # 修剪q,k,v\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n        # 更新当前的头数为修剪后的头数\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        # 更新所有头的嵌入表示大小为修剪后的\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads) # 更新已经修剪的头集合\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        self_outputs = self.self( \n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        # 注意力前后残差\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:56:45.547288Z","iopub.execute_input":"2025-05-30T05:56:45.547885Z","iopub.status.idle":"2025-05-30T05:56:45.555516Z","shell.execute_reply.started":"2025-05-30T05:56:45.547864Z","shell.execute_reply":"2025-05-30T05:56:45.554948Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # 升维层\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:57:59.438661Z","iopub.execute_input":"2025-05-30T05:57:59.438942Z","iopub.status.idle":"2025-05-30T05:57:59.443569Z","shell.execute_reply.started":"2025-05-30T05:57:59.438922Z","shell.execute_reply":"2025-05-30T05:57:59.443006Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # 降维层\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:58:29.256510Z","iopub.execute_input":"2025-05-30T05:58:29.257084Z","iopub.status.idle":"2025-05-30T05:58:29.261421Z","shell.execute_reply.started":"2025-05-30T05:58:29.257062Z","shell.execute_reply":"2025-05-30T05:58:29.260738Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"config.chunk_size_feed_forward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T05:59:04.474212Z","iopub.execute_input":"2025-05-30T05:59:04.474756Z","iopub.status.idle":"2025-05-30T05:59:04.478893Z","shell.execute_reply.started":"2025-05-30T05:59:04.474735Z","shell.execute_reply":"2025-05-30T05:59:04.478384Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"config.is_decoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:00:16.131082Z","iopub.execute_input":"2025-05-30T06:00:16.131654Z","iopub.status.idle":"2025-05-30T06:00:16.136227Z","shell.execute_reply.started":"2025-05-30T06:00:16.131633Z","shell.execute_reply":"2025-05-30T06:00:16.135681Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"config.add_cross_attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:00:42.215584Z","iopub.execute_input":"2025-05-30T06:00:42.216162Z","iopub.status.idle":"2025-05-30T06:00:42.220422Z","shell.execute_reply.started":"2025-05-30T06:00:42.216139Z","shell.execute_reply":"2025-05-30T06:00:42.219770Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"class BertLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward # 前馈分块大小\n        self.seq_len_dim = 1 # 序列长度所在维度\n        self.attention = BertAttention(config) # 注意力\n        self.is_decoder = config.is_decoder # 是否是解码器架构\n        self.add_cross_attention = config.add_cross_attention # 是否添加交叉注意力\n        if self.add_cross_attention: # 如果需要添加交叉注意力(encoder-decoder)\n            if not self.is_decoder: # 如果不是解码器架构\n                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\") # 交叉注意力\n        self.intermediate = BertIntermediate(config) \n        self.output = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # 如果缓存存在,这里获取解码器自注意力的缓存\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention( # 自注意力\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,# 缓存\n        )\n        attention_output = self_attention_outputs[0] \n\n        # 如果是解码器架构\n        if self.is_decoder:\n            outputs = self_attention_outputs[1:-1] # 注意力权重矩阵\n            present_key_value = self_attention_outputs[-1] # 当前缓存\n        else: # 编码器没缓存\n            outputs = self_attention_outputs[1:] \n        \n        cross_attn_present_key_value = None # 交叉注意力的当前缓存\n        # 如果是解码器,并且有编码器输出\n        if self.is_decoder and encoder_hidden_states is not None:\n            # 这时就必须有设定交叉注意力类\n            if not hasattr(self, \"crossattention\"): \n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                    \" by setting `config.add_cross_attention=True`\"\n                )\n\n            # 如果past_key_value存在,这时的交叉缓存是在past_key_value[-2:]后两个位置\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            cross_attention_outputs = self.crossattention( # 交叉注意力\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                cross_attn_past_key_value,\n                output_attentions,\n            )\n            attention_output = cross_attention_outputs[0] # 经过解码器后的输出\n            outputs = outputs + cross_attention_outputs[1:-1]  # 交叉注意力权重矩阵\n            # 交叉注意力缓存\n            cross_attn_present_key_value = cross_attention_outputs[-1]\n            # 当前缓存包括自注意力缓存和交叉注意力缓存\n            present_key_value = present_key_value + cross_attn_present_key_value\n        # 应用分块前馈\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n        # 如果是解码器架构\n        if self.is_decoder: # 解码器需要缓存\n            outputs = outputs + (present_key_value,)\n        return outputs\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output) # 前馈前后残差\n        return layer_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:17:45.886350Z","iopub.execute_input":"2025-05-30T06:17:45.886911Z","iopub.status.idle":"2025-05-30T06:17:45.896246Z","shell.execute_reply.started":"2025-05-30T06:17:45.886890Z","shell.execute_reply":"2025-05-30T06:17:45.895661Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"config.num_hidden_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:18:26.486232Z","iopub.execute_input":"2025-05-30T06:18:26.486885Z","iopub.status.idle":"2025-05-30T06:18:26.491115Z","shell.execute_reply.started":"2025-05-30T06:18:26.486860Z","shell.execute_reply":"2025-05-30T06:18:26.490361Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"class BertEncoder(nn.Module): # 编码器\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        # 层列表\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False # 设置是否使用梯度检查\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        all_hidden_states = () if output_hidden_states else None # 用来存储每层的输入\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n        # 如果是训练模式,并且指定使用梯度检查\n        if self.gradient_checkpointing and self.training:\n            if use_cache: \n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n        # \n        next_decoder_cache = () if use_cache else None\n        for i, layer_module in enumerate(self.layer): # 遍历每个编码器层\n            if output_hidden_states: \n                all_hidden_states = all_hidden_states + (hidden_states,)\n            layer_head_mask = head_mask[i] if head_mask is not None else None # 头掩码\n            past_key_value = past_key_values[i] if past_key_values is not None else None # 每层的缓存\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    layer_module.__call__,\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )\n            else: # 正确情况\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0] # 每层编码器的输出\n            if use_cache: # 如果用缓存把每层的缓存当成元组的元素拼接\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions: # 如果需要输出注意力矩阵\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                if self.config.add_cross_attention:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n        # 拼接最后一层输出\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:28:16.457391Z","iopub.execute_input":"2025-05-30T06:28:16.458089Z","iopub.status.idle":"2025-05-30T06:28:16.467724Z","shell.execute_reply.started":"2025-05-30T06:28:16.458064Z","shell.execute_reply":"2025-05-30T06:28:16.466999Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"((1,2),)+()+((3,4),)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:26:42.392018Z","iopub.execute_input":"2025-05-30T06:26:42.392298Z","iopub.status.idle":"2025-05-30T06:26:42.397313Z","shell.execute_reply.started":"2025-05-30T06:26:42.392276Z","shell.execute_reply":"2025-05-30T06:26:42.396534Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"((1, 2), (3, 4))"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"class BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # 第一个cls的输出 用来作为句子表示(b,d)\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:29:38.994818Z","iopub.execute_input":"2025-05-30T06:29:38.995226Z","iopub.status.idle":"2025-05-30T06:29:39.000387Z","shell.execute_reply.started":"2025-05-30T06:29:38.995198Z","shell.execute_reply":"2025-05-30T06:29:38.999514Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"class BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str): # 激活函数\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) # norm\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states) \n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:31:07.552598Z","iopub.execute_input":"2025-05-30T06:31:07.553443Z","iopub.status.idle":"2025-05-30T06:31:07.558212Z","shell.execute_reply.started":"2025-05-30T06:31:07.553411Z","shell.execute_reply":"2025-05-30T06:31:07.557603Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"class BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n        # 预测头\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        \n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        # 需要两个变量之间的联系，以便使用“resize_token_embeddings”正确调整偏差的大小\n        self.decoder.bias = self.bias\n    def _tie_weights(self):\n        self.decoder.bias = self.bias\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) # (b,s,v)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:34:26.791760Z","iopub.execute_input":"2025-05-30T06:34:26.792046Z","iopub.status.idle":"2025-05-30T06:34:26.796767Z","shell.execute_reply.started":"2025-05-30T06:34:26.792025Z","shell.execute_reply":"2025-05-30T06:34:26.796225Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"class BertOnlyMLMHead(nn.Module): # 掩码语言头\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config) \n    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n        prediction_scores = self.predictions(sequence_output) # (b,s,v)\n        return prediction_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:35:03.903196Z","iopub.execute_input":"2025-05-30T06:35:03.903458Z","iopub.status.idle":"2025-05-30T06:35:03.907552Z","shell.execute_reply.started":"2025-05-30T06:35:03.903440Z","shell.execute_reply":"2025-05-30T06:35:03.906897Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"class BertOnlyNSPHead(nn.Module): # nsp 句子二分类\n    def __init__(self, config):\n        super().__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n    def forward(self, pooled_output): # 池化输出(b,d)\n        seq_relationship_score = self.seq_relationship(pooled_output) # 序列关系得分\n        return seq_relationship_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:36:46.834570Z","iopub.execute_input":"2025-05-30T06:36:46.835224Z","iopub.status.idle":"2025-05-30T06:36:46.839009Z","shell.execute_reply.started":"2025-05-30T06:36:46.835198Z","shell.execute_reply":"2025-05-30T06:36:46.838451Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"class BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config) # 掩码语言头\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n    # 输入是序列输出和池化输出 (b,s,d) (b,d)\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output) # (b,s,v)\n        seq_relationship_score = self.seq_relationship(pooled_output) # (b,2)\n        return prediction_scores, seq_relationship_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:38:59.071898Z","iopub.execute_input":"2025-05-30T06:38:59.072210Z","iopub.status.idle":"2025-05-30T06:38:59.077323Z","shell.execute_reply.started":"2025-05-30T06:38:59.072191Z","shell.execute_reply":"2025-05-30T06:38:59.076594Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class BertPreTrainedModel(PreTrainedModel):\n    config_class = BertConfig # 配置类\n    load_tf_weights = load_tf_weights_in_bert # 在pytorch中加载tf权重\n    base_model_prefix = \"bert\"\n    supports_gradient_checkpointing = True # 是否支持梯度检查\n    _supports_sdpa = True # 是否支持sdpa\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear): # 如果当前模块是线性层\n            # 权重正态分布初始化\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None: # 截距初始化\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding): # 如果是嵌入模块\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None: # 对填充token进行0初始化\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm): # 如果是norm层\n            module.bias.data.zero_() # 截距0初始化\n            module.weight.data.fill_(1.0) # 对权重1初始化\n        elif isinstance(module, BertLMPredictionHead):\n            module.bias.data.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:44:06.857084Z","iopub.execute_input":"2025-05-30T06:44:06.857384Z","iopub.status.idle":"2025-05-30T06:44:06.863320Z","shell.execute_reply.started":"2025-05-30T06:44:06.857361Z","shell.execute_reply":"2025-05-30T06:44:06.862682Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"@dataclass\nclass BertForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    Output type of [`BertForPreTraining`].\n\n    Args:\n        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n            (classification) loss.\n        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n            before SoftMax).\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n            shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None # 损失\n    prediction_logits: Optional[torch.FloatTensor] = None \n    seq_relationship_logits: Optional[torch.FloatTensor] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:45:01.473420Z","iopub.execute_input":"2025-05-30T06:45:01.473923Z","iopub.status.idle":"2025-05-30T06:45:01.479338Z","shell.execute_reply.started":"2025-05-30T06:45:01.473901Z","shell.execute_reply":"2025-05-30T06:45:01.478678Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"BERT_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`BertConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `({0})`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n\n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n\n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:45:16.064937Z","iopub.execute_input":"2025-05-30T06:45:16.065527Z","iopub.status.idle":"2025-05-30T06:45:16.070277Z","shell.execute_reply.started":"2025-05-30T06:45:16.065502Z","shell.execute_reply":"2025-05-30T06:45:16.069653Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"@add_start_docstrings(\n    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n    BERT_START_DOCSTRING,\n)\nclass BertModel(BertPreTrainedModel):\n    _no_split_modules = [\"BertEmbeddings\", \"BertLayer\"] # 设定不可拆分的模块\n    # add_pooling_layer是否添加池化层\n    def __init__(self, config, add_pooling_layer=True): \n        super().__init__(config)\n        self.config = config\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n        self.attn_implementation = config._attn_implementation\n        self.position_embedding_type = config.position_embedding_type\n        # 执行权重初始化和其他后处理操作\n        self.post_init()\n    def get_input_embeddings(self): # 获取词嵌入\n        return self.embeddings.word_embeddings\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n    # 修剪每一层的头\n    def _prune_heads(self, heads_to_prune):\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        \n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder: # 如果是解码器架构\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else: # 编码器不用缓存\n            use_cache = False\n        # 设定输入形状\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n        batch_size, seq_length = input_shape \n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        # 缓存key,value \n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n        # 设定token type ids\n        if token_type_ids is None:\n            if hasattr(self.embeddings, \"token_type_ids\"):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n        embedding_output = self.embeddings( # 嵌入\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        # 设定默认的attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n        # 是否使用sdpa注意力mask\n        # 如果注意力实现是sdpa,并且位置嵌入类型是绝对,并且没有头掩码,并且不设定输出attentions\n        use_sdpa_attention_masks = (\n            self.attn_implementation == \"sdpa\"\n            and self.position_embedding_type == \"absolute\"\n            and head_mask is None\n            and not output_attentions\n        )\n        # 扩展attention_mask的维度\n        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n            # 如果是解码器架构\n            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n            if self.config.is_decoder: # 因果\n                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n                    attention_mask,\n                    input_shape,\n                    embedding_output,\n                    past_key_values_length,\n                )\n            else: \n                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n                )\n        else: \n            # 我们可以提供一个维度为 [batch_size, from_seq_length, to_seq_length] 的自注意力掩码\n            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # 如果是解码器跨注意力\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None: # 设定默认的编码器填充掩码\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n                # Expand the attention mask for SDPA.\n                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n                )\n            else:\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else: # 不是跨注意力的情况\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n        encoder_outputs = self.encoder( # 编码器输出\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:08:31.020559Z","iopub.execute_input":"2025-05-30T07:08:31.020842Z","iopub.status.idle":"2025-05-30T07:08:31.039890Z","shell.execute_reply.started":"2025-05-30T07:08:31.020822Z","shell.execute_reply":"2025-05-30T07:08:31.039227Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n    sentence prediction (classification)` head.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForPreTraining(BertPreTrainedModel):\n    # 指定要共享的权重键\n    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n        self.cls.predictions.bias = new_embeddings.bias\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        next_sentence_label: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n        r\"\"\"\n        Returns:\n        Example:\n        ```python\n        >>> from transformers import AutoTokenizer, BertForPreTraining\n        >>> import torch\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n        >>> model = BertForPreTraining.from_pretrained(\"google-bert/bert-base-uncased\")\n\n        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n        >>> outputs = model(**inputs)\n\n        >>> prediction_logits = outputs.prediction_logits\n        >>> seq_relationship_logits = outputs.seq_relationship_logits\n        ```\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        \n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        # outputs默认是元组\n        sequence_output, pooled_output = outputs[:2]\n        #(b,s,v),(b,2) MLM logits NSP logits\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n        total_loss = None \n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss() # 交叉熵损失函数\n            # 掩码语言损失\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss # 总损失 MLM 损失+NSP 损失\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:19:41.257828Z","iopub.execute_input":"2025-05-30T07:19:41.258121Z","iopub.status.idle":"2025-05-30T07:19:41.270018Z","shell.execute_reply.started":"2025-05-30T07:19:41.258100Z","shell.execute_reply":"2025-05-30T07:19:41.269321Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"help(BertForPreTraining)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForPreTraining","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:21:52.717747Z","iopub.execute_input":"2025-05-30T07:21:52.718488Z","iopub.status.idle":"2025-05-30T07:21:52.796255Z","shell.execute_reply.started":"2025-05-30T07:21:52.718465Z","shell.execute_reply":"2025-05-30T07:21:52.795667Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = BertForPreTraining.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:22:04.082766Z","iopub.execute_input":"2025-05-30T07:22:04.083072Z","iopub.status.idle":"2025-05-30T07:22:07.736952Z","shell.execute_reply.started":"2025-05-30T07:22:04.083050Z","shell.execute_reply":"2025-05-30T07:22:07.736359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc5a2a6e2dc49569ac8544f9a50a89e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79ce9f54272434786bb595822f0b61b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16d0a4cf60124954909b8e69dbb59168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9827f2999b88438e8e33a8253a5903d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e66602e785047b0be4b128149ce9887"}},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:22:18.186092Z","iopub.execute_input":"2025-05-30T07:22:18.186374Z","iopub.status.idle":"2025-05-30T07:22:18.857310Z","shell.execute_reply.started":"2025-05-30T07:22:18.186354Z","shell.execute_reply":"2025-05-30T07:22:18.856572Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:22:24.665706Z","iopub.execute_input":"2025-05-30T07:22:24.665998Z","iopub.status.idle":"2025-05-30T07:22:24.720767Z","shell.execute_reply.started":"2025-05-30T07:22:24.665950Z","shell.execute_reply":"2025-05-30T07:22:24.720212Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"BertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ -7.8962,  -7.8105,  -7.7903,  ...,  -7.0694,  -7.1693,  -4.3590],\n         [ -8.4461,  -8.4401,  -8.5044,  ...,  -8.0625,  -7.9909,  -5.7160],\n         [-15.2953, -15.4727, -15.5865,  ..., -12.9857, -11.7038, -11.4293],\n         ...,\n         [-14.0628, -14.2535, -14.3645,  ..., -12.7151, -11.1621, -10.2317],\n         [-10.6576, -10.7892, -11.0402,  ..., -10.3233, -10.1578,  -3.7721],\n         [-11.3383, -11.4590, -11.1767,  ...,  -9.2152,  -9.5209,  -9.5571]]],\n       grad_fn=<ViewBackward0>), seq_relationship_logits=tensor([[ 3.3474, -2.0613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"prediction_logits = outputs.prediction_logits\nprediction_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:22:54.790208Z","iopub.execute_input":"2025-05-30T07:22:54.790452Z","iopub.status.idle":"2025-05-30T07:22:54.795161Z","shell.execute_reply.started":"2025-05-30T07:22:54.790433Z","shell.execute_reply":"2025-05-30T07:22:54.794364Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 30522])"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"seq_relationship_logits = outputs.seq_relationship_logits\nseq_relationship_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:23:09.261705Z","iopub.execute_input":"2025-05-30T07:23:09.262178Z","iopub.status.idle":"2025-05-30T07:23:09.266537Z","shell.execute_reply.started":"2025-05-30T07:23:09.262156Z","shell.execute_reply":"2025-05-30T07:23:09.266004Z"}},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2])"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"# 生成形式的\n@add_start_docstrings(\n    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BERT_START_DOCSTRING\n)\nclass BertLMHeadModel(BertPreTrainedModel, GenerationMixin):\n    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n    def __init__(self, config):\n        super().__init__(config)\n        # 要使用BertLMHeadModel,必须设定解码器模式\n        if not config.is_decoder: \n            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n        # 设定只输出mlm logits,不输出池化\n        self.bert = BertModel(config, add_pooling_layer=False) \n        self.cls = BertOnlyMLMHead(config)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n        self.cls.predictions.bias = new_embeddings.bias\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=CausalLMOutputWithCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.Tensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **loss_kwargs,\n    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None: # 如果传入了标签,就说明是训练模式,不需要缓存\n            use_cache = False\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0] # (b,s,d)\n        prediction_scores = self.cls(sequence_output) # (b,s,v)\n        lm_loss = None\n        if labels is not None:\n            lm_loss = self.loss_function(prediction_scores,\n                                         labels, \n                                         self.config.vocab_size, \n                                         **loss_kwargs)\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def _reorder_cache(self, past_key_values, beam_idx):\n        reordered_past = () # 重新排序的缓存k,v\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:36:02.540152Z","iopub.execute_input":"2025-05-30T07:36:02.540753Z","iopub.status.idle":"2025-05-30T07:36:02.557006Z","shell.execute_reply.started":"2025-05-30T07:36:02.540729Z","shell.execute_reply":"2025-05-30T07:36:02.556426Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"help(BertLMHeadModel.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertLMHeadModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:37:14.182913Z","iopub.execute_input":"2025-05-30T07:37:14.183653Z","iopub.status.idle":"2025-05-30T07:37:14.186912Z","shell.execute_reply.started":"2025-05-30T07:37:14.183631Z","shell.execute_reply":"2025-05-30T07:37:14.186254Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = BertLMHeadModel.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:37:23.332955Z","iopub.execute_input":"2025-05-30T07:37:23.333467Z","iopub.status.idle":"2025-05-30T07:37:23.954117Z","shell.execute_reply.started":"2025-05-30T07:37:23.333444Z","shell.execute_reply":"2025-05-30T07:37:23.953591Z"}},"outputs":[{"name":"stderr","text":"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:37:50.823725Z","iopub.execute_input":"2025-05-30T07:37:50.824021Z","iopub.status.idle":"2025-05-30T07:37:50.953070Z","shell.execute_reply.started":"2025-05-30T07:37:50.823996Z","shell.execute_reply":"2025-05-30T07:37:50.952395Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"loss = outputs.loss\nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:38:08.505379Z","iopub.execute_input":"2025-05-30T07:38:08.505636Z","iopub.status.idle":"2025-05-30T07:38:08.511184Z","shell.execute_reply.started":"2025-05-30T07:38:08.505617Z","shell.execute_reply":"2025-05-30T07:38:08.510490Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"tensor(15.6525, grad_fn=<NllLossBackward0>)"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"logits = outputs.logits\nlogits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:38:27.768406Z","iopub.execute_input":"2025-05-30T07:38:27.769121Z","iopub.status.idle":"2025-05-30T07:38:27.773503Z","shell.execute_reply.started":"2025-05-30T07:38:27.769097Z","shell.execute_reply":"2025-05-30T07:38:27.772777Z"}},"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 30522])"},"metadata":{}}],"execution_count":89},{"cell_type":"code","source":"# 掩码语言模型\n@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top.\"\"\", BERT_START_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n    def __init__(self, config):\n        super().__init__(config)\n        # 这种是只有编码器的架构,掩码语言模型\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n        self.cls.predictions.bias = new_embeddings.bias\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"'paris'\",\n        expected_loss=0.88,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n        \n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n        masked_lm_loss = None # 掩码语言损失\n        # 计算掩码语言损失\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n    # 为生成准备输入\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape \n        effective_batch_size = input_shape[0] # 有效批次大小\n        #  add a dummy token\n        if self.config.pad_token_id is None:\n            raise ValueError(\"The PAD token should be defined for generation\")\n        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n        dummy_token = torch.full(\n            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n        )\n        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n    @classmethod # 类方法\n    def can_generate(cls) -> bool:\n        \"\"\"\n        Legacy correction: BertForMaskedLM can't call `generate()` from GenerationMixin.\n        Remove after v4.50, when we stop making `PreTrainedModel` inherit from `GenerationMixin`.\n        \"\"\"\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:46:25.986943Z","iopub.execute_input":"2025-05-30T07:46:25.987220Z","iopub.status.idle":"2025-05-30T07:46:26.001456Z","shell.execute_reply.started":"2025-05-30T07:46:25.987202Z","shell.execute_reply":"2025-05-30T07:46:26.000553Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"help(BertForMaskedLM.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForMaskedLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:47:21.258537Z","iopub.execute_input":"2025-05-30T07:47:21.259133Z","iopub.status.idle":"2025-05-30T07:47:21.262466Z","shell.execute_reply.started":"2025-05-30T07:47:21.259110Z","shell.execute_reply":"2025-05-30T07:47:21.261825Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:47:34.932278Z","iopub.execute_input":"2025-05-30T07:47:34.933013Z","iopub.status.idle":"2025-05-30T07:47:35.272045Z","shell.execute_reply.started":"2025-05-30T07:47:34.932957Z","shell.execute_reply":"2025-05-30T07:47:35.271336Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:47:43.895260Z","iopub.execute_input":"2025-05-30T07:47:43.896164Z","iopub.status.idle":"2025-05-30T07:47:43.900448Z","shell.execute_reply.started":"2025-05-30T07:47:43.896135Z","shell.execute_reply":"2025-05-30T07:47:43.899583Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:47:55.287569Z","iopub.execute_input":"2025-05-30T07:47:55.287839Z","iopub.status.idle":"2025-05-30T07:47:55.294793Z","shell.execute_reply.started":"2025-05-30T07:47:55.287821Z","shell.execute_reply":"2025-05-30T07:47:55.293956Z"}},"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":96},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:48:09.486727Z","iopub.execute_input":"2025-05-30T07:48:09.487513Z","iopub.status.idle":"2025-05-30T07:48:09.576687Z","shell.execute_reply.started":"2025-05-30T07:48:09.487478Z","shell.execute_reply":"2025-05-30T07:48:09.576117Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:48:24.935830Z","iopub.execute_input":"2025-05-30T07:48:24.936545Z","iopub.status.idle":"2025-05-30T07:48:24.940995Z","shell.execute_reply.started":"2025-05-30T07:48:24.936520Z","shell.execute_reply":"2025-05-30T07:48:24.940278Z"}},"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 9, 30522])"},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"# retrieve index of [MASK]\nmask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:48:40.705003Z","iopub.execute_input":"2025-05-30T07:48:40.705478Z","iopub.status.idle":"2025-05-30T07:48:40.716049Z","shell.execute_reply.started":"2025-05-30T07:48:40.705456Z","shell.execute_reply":"2025-05-30T07:48:40.715358Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"mask_token_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:48:47.490859Z","iopub.execute_input":"2025-05-30T07:48:47.491128Z","iopub.status.idle":"2025-05-30T07:48:47.496337Z","shell.execute_reply.started":"2025-05-30T07:48:47.491111Z","shell.execute_reply":"2025-05-30T07:48:47.495544Z"}},"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"tensor([6])"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:49:03.921509Z","iopub.execute_input":"2025-05-30T07:49:03.922243Z","iopub.status.idle":"2025-05-30T07:49:03.937648Z","shell.execute_reply.started":"2025-05-30T07:49:03.922217Z","shell.execute_reply":"2025-05-30T07:49:03.937004Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"logits[0, [0]].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:50:07.155588Z","iopub.execute_input":"2025-05-30T07:50:07.155854Z","iopub.status.idle":"2025-05-30T07:50:07.161865Z","shell.execute_reply.started":"2025-05-30T07:50:07.155835Z","shell.execute_reply":"2025-05-30T07:50:07.161170Z"}},"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 30522])"},"metadata":{}}],"execution_count":106},{"cell_type":"code","source":"predicted_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:50:23.318812Z","iopub.execute_input":"2025-05-30T07:50:23.319496Z","iopub.status.idle":"2025-05-30T07:50:23.324277Z","shell.execute_reply.started":"2025-05-30T07:50:23.319460Z","shell.execute_reply":"2025-05-30T07:50:23.323726Z"}},"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"tensor([3000])"},"metadata":{}}],"execution_count":107},{"cell_type":"code","source":"tokenizer.decode(predicted_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:50:35.504741Z","iopub.execute_input":"2025-05-30T07:50:35.505471Z","iopub.status.idle":"2025-05-30T07:50:35.510033Z","shell.execute_reply.started":"2025-05-30T07:50:35.505446Z","shell.execute_reply":"2025-05-30T07:50:35.509389Z"}},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"'paris'"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:50:49.143862Z","iopub.execute_input":"2025-05-30T07:50:49.144326Z","iopub.status.idle":"2025-05-30T07:50:49.148198Z","shell.execute_reply.started":"2025-05-30T07:50:49.144303Z","shell.execute_reply":"2025-05-30T07:50:49.147589Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"labels ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:50:57.619173Z","iopub.execute_input":"2025-05-30T07:50:57.619908Z","iopub.status.idle":"2025-05-30T07:50:57.625170Z","shell.execute_reply.started":"2025-05-30T07:50:57.619887Z","shell.execute_reply":"2025-05-30T07:50:57.624535Z"}},"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"tensor([[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102]])"},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"# mask labels of non-[MASK] tokens\nlabels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:51:22.267036Z","iopub.execute_input":"2025-05-30T07:51:22.267739Z","iopub.status.idle":"2025-05-30T07:51:22.278040Z","shell.execute_reply.started":"2025-05-30T07:51:22.267712Z","shell.execute_reply":"2025-05-30T07:51:22.277340Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"labels ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:51:30.139043Z","iopub.execute_input":"2025-05-30T07:51:30.139732Z","iopub.status.idle":"2025-05-30T07:51:30.144810Z","shell.execute_reply.started":"2025-05-30T07:51:30.139710Z","shell.execute_reply":"2025-05-30T07:51:30.144181Z"}},"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"tensor([[-100, -100, -100, -100, -100, -100, 3000, -100, -100]])"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"outputs = model(**inputs, labels=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:51:37.949591Z","iopub.execute_input":"2025-05-30T07:51:37.949842Z","iopub.status.idle":"2025-05-30T07:51:38.026263Z","shell.execute_reply.started":"2025-05-30T07:51:37.949827Z","shell.execute_reply":"2025-05-30T07:51:38.025664Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"round(outputs.loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:51:44.956339Z","iopub.execute_input":"2025-05-30T07:51:44.956627Z","iopub.status.idle":"2025-05-30T07:51:44.962222Z","shell.execute_reply.started":"2025-05-30T07:51:44.956607Z","shell.execute_reply":"2025-05-30T07:51:44.961209Z"}},"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"0.88"},"metadata":{}}],"execution_count":114},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top.\"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        # Initialize weights and apply final processing\n        self.post_init()\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Tuple[torch.Tensor], NextSentencePredictorOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see `input_ids` docstring). Indices should be in `[0, 1]`:\n\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, BertForNextSentencePrediction\n        >>> import torch\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n        >>> model = BertForNextSentencePrediction.from_pretrained(\"google-bert/bert-base-uncased\")\n\n        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n        >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n        >>> encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n\n        >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n        >>> logits = outputs.logits\n        >>> assert logits[0, 0] < logits[0, 1]  # next sentence was random\n        ```\n        \"\"\"\n        if \"next_sentence_label\" in kwargs:\n            warnings.warn(\n                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use\"\n                \" `labels` instead.\",\n                FutureWarning,\n            )\n            labels = kwargs.pop(\"next_sentence_label\")\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        pooled_output = outputs[1] # 池化输出\n        seq_relationship_scores = self.cls(pooled_output) # (b,2)\n        next_sentence_loss = None  \n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n\n        if not return_dict:\n            output = (seq_relationship_scores,) + outputs[2:]\n            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n\n        return NextSentencePredictorOutput(\n            loss=next_sentence_loss,\n            logits=seq_relationship_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:54:29.375446Z","iopub.execute_input":"2025-05-30T07:54:29.375715Z","iopub.status.idle":"2025-05-30T07:54:29.387424Z","shell.execute_reply.started":"2025-05-30T07:54:29.375697Z","shell.execute_reply":"2025-05-30T07:54:29.386685Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"help(BertForNextSentencePrediction.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForNextSentencePrediction # 下一句预测","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:55:12.881595Z","iopub.execute_input":"2025-05-30T07:55:12.882205Z","iopub.status.idle":"2025-05-30T07:55:12.885617Z","shell.execute_reply.started":"2025-05-30T07:55:12.882184Z","shell.execute_reply":"2025-05-30T07:55:12.885006Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = BertForNextSentencePrediction.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:55:33.293140Z","iopub.execute_input":"2025-05-30T07:55:33.293932Z","iopub.status.idle":"2025-05-30T07:55:33.608290Z","shell.execute_reply.started":"2025-05-30T07:55:33.293907Z","shell.execute_reply":"2025-05-30T07:55:33.607502Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:55:40.909106Z","iopub.execute_input":"2025-05-30T07:55:40.909397Z","iopub.status.idle":"2025-05-30T07:55:40.913007Z","shell.execute_reply.started":"2025-05-30T07:55:40.909376Z","shell.execute_reply":"2025-05-30T07:55:40.912439Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\nencoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:55:53.595907Z","iopub.execute_input":"2025-05-30T07:55:53.596517Z","iopub.status.idle":"2025-05-30T07:55:53.600632Z","shell.execute_reply.started":"2025-05-30T07:55:53.596495Z","shell.execute_reply":"2025-05-30T07:55:53.600010Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:55:59.147092Z","iopub.execute_input":"2025-05-30T07:55:59.147656Z","iopub.status.idle":"2025-05-30T07:55:59.153599Z","shell.execute_reply.started":"2025-05-30T07:55:59.147636Z","shell.execute_reply":"2025-05-30T07:55:59.152900Z"}},"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  1999,  3304,  1010, 10733,  2366,  1999,  5337, 10906,  1010,\n          2107,  2004,  2012,  1037,  4825,  1010,  2003,  3591,  4895, 14540,\n          6610,  2094,  1012,   102,  1996,  3712,  2003,  2630,  2349,  2000,\n          1996,  7820, 19934,  1997,  2630,  2422,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"outputs = model(**encoding, labels=torch.LongTensor([1]))\nlogits = outputs.logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:56:47.592036Z","iopub.execute_input":"2025-05-30T07:56:47.592314Z","iopub.status.idle":"2025-05-30T07:56:47.747457Z","shell.execute_reply.started":"2025-05-30T07:56:47.592295Z","shell.execute_reply":"2025-05-30T07:56:47.746485Z"}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:56:56.448203Z","iopub.execute_input":"2025-05-30T07:56:56.448479Z","iopub.status.idle":"2025-05-30T07:56:56.453352Z","shell.execute_reply.started":"2025-05-30T07:56:56.448459Z","shell.execute_reply":"2025-05-30T07:56:56.452761Z"}},"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2])"},"metadata":{}}],"execution_count":123},{"cell_type":"code","source":"logits[0, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:57:21.325003Z","iopub.execute_input":"2025-05-30T07:57:21.325284Z","iopub.status.idle":"2025-05-30T07:57:21.331572Z","shell.execute_reply.started":"2025-05-30T07:57:21.325265Z","shell.execute_reply":"2025-05-30T07:57:21.331014Z"}},"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"tensor(-3.0729, grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":124},{"cell_type":"code","source":"assert logits[0, 0] < logits[0, 1]  # next sentence was random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:57:30.130811Z","iopub.execute_input":"2025-05-30T07:57:30.131429Z","iopub.status.idle":"2025-05-30T07:57:30.135232Z","shell.execute_reply.started":"2025-05-30T07:57:30.131399Z","shell.execute_reply":"2025-05-30T07:57:30.134663Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"# 添加起始文档字符串\n@add_start_docstrings(\n    \"\"\"\n    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForSequenceClassification(BertPreTrainedModel):  # 序列分类\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n        self.bert = BertModel(config)\n        classifier_dropout = ( #  分类器dropout率\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels) # 几分类\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        # 是否返回字典\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.bert( \n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1] # 池化输出\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output) # 分类\n\n        loss = None # 设定分类损失\n        if labels is not None: # 如果标签存在\n            if self.config.problem_type is None: # 如果配置中配置了问题类型\n                if self.num_labels == 1: # 如果num_labels == 1\n                    self.config.problem_type = \"regression\" # 回归\n                # num_labels > 1,单标签分类\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else: # 如果标签类型不是整数,多标签分类\n                    self.config.problem_type = \"multi_label_classification\"\n            # 如果是回归类型\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss() \n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            # 单标签分类\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\": # 多标签分类\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n        if not return_dict: \n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n        # 序列分类器输出\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:06:52.072317Z","iopub.execute_input":"2025-05-30T08:06:52.072622Z","iopub.status.idle":"2025-05-30T08:06:52.086702Z","shell.execute_reply.started":"2025-05-30T08:06:52.072602Z","shell.execute_reply":"2025-05-30T08:06:52.086010Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"help(BertForSequenceClassification.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\nmodel = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:08:23.562234Z","iopub.execute_input":"2025-05-30T08:08:23.562868Z","iopub.status.idle":"2025-05-30T08:08:27.411777Z","shell.execute_reply.started":"2025-05-30T08:08:23.562843Z","shell.execute_reply":"2025-05-30T08:08:27.411241Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b361d487a5834582ab0082d7a493df2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/520 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0e507840ca4140885c38c6d3c821f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffec406850c433fbaa9ce89732951c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"840aa78be56a41baa63d34473d6a34a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7afc438da617477394b7f9b25fdb792a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6e3be71a344fbf818f446651752894"}},"metadata":{}}],"execution_count":128},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:08:31.910764Z","iopub.execute_input":"2025-05-30T08:08:31.911360Z","iopub.status.idle":"2025-05-30T08:08:31.915081Z","shell.execute_reply.started":"2025-05-30T08:08:31.911338Z","shell.execute_reply":"2025-05-30T08:08:31.914531Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:08:42.161383Z","iopub.execute_input":"2025-05-30T08:08:42.161648Z","iopub.status.idle":"2025-05-30T08:08:42.226602Z","shell.execute_reply.started":"2025-05-30T08:08:42.161630Z","shell.execute_reply":"2025-05-30T08:08:42.226008Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"predicted_class_id = logits.argmax().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:08:48.954287Z","iopub.execute_input":"2025-05-30T08:08:48.954982Z","iopub.status.idle":"2025-05-30T08:08:48.959607Z","shell.execute_reply.started":"2025-05-30T08:08:48.954929Z","shell.execute_reply":"2025-05-30T08:08:48.958801Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"predicted_class_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:08:58.415276Z","iopub.execute_input":"2025-05-30T08:08:58.415567Z","iopub.status.idle":"2025-05-30T08:08:58.420656Z","shell.execute_reply.started":"2025-05-30T08:08:58.415547Z","shell.execute_reply":"2025-05-30T08:08:58.420088Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"model.config.id2label[predicted_class_id]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:09:06.847492Z","iopub.execute_input":"2025-05-30T08:09:06.847767Z","iopub.status.idle":"2025-05-30T08:09:06.852988Z","shell.execute_reply.started":"2025-05-30T08:09:06.847748Z","shell.execute_reply":"2025-05-30T08:09:06.852373Z"}},"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"'LABEL_1'"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\nnum_labels = len(model.config.id2label)\nnum_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:09:36.280186Z","iopub.execute_input":"2025-05-30T08:09:36.280465Z","iopub.status.idle":"2025-05-30T08:09:36.285811Z","shell.execute_reply.started":"2025-05-30T08:09:36.280447Z","shell.execute_reply":"2025-05-30T08:09:36.285036Z"}},"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":134},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    \"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels)\nlabels = torch.tensor([1])\nloss = model(**inputs, labels=labels).loss\nround(loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:10:05.261361Z","iopub.execute_input":"2025-05-30T08:10:05.261669Z","iopub.status.idle":"2025-05-30T08:10:06.169075Z","shell.execute_reply.started":"2025-05-30T08:10:05.261649Z","shell.execute_reply":"2025-05-30T08:10:06.168328Z"}},"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"0.01"},"metadata":{}}],"execution_count":135},{"cell_type":"code","source":" \n    # >>> tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n    # >>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", problem_type=\"multi_label_classification\")\n    \n    # >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n    \n    # >>> with torch.no_grad():\n    # ...     logits = model(**inputs).logits\n    \n    # >>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n    \n    # >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n    # >>> num_labels = len(model.config.id2label)\n    # >>> model = BertForSequenceClassification.from_pretrained(\n    # ...     \"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n    # ... )\n    \n    # >>> labels = torch.sum(\n    # ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n    # ... ).to(torch.float)\n    # >>> loss = model(**inputs, labels=labels).loss\n    # ```","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForMultipleChoice(BertPreTrainedModel): # 多选任务\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        # Initialize weights and apply final processing\n        self.post_init()\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n            `input_ids` above)\n        \"\"\"\n        # 是否返回字典形式\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1] # 选项大小\n        # -->(b*num_choices,s)\n        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert( \n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1] # 池化输出\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output) #(b*num_choices,1)\n        reshaped_logits = logits.view(-1, num_choices)\n        loss = None \n        if labels is not None: # 如果没传入标签\n            loss_fct = CrossEntropyLoss() # 交叉熵损失\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:17:38.808367Z","iopub.execute_input":"2025-05-30T08:17:38.808666Z","iopub.status.idle":"2025-05-30T08:17:38.821813Z","shell.execute_reply.started":"2025-05-30T08:17:38.808645Z","shell.execute_reply":"2025-05-30T08:17:38.821266Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"help(BertForMultipleChoice.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForMultipleChoice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:18:22.968333Z","iopub.execute_input":"2025-05-30T08:18:22.969070Z","iopub.status.idle":"2025-05-30T08:18:22.972183Z","shell.execute_reply.started":"2025-05-30T08:18:22.969048Z","shell.execute_reply":"2025-05-30T08:18:22.971466Z"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = BertForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:18:32.671302Z","iopub.execute_input":"2025-05-30T08:18:32.671557Z","iopub.status.idle":"2025-05-30T08:18:33.027428Z","shell.execute_reply.started":"2025-05-30T08:18:32.671538Z","shell.execute_reply":"2025-05-30T08:18:33.026875Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForMultipleChoice were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\nchoice0 = \"It is eaten with a fork and a knife.\"\nchoice1 = \"It is eaten while held in the hand.\"\nlabels = torch.tensor(0).unsqueeze(0)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:18:59.333047Z","iopub.execute_input":"2025-05-30T08:18:59.333774Z","iopub.status.idle":"2025-05-30T08:18:59.337415Z","shell.execute_reply.started":"2025-05-30T08:18:59.333754Z","shell.execute_reply":"2025-05-30T08:18:59.336642Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:19:53.322156Z","iopub.execute_input":"2025-05-30T08:19:53.322690Z","iopub.status.idle":"2025-05-30T08:19:53.327908Z","shell.execute_reply.started":"2025-05-30T08:19:53.322667Z","shell.execute_reply":"2025-05-30T08:19:53.327125Z"}},"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"tensor([0])"},"metadata":{}}],"execution_count":144},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:19:08.588747Z","iopub.execute_input":"2025-05-30T08:19:08.589286Z","iopub.status.idle":"2025-05-30T08:19:08.593810Z","shell.execute_reply.started":"2025-05-30T08:19:08.589262Z","shell.execute_reply":"2025-05-30T08:19:08.593091Z"}},"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"torch.Size([1])"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"# choice0 is correct (according to Wikipedia ;)), batch size 1\nencoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:20:46.059131Z","iopub.execute_input":"2025-05-30T08:20:46.059413Z","iopub.status.idle":"2025-05-30T08:20:46.063766Z","shell.execute_reply.started":"2025-05-30T08:20:46.059392Z","shell.execute_reply":"2025-05-30T08:20:46.063219Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"encoding['input_ids'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:21:49.266026Z","iopub.execute_input":"2025-05-30T08:21:49.266656Z","iopub.status.idle":"2025-05-30T08:21:49.271188Z","shell.execute_reply.started":"2025-05-30T08:21:49.266632Z","shell.execute_reply":"2025-05-30T08:21:49.270428Z"}},"outputs":[{"execution_count":147,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 35])"},"metadata":{}}],"execution_count":147},{"cell_type":"code","source":"aa={k: v.unsqueeze(0) for k, v in encoding.items()}\naa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:22:55.986792Z","iopub.execute_input":"2025-05-30T08:22:55.987159Z","iopub.status.idle":"2025-05-30T08:22:55.994079Z","shell.execute_reply.started":"2025-05-30T08:22:55.987137Z","shell.execute_reply":"2025-05-30T08:22:55.993455Z"}},"outputs":[{"execution_count":150,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[[  101,  1999,  3304,  1010, 10733,  2366,  1999,  5337, 10906,  1010,\n            2107,  2004,  2012,  1037,  4825,  1010,  2003,  3591,  4895, 14540,\n            6610,  2094,  1012,   102,  2009,  2003,  8828,  2007,  1037,  9292,\n            1998,  1037,  5442,  1012,   102],\n          [  101,  1999,  3304,  1010, 10733,  2366,  1999,  5337, 10906,  1010,\n            2107,  2004,  2012,  1037,  4825,  1010,  2003,  3591,  4895, 14540,\n            6610,  2094,  1012,   102,  2009,  2003,  8828,  2096,  2218,  1999,\n            1996,  2192,  1012,   102,     0]]]),\n 'token_type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]),\n 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]])}"},"metadata":{}}],"execution_count":150},{"cell_type":"code","source":"aa['input_ids'].shape ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:24:57.709389Z","iopub.execute_input":"2025-05-30T08:24:57.709646Z","iopub.status.idle":"2025-05-30T08:24:57.714616Z","shell.execute_reply.started":"2025-05-30T08:24:57.709629Z","shell.execute_reply":"2025-05-30T08:24:57.713848Z"}},"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2, 35])"},"metadata":{}}],"execution_count":155},{"cell_type":"code","source":"outputs = model(**aa, labels=labels)  # batch size is 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:24:59.890099Z","iopub.execute_input":"2025-05-30T08:24:59.890661Z","iopub.status.idle":"2025-05-30T08:25:00.013729Z","shell.execute_reply.started":"2025-05-30T08:24:59.890638Z","shell.execute_reply":"2025-05-30T08:25:00.012956Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"outputs.logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:25:17.036860Z","iopub.execute_input":"2025-05-30T08:25:17.037160Z","iopub.status.idle":"2025-05-30T08:25:17.042697Z","shell.execute_reply.started":"2025-05-30T08:25:17.037128Z","shell.execute_reply":"2025-05-30T08:25:17.042096Z"}},"outputs":[{"execution_count":158,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0077, -0.0309]], grad_fn=<ViewBackward0>)"},"metadata":{}}],"execution_count":158},{"cell_type":"code","source":"print(outputs.logits.shape,outputs.loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:25:40.423313Z","iopub.execute_input":"2025-05-30T08:25:40.424035Z","iopub.status.idle":"2025-05-30T08:25:40.428637Z","shell.execute_reply.started":"2025-05-30T08:25:40.424004Z","shell.execute_reply":"2025-05-30T08:25:40.428013Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 2]) tensor(0.6816, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":160},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForTokenClassification(BertPreTrainedModel): # Token分类\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = BertModel(config, add_pooling_layer=False) # 不带池化输出\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert( \n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0] # (b,s,d)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output) # (b,s,num_labels)\n\n        loss = None\n        if labels is not None: # 如果传入了标签\n            loss_fct = CrossEntropyLoss() \n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:29:11.421175Z","iopub.execute_input":"2025-05-30T08:29:11.421791Z","iopub.status.idle":"2025-05-30T08:29:11.433589Z","shell.execute_reply.started":"2025-05-30T08:29:11.421767Z","shell.execute_reply":"2025-05-30T08:29:11.432770Z"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"help(BertForTokenClassification.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForTokenClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:29:53.019206Z","iopub.execute_input":"2025-05-30T08:29:53.019945Z","iopub.status.idle":"2025-05-30T08:29:53.023430Z","shell.execute_reply.started":"2025-05-30T08:29:53.019918Z","shell.execute_reply":"2025-05-30T08:29:53.022630Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\nmodel = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:30:04.283940Z","iopub.execute_input":"2025-05-30T08:30:04.284685Z","iopub.status.idle":"2025-05-30T08:30:10.494949Z","shell.execute_reply.started":"2025-05-30T08:30:04.284658Z","shell.execute_reply":"2025-05-30T08:30:10.494405Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545f2a40a4ac42d8b9bfd22c631ad04b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec7dcfaba6c40218d04234979b71de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c993ab419f124fecb318d28580e1c4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153bbf0275df4fa3a85737fd1f4d70b3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":164},{"cell_type":"code","source":"# add_special_tokens=False不添加特殊token\ninputs = tokenizer(\n   \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:30:58.528652Z","iopub.execute_input":"2025-05-30T08:30:58.528948Z","iopub.status.idle":"2025-05-30T08:30:58.533619Z","shell.execute_reply.started":"2025-05-30T08:30:58.528927Z","shell.execute_reply":"2025-05-30T08:30:58.533026Z"}},"outputs":[],"execution_count":165},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:31:05.028263Z","iopub.execute_input":"2025-05-30T08:31:05.028527Z","iopub.status.idle":"2025-05-30T08:31:05.034810Z","shell.execute_reply.started":"2025-05-30T08:31:05.028508Z","shell.execute_reply":"2025-05-30T08:31:05.034085Z"}},"outputs":[{"execution_count":166,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[20164, 10932,  2271,  7954,  1110,   170,  1419,  1359,  1107,  2123,\n          1105,  1203,  1365]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":166},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:31:18.813495Z","iopub.execute_input":"2025-05-30T08:31:18.813778Z","iopub.status.idle":"2025-05-30T08:31:19.151830Z","shell.execute_reply.started":"2025-05-30T08:31:18.813758Z","shell.execute_reply":"2025-05-30T08:31:19.151011Z"}},"outputs":[],"execution_count":167},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:31:35.842499Z","iopub.execute_input":"2025-05-30T08:31:35.842768Z","iopub.status.idle":"2025-05-30T08:31:35.847857Z","shell.execute_reply.started":"2025-05-30T08:31:35.842744Z","shell.execute_reply":"2025-05-30T08:31:35.847199Z"}},"outputs":[{"execution_count":168,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 13, 9])"},"metadata":{}}],"execution_count":168},{"cell_type":"code","source":"predicted_token_class_ids = logits.argmax(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:31:43.704210Z","iopub.execute_input":"2025-05-30T08:31:43.704519Z","iopub.status.idle":"2025-05-30T08:31:43.708508Z","shell.execute_reply.started":"2025-05-30T08:31:43.704496Z","shell.execute_reply":"2025-05-30T08:31:43.707661Z"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":"predicted_token_class_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:34:25.847810Z","iopub.execute_input":"2025-05-30T08:34:25.848151Z","iopub.status.idle":"2025-05-30T08:34:25.853640Z","shell.execute_reply.started":"2025-05-30T08:34:25.848093Z","shell.execute_reply":"2025-05-30T08:34:25.853026Z"}},"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 6, 6, 6, 0, 0, 0, 0, 0, 8, 0, 8, 8]])"},"metadata":{}}],"execution_count":174},{"cell_type":"code","source":"# 注意，对 token 进行分类，而不是对输入词进行分类，这意味着预测的标记类别可能比单词多。多个 token可能代表同一个词\npredicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\npredicted_tokens_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:33:24.170105Z","iopub.execute_input":"2025-05-30T08:33:24.170419Z","iopub.status.idle":"2025-05-30T08:33:24.177223Z","shell.execute_reply.started":"2025-05-30T08:33:24.170400Z","shell.execute_reply":"2025-05-30T08:33:24.176554Z"}},"outputs":[{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"['O',\n 'I-ORG',\n 'I-ORG',\n 'I-ORG',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'I-LOC',\n 'O',\n 'I-LOC',\n 'I-LOC']"},"metadata":{}}],"execution_count":171},{"cell_type":"code","source":"labels = predicted_token_class_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:34:05.985424Z","iopub.execute_input":"2025-05-30T08:34:05.985886Z","iopub.status.idle":"2025-05-30T08:34:05.989092Z","shell.execute_reply.started":"2025-05-30T08:34:05.985864Z","shell.execute_reply":"2025-05-30T08:34:05.988499Z"}},"outputs":[],"execution_count":172},{"cell_type":"code","source":"loss = model(**inputs, labels=labels).loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:34:29.442120Z","iopub.execute_input":"2025-05-30T08:34:29.442584Z","iopub.status.idle":"2025-05-30T08:34:29.701021Z","shell.execute_reply.started":"2025-05-30T08:34:29.442562Z","shell.execute_reply":"2025-05-30T08:34:29.700402Z"}},"outputs":[],"execution_count":175},{"cell_type":"code","source":"round(loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:34:34.986183Z","iopub.execute_input":"2025-05-30T08:34:34.986454Z","iopub.status.idle":"2025-05-30T08:34:34.991038Z","shell.execute_reply.started":"2025-05-30T08:34:34.986433Z","shell.execute_reply":"2025-05-30T08:34:34.990488Z"}},"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"0.01"},"metadata":{}}],"execution_count":176},{"cell_type":"code","source":"config.num_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:36:47.484297Z","iopub.execute_input":"2025-05-30T08:36:47.484627Z","iopub.status.idle":"2025-05-30T08:36:47.489998Z","shell.execute_reply.started":"2025-05-30T08:36:47.484606Z","shell.execute_reply":"2025-05-30T08:36:47.489391Z"}},"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":177},{"cell_type":"code","source":"aaa=torch.randn((2,3,2))\naaa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:41:02.044077Z","iopub.execute_input":"2025-05-30T08:41:02.044339Z","iopub.status.idle":"2025-05-30T08:41:02.050667Z","shell.execute_reply.started":"2025-05-30T08:41:02.044321Z","shell.execute_reply":"2025-05-30T08:41:02.050018Z"}},"outputs":[{"execution_count":186,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 1.6488,  1.5183],\n         [ 0.6272, -1.4035],\n         [-0.3378,  2.1239]],\n\n        [[-0.3483, -0.5490],\n         [ 0.0897, -0.4275],\n         [-0.5186, -0.3892]]])"},"metadata":{}}],"execution_count":186},{"cell_type":"code","source":"len(aaa.split(2, dim=-1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:41:06.644783Z","iopub.execute_input":"2025-05-30T08:41:06.645079Z","iopub.status.idle":"2025-05-30T08:41:06.650216Z","shell.execute_reply.started":"2025-05-30T08:41:06.645060Z","shell.execute_reply":"2025-05-30T08:41:06.649479Z"}},"outputs":[{"execution_count":187,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":187},{"cell_type":"code","source":"len(aaa.split(1, dim=-1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:41:18.496586Z","iopub.execute_input":"2025-05-30T08:41:18.496848Z","iopub.status.idle":"2025-05-30T08:41:18.501920Z","shell.execute_reply.started":"2025-05-30T08:41:18.496829Z","shell.execute_reply":"2025-05-30T08:41:18.501253Z"}},"outputs":[{"execution_count":189,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":189},{"cell_type":"code","source":"aaa.split(1, dim=-1)[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:41:54.174025Z","iopub.execute_input":"2025-05-30T08:41:54.174714Z","iopub.status.idle":"2025-05-30T08:41:54.179656Z","shell.execute_reply.started":"2025-05-30T08:41:54.174691Z","shell.execute_reply":"2025-05-30T08:41:54.179015Z"}},"outputs":[{"execution_count":191,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 3, 1])"},"metadata":{}}],"execution_count":191},{"cell_type":"code","source":"aaa.split(1, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:42:02.180308Z","iopub.execute_input":"2025-05-30T08:42:02.180852Z","iopub.status.idle":"2025-05-30T08:42:02.186981Z","shell.execute_reply.started":"2025-05-30T08:42:02.180829Z","shell.execute_reply":"2025-05-30T08:42:02.186218Z"}},"outputs":[{"execution_count":192,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 1.6488],\n          [ 0.6272],\n          [-0.3378]],\n \n         [[-0.3483],\n          [ 0.0897],\n          [-0.5186]]]),\n tensor([[[ 1.5183],\n          [-1.4035],\n          [ 2.1239]],\n \n         [[-0.5490],\n          [-0.4275],\n          [-0.3892]]]))"},"metadata":{}}],"execution_count":192},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels \n        self.bert = BertModel(config, add_pooling_layer=False) # 不带池化层\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n        # 初始化权重并应用最终处理\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_QA,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        qa_target_start_index=_QA_TARGET_START_INDEX,\n        qa_target_end_index=_QA_TARGET_END_INDEX,\n        expected_output=_QA_EXPECTED_OUTPUT,\n        expected_loss=_QA_EXPECTED_LOSS,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        start_positions: Optional[torch.Tensor] = None,\n        end_positions: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0] # (b,s,d)\n        logits = self.qa_outputs(sequence_output) # (b,s,2)\n        start_logits, end_logits = logits.split(1, dim=-1) # (b,s,1)\n        start_logits = start_logits.squeeze(-1).contiguous() \n        end_logits = end_logits.squeeze(-1).contiguous()\n        total_loss = None\n        # 如果存在起始和结束位置\n        if start_positions is not None and end_positions is not None:\n            # 如果我们使用多 GPU，则拆分添加一个维度\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            ignored_index = start_logits.size(1)\n            # 限制到一定范围\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n            # 计算损失\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:46:13.899292Z","iopub.execute_input":"2025-05-30T08:46:13.899889Z","iopub.status.idle":"2025-05-30T08:46:13.912342Z","shell.execute_reply.started":"2025-05-30T08:46:13.899867Z","shell.execute_reply":"2025-05-30T08:46:13.911782Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"help(BertForQuestionAnswering.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertForQuestionAnswering","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:47:04.171855Z","iopub.execute_input":"2025-05-30T08:47:04.172498Z","iopub.status.idle":"2025-05-30T08:47:04.176107Z","shell.execute_reply.started":"2025-05-30T08:47:04.172474Z","shell.execute_reply":"2025-05-30T08:47:04.175406Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\nmodel = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:47:25.891128Z","iopub.execute_input":"2025-05-30T08:47:25.891669Z","iopub.status.idle":"2025-05-30T08:47:29.295762Z","shell.execute_reply.started":"2025-05-30T08:47:25.891649Z","shell.execute_reply":"2025-05-30T08:47:29.295207Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58d2caaff764b09be9213a20202e55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48163c23b7b14794b3175fd950df026c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c44530f05a434f9d3b834fd4eaaafd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77da55acab44ac88e57cda5da01ded0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5835b589af1f4544ad27239da836ad63"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":196},{"cell_type":"code","source":"question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:47:36.782758Z","iopub.execute_input":"2025-05-30T08:47:36.783059Z","iopub.status.idle":"2025-05-30T08:47:36.786556Z","shell.execute_reply.started":"2025-05-30T08:47:36.783037Z","shell.execute_reply":"2025-05-30T08:47:36.786031Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"inputs = tokenizer(question, text, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:47:44.778228Z","iopub.execute_input":"2025-05-30T08:47:44.779018Z","iopub.status.idle":"2025-05-30T08:47:44.782894Z","shell.execute_reply.started":"2025-05-30T08:47:44.778989Z","shell.execute_reply":"2025-05-30T08:47:44.782295Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:48:17.423433Z","iopub.execute_input":"2025-05-30T08:48:17.424024Z","iopub.status.idle":"2025-05-30T08:48:17.429439Z","shell.execute_reply.started":"2025-05-30T08:48:17.423993Z","shell.execute_reply":"2025-05-30T08:48:17.428827Z"}},"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2627,  1108,  3104,  1124, 15703,   136,   102,  3104,  1124,\n         15703,  1108,   170,  3505, 16797,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":200},{"cell_type":"code","source":"with torch.no_grad():\n    outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:48:09.556855Z","iopub.execute_input":"2025-05-30T08:48:09.557579Z","iopub.status.idle":"2025-05-30T08:48:09.642444Z","shell.execute_reply.started":"2025-05-30T08:48:09.557554Z","shell.execute_reply":"2025-05-30T08:48:09.641723Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"answer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:48:32.478809Z","iopub.execute_input":"2025-05-30T08:48:32.479101Z","iopub.status.idle":"2025-05-30T08:48:32.483084Z","shell.execute_reply.started":"2025-05-30T08:48:32.479080Z","shell.execute_reply":"2025-05-30T08:48:32.482486Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"print(answer_start_index,answer_end_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:49:06.498149Z","iopub.execute_input":"2025-05-30T08:49:06.498420Z","iopub.status.idle":"2025-05-30T08:49:06.503364Z","shell.execute_reply.started":"2025-05-30T08:49:06.498402Z","shell.execute_reply":"2025-05-30T08:49:06.502596Z"}},"outputs":[{"name":"stdout","text":"tensor(12) tensor(14)\n","output_type":"stream"}],"execution_count":203},{"cell_type":"code","source":"predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:49:19.572895Z","iopub.execute_input":"2025-05-30T08:49:19.573625Z","iopub.status.idle":"2025-05-30T08:49:19.578823Z","shell.execute_reply.started":"2025-05-30T08:49:19.573602Z","shell.execute_reply":"2025-05-30T08:49:19.578096Z"}},"outputs":[{"execution_count":204,"output_type":"execute_result","data":{"text/plain":"'a nice puppet'"},"metadata":{}}],"execution_count":204},{"cell_type":"code","source":"# target is \"nice puppet\"\ntarget_start_index = torch.tensor([14])\ntarget_end_index = torch.tensor([15])\noutputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\nloss = outputs.loss\nround(loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:49:58.574505Z","iopub.execute_input":"2025-05-30T08:49:58.575294Z","iopub.status.idle":"2025-05-30T08:49:58.698685Z","shell.execute_reply.started":"2025-05-30T08:49:58.575269Z","shell.execute_reply":"2025-05-30T08:49:58.698084Z"}},"outputs":[{"execution_count":206,"output_type":"execute_result","data":{"text/plain":"7.41"},"metadata":{}}],"execution_count":206},{"cell_type":"code","source":"__all__ = [\n    \"BertForMaskedLM\", # 掩码语言模型\n    \"BertForMultipleChoice\", # 多选\n    \"BertForNextSentencePrediction\", # nsp预测\n    \"BertForPreTraining\",\n    \"BertForQuestionAnswering\", # 问答\n    \"BertForSequenceClassification\", # 序列分类\n    \"BertForTokenClassification\", # 标记分类\n    \"BertLayer\", \n    \"BertLMHeadModel\", \n    \"BertModel\",\n    \"BertPreTrainedModel\",\n    \"load_tf_weights_in_bert\",\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}