{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Callable, Optional, Tuple # 用于类型注解，提高代码可读性和静态检查\nimport flax\nimport flax.linen as nn # flax.linen 是其模块化 API，类似于 PyTorch 的 nn.Module\nimport jax\nimport jax.numpy as jnp # jax.numpy 是其 NumPy 接口，用于张量操作\nimport numpy as np # 标准的数值计算库\n# Flax 中的 FrozenDict 是不可变的嵌套字典，用于存储模型参数和状态；freeze/unfreeze 用于参数的冻结和解冻\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\n# Flax 中实现的标准点积注意力机制，用于计算注意力权重\nfrom flax.linen.attention import dot_product_attention_weights\n# 用于将嵌套结构的参数字典展开/重建，方便操作模型参数。\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax import lax # JAX 中的底层操作接口，支持控制流、跨设备通信等\n# HuggingFace 提供的 Flax 模型输出类，用于组织模型的返回结果，适配不同任务（如分类、问答、填空等）。\nfrom transformers.modeling_flax_outputs import (\n    FlaxBaseModelOutput,\n    FlaxBaseModelOutputWithPooling,\n    FlaxMaskedLMOutput,\n    FlaxMultipleChoiceModelOutput,\n    FlaxQuestionAnsweringModelOutput,\n    FlaxSequenceClassifierOutput,\n    FlaxTokenClassifierOutput,\n)\nfrom transformers.modeling_flax_utils import (\n    ACT2FN, # 激活函数名称到实际函数的映射，如 \"gelu\" -> nn.gelu\n    FlaxPreTrainedModel,  # 所有Flax模型的基类，实现了权重加载/保存、配置管理等功能\n    append_call_sample_docstring,\n    append_replace_return_docstrings,\n    overwrite_call_docstring, # 用于自动生成文档字符串（docstring）的方法装饰器\n)\n # 所有输出类的基类 用于添加文档开头的说明字符串 Transformers 的日志工具\nfrom transformers.utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n# HuggingFace 中 ALBERT 模型的配置类，定义超参数（如层数、隐藏维度等）\nfrom transformers.models.albert.configuration_albert import AlbertConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T04:47:25.911549Z","iopub.execute_input":"2025-05-24T04:47:25.911851Z","iopub.status.idle":"2025-05-24T04:47:25.919695Z","shell.execute_reply.started":"2025-05-24T04:47:25.911821Z","shell.execute_reply":"2025-05-24T04:47:25.918150Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"logger = logging.get_logger(__name__) # 日志对象\n_CHECKPOINT_FOR_DOC = \"albert/albert-base-v2\"\n_CONFIG_FOR_DOC = \"AlbertConfig\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T04:47:25.925104Z","iopub.execute_input":"2025-05-24T04:47:25.925693Z","iopub.status.idle":"2025-05-24T04:47:25.955919Z","shell.execute_reply.started":"2025-05-24T04:47:25.925666Z","shell.execute_reply":"2025-05-24T04:47:25.954829Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 使用 flax.struct.dataclass 定义结构化输出，用于保存模型前向传播的多项结果\n@flax.struct.dataclass\nclass FlaxAlbertForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    用于 ALBERT 预训练任务（MLM + SOP）的模型输出结构。\n\n    设计意图：\n    - 统一模型输出格式，便于兼容 HuggingFace 的下游任务框架。\n    - 使用 `flax.struct.dataclass` 生成不可变对象，适合 JAX/Flax 的纯函数式设计。\n    - 继承 ModelOutput，允许按属性名或字典方式访问。\n\n    包含字段：\n    - prediction_logits：MLM 输出，表示词表上每个 token 的预测 logits。\n    - sop_logits：SOP 输出，判断两段文本是否连续。\n    - hidden_states：可选，返回所有层的中间表示（含 embedding）。\n    - attentions：可选，返回每层的注意力权重。\n    \"\"\"\n    # (batch_size, seq_len, vocab_size)，MLM 模型头的输出 logits，未经过 softmax\n    prediction_logits: jnp.ndarray = None\n    # (batch_size, 2)，SOP 任务的输出 logits，用于判断两个句子是否连续\n    sop_logits: jnp.ndarray = None\n    # 所有层的中间隐藏状态，包括 embedding 层，可选返回，用于调试或分析\n    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n    # 所有层的注意力权重（softmax 后），可选返回，用于可视化或解释模型行为\n    attentions: Optional[Tuple[jnp.ndarray]] = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T04:55:37.308237Z","iopub.execute_input":"2025-05-24T04:55:37.309088Z","iopub.status.idle":"2025-05-24T04:55:37.319336Z","shell.execute_reply.started":"2025-05-24T04:55:37.309048Z","shell.execute_reply":"2025-05-24T04:55:37.318024Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"ALBERT_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)\n\n    This model is also a\n    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as\n    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and\n    behavior.\n\n    Finally, this model supports inherent JAX features such as:\n\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n            `jax.numpy.bfloat16` (on TPUs).\n\n            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n            specified all the computation will be performed with the given `dtype`.\n\n            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n            parameters.**\n\n            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n            [`~FlaxPreTrainedModel.to_bf16`].\n\"\"\"\n\nALBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`numpy.ndarray` of shape `({0})`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`numpy.ndarray` of shape `({0})`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`numpy.ndarray` of shape `({0})`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n\n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n\n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`numpy.ndarray` of shape `({0})`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T04:56:11.206808Z","iopub.execute_input":"2025-05-24T04:56:11.207217Z","iopub.status.idle":"2025-05-24T04:56:11.215519Z","shell.execute_reply.started":"2025-05-24T04:56:11.207187Z","shell.execute_reply":"2025-05-24T04:56:11.214521Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 构建 ALBERT 的输入嵌入，包括词嵌入、位置嵌入、segment嵌入（token_type）\nclass FlaxAlbertEmbeddings(nn.Module):\n    \n    config: AlbertConfig # 模型配置类，定义嵌入维度、词表大小等\n    dtype: jnp.dtype = jnp.float32  # 设置计算精度\n\n    def setup(self):\n         # 词嵌入层：将 token id 映射到 embedding 向量\n        self.word_embeddings = nn.Embed(\n            self.config.vocab_size,  # 词表大小\n            self.config.embedding_size,  # 嵌入维度（注意：ALBERT 中此处小于 hidden_size）\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        )\n         # 位置嵌入层：为每个 token 提供位置信息\n        self.position_embeddings = nn.Embed(\n            self.config.max_position_embeddings,  # 最大支持的位置数\n            self.config.embedding_size,  # 同样使用 embedding_size\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        )\n        # token_type 嵌入层：通常用于区分句子 A/B（segment embedding）\n        self.token_type_embeddings = nn.Embed(\n            self.config.type_vocab_size,  # 通常为 2，表示句子对\n            self.config.embedding_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        )\n        # 使用 LayerNorm 正则化嵌入结果（提升收敛与稳定性）\n        # nn.LayerNorm 是一种无状态的归一化方法，它不像 BatchNorm 那样依赖均值与方差的滑动统计（moving averages）。\n        # LayerNorm 在训练和评估时行为完全一致，始终使用当前输入样本本身的均值和方差，不依赖也不保存任何全局统计量。\n        self.LayerNorm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n        # Dropout 作为正则化，防止过拟合\n        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n\n    def __call__(self, input_ids, token_type_ids, position_ids, deterministic: bool = True):\n        # 获取词嵌入（int32 类型以匹配 nn.Embed 的要求）\n        inputs_embeds = self.word_embeddings(input_ids.astype(\"i4\"))\n        # 获取位置嵌入\n        position_embeds = self.position_embeddings(position_ids.astype(\"i4\"))\n        # 获取 token_type（segment）嵌入\n        token_type_embeddings = self.token_type_embeddings(token_type_ids.astype(\"i4\"))\n\n        # 将三者逐元素相加，作为最终输入表示（符合 BERT/ALBERT 的输入设计）\n        hidden_states = inputs_embeds + token_type_embeddings + position_embeds\n\n        # 应用 LayerNorm 与 Dropout\n        hidden_states = self.LayerNorm(hidden_states)\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:08:05.121497Z","iopub.execute_input":"2025-05-24T05:08:05.121889Z","iopub.status.idle":"2025-05-24T05:08:05.133610Z","shell.execute_reply.started":"2025-05-24T05:08:05.121861Z","shell.execute_reply":"2025-05-24T05:08:05.132689Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# \"i4\" 是 JAX/NumPy 中的一种 数据类型标识符，表示：\n# int32，即 4 字节（32 位）有符号整数。\n# 为什么用 \"i4\"？\n# 在 JAX 中，nn.Embed 要求输入必须是整数索引类型，一般是 int32 或 int64。使用 \"i4\" 可以确保：\n# input_ids.astype(\"i4\")\n# 转换后的张量是 int32 类型，符合 nn.Embed 要求，避免类型报错。\n# 这段 FlaxAlbertEmbeddings 代码中，嵌入层、LayerNorm、Dropout 都是单设备、单块运行，未使用任何分片策略。\n# 如果需要实现分布式训练或分片，可以在模型设计或训练代码中添加相关分片约束和多设备映射","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_CHECKPOINT_FOR_DOC='albert-xxlarge-v2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:54:50.631236Z","iopub.execute_input":"2025-05-24T06:54:50.631542Z","iopub.status.idle":"2025-05-24T06:54:50.635803Z","shell.execute_reply.started":"2025-05-24T06:54:50.631523Z","shell.execute_reply":"2025-05-24T06:54:50.634705Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"config = AlbertConfig.from_pretrained(_CHECKPOINT_FOR_DOC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:54:52.923927Z","iopub.execute_input":"2025-05-24T06:54:52.924326Z","iopub.status.idle":"2025-05-24T06:54:53.154584Z","shell.execute_reply.started":"2025-05-24T06:54:52.924302Z","shell.execute_reply":"2025-05-24T06:54:53.153441Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:54:55.066071Z","iopub.execute_input":"2025-05-24T06:54:55.066811Z","iopub.status.idle":"2025-05-24T06:54:55.074991Z","shell.execute_reply.started":"2025-05-24T06:54:55.066785Z","shell.execute_reply":"2025-05-24T06:54:55.073978Z"}},"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"AlbertConfig {\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 16384,\n  \"layer_norm_eps\": 1e-12,\n  \"layers_to_keep\": [],\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 64,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}"},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"embeddings=FlaxAlbertEmbeddings(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:54:59.547001Z","iopub.execute_input":"2025-05-24T06:54:59.547306Z","iopub.status.idle":"2025-05-24T06:54:59.552388Z","shell.execute_reply.started":"2025-05-24T06:54:59.547284Z","shell.execute_reply":"2025-05-24T06:54:59.551257Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"from transformers import AlbertTokenizer\ntokenizer = AlbertTokenizer.from_pretrained(_CHECKPOINT_FOR_DOC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:02.665146Z","iopub.execute_input":"2025-05-24T06:55:02.665476Z","iopub.status.idle":"2025-05-24T06:55:03.093133Z","shell.execute_reply.started":"2025-05-24T06:55:02.665452Z","shell.execute_reply":"2025-05-24T06:55:03.092090Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"# 模拟输入\ntext = \"Deep learning changes the world\"\n# 编码文本，返回 NumPy 格式（Flax 使用）\nx = tokenizer(text, return_tensors=\"np\")\n# 构造 position_ids\nseq_length = x[\"input_ids\"].shape[1]\nx[\"position_ids\"] = np.arange(seq_length)[None, :]  # shape (1, seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:06.121656Z","iopub.execute_input":"2025-05-24T06:55:06.121972Z","iopub.status.idle":"2025-05-24T06:55:06.127620Z","shell.execute_reply.started":"2025-05-24T06:55:06.121947Z","shell.execute_reply":"2025-05-24T06:55:06.126724Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"x.pop('attention_mask')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:08.810657Z","iopub.execute_input":"2025-05-24T06:55:08.811002Z","iopub.status.idle":"2025-05-24T06:55:08.818668Z","shell.execute_reply.started":"2025-05-24T06:55:08.810978Z","shell.execute_reply":"2025-05-24T06:55:08.817461Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"array([[1, 1, 1, 1, 1, 1, 1]])"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:11.121465Z","iopub.execute_input":"2025-05-24T06:55:11.122069Z","iopub.status.idle":"2025-05-24T06:55:11.129726Z","shell.execute_reply.started":"2025-05-24T06:55:11.121914Z","shell.execute_reply":"2025-05-24T06:55:11.128689Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"{'input_ids': array([[   2,  855, 2477, 1693,   14,  126,    3]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]]), 'position_ids': array([[0, 1, 2, 3, 4, 5, 6]])}"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"variables = embeddings.init(jax.random.key(0),**x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:13.994610Z","iopub.execute_input":"2025-05-24T06:55:13.994956Z","iopub.status.idle":"2025-05-24T06:55:14.157783Z","shell.execute_reply.started":"2025-05-24T06:55:13.994932Z","shell.execute_reply":"2025-05-24T06:55:14.156757Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"jax.tree_util.tree_map(jnp.shape, variables)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:16.249641Z","iopub.execute_input":"2025-05-24T06:55:16.249969Z","iopub.status.idle":"2025-05-24T06:55:16.257334Z","shell.execute_reply.started":"2025-05-24T06:55:16.249937Z","shell.execute_reply":"2025-05-24T06:55:16.256349Z"}},"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"{'params': {'LayerNorm': {'bias': (128,), 'scale': (128,)},\n  'position_embeddings': {'embedding': (512, 128)},\n  'token_type_embeddings': {'embedding': (2, 128)},\n  'word_embeddings': {'embedding': (30000, 128)}}}"},"metadata":{}}],"execution_count":90},{"cell_type":"code","source":"params = variables['params']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:28:30.772365Z","iopub.execute_input":"2025-05-24T05:28:30.772675Z","iopub.status.idle":"2025-05-24T05:28:30.777832Z","shell.execute_reply.started":"2025-05-24T05:28:30.772655Z","shell.execute_reply":"2025-05-24T05:28:30.776810Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"config.hidden_dropout_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:34:40.262845Z","iopub.execute_input":"2025-05-24T05:34:40.263250Z","iopub.status.idle":"2025-05-24T05:34:40.270492Z","shell.execute_reply.started":"2025-05-24T05:34:40.263224Z","shell.execute_reply":"2025-05-24T05:34:40.269576Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"hidden_states = embeddings.apply(\n  {'params': params}, # 参数\n  **x, # 输入数据\n  deterministic=False, \n  rngs={'dropout': jax.random.key(seed=88)}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:33:02.787828Z","iopub.execute_input":"2025-05-24T05:33:02.788198Z","iopub.status.idle":"2025-05-24T05:33:02.808684Z","shell.execute_reply.started":"2025-05-24T05:33:02.788174Z","shell.execute_reply":"2025-05-24T05:33:02.807465Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"hidden_states.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:33:36.714524Z","iopub.execute_input":"2025-05-24T05:33:36.714805Z","iopub.status.idle":"2025-05-24T05:33:36.720499Z","shell.execute_reply.started":"2025-05-24T05:33:36.714787Z","shell.execute_reply":"2025-05-24T05:33:36.719704Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(1, 7, 128)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"config.layer_norm_eps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:43:09.328178Z","iopub.execute_input":"2025-05-24T05:43:09.329465Z","iopub.status.idle":"2025-05-24T05:43:09.337974Z","shell.execute_reply.started":"2025-05-24T05:43:09.329423Z","shell.execute_reply":"2025-05-24T05:43:09.335773Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"1e-12"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"jnp.finfo(jnp.float32).min","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:48:45.831870Z","iopub.execute_input":"2025-05-24T05:48:45.832260Z","iopub.status.idle":"2025-05-24T05:48:45.840760Z","shell.execute_reply.started":"2025-05-24T05:48:45.832234Z","shell.execute_reply":"2025-05-24T05:48:45.839611Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"-3.4028235e+38"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"class FlaxAlbertSelfAttention(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32  # 计算精度\n\n    def setup(self):\n        # 确保 hidden_size 能被 num_attention_heads 整除，便于多头注意力切分\n        if self.config.hidden_size % self.config.num_attention_heads != 0:\n            raise ValueError(\n                \"`config.hidden_size`: {self.config.hidden_size} has to be a multiple of `config.num_attention_heads` \"\n                \"                   : {self.config.num_attention_heads}\"\n            )\n        self.query = nn.Dense(  # query 映射层\n            self.config.hidden_size,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n        )\n        self.key = nn.Dense(  # key 映射层\n            self.config.hidden_size,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n        )\n        self.value = nn.Dense(  # value 映射层\n            self.config.hidden_size,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n        )\n        self.dense = nn.Dense( # 最后的输出映射层\n            self.config.hidden_size,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        # 残差连接后的 LayerNorm，稳定训练\n        self.LayerNorm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n        # Dropout 层，防止过拟合\n        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n\n    def __call__(self, hidden_states, attention_mask, deterministic=True, output_attentions: bool = False):\n        # 计算每个注意力头的维度\n        head_dim = self.config.hidden_size // self.config.num_attention_heads\n        # 线性变换获得 Query，Key，Value 向量，并 reshape 为多头格式\n        query_states = self.query(hidden_states).reshape(\n            hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim)\n        )\n        value_states = self.value(hidden_states).reshape(\n            hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim)\n        )\n        key_states = self.key(hidden_states).reshape(\n            hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim)\n        )\n\n        # attention_mask 转换为 bias，用于在softmax前遮蔽（mask）无效位置\n        if attention_mask is not None:\n            # attention mask in the form of attention bias\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2)) # 扩展维度匹配 attention 权重 shape\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype), # 有效位置为0\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype), # 无效位置为负无穷\n            )\n        else:\n            attention_bias = None\n        # attention_bias\n        # 先计算 query 和 key 的点积，得到注意力分数（logits）。\n        # 将 attention mask 转换成偏置（大负数）加到这些分数上，让无效位置分数变得极小。\n        # 对加了偏置的分数做 softmax，softmax 输出中无效位置的概率几乎为 0。\n        # 最后用这个概率加权 value。\n        # dropout 的随机数生成器，训练时激活，推理时关闭\n        dropout_rng = None\n        if not deterministic and self.config.attention_probs_dropout_prob > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        # 计算注意力权重（query 与 key 的点积注意力），并应用 dropout 和掩码\n        attn_weights = dot_product_attention_weights(\n            query_states, # [batch, seq_len, num_heads, head_dim] 经过线性变换的查询向量Q\n            key_states, # 经过线性变换的键向量K\n            bias=attention_bias, # 注意力偏置，将mask转换的大负数添加到对应位置，用于屏蔽无效token\n            dropout_rng=dropout_rng,  # dropout随机种子，仅在训练且dropout概率>0时使用\n            dropout_rate=self.config.attention_probs_dropout_prob,  # 注意力概率的dropout概率\n            broadcast_dropout=True, # 是否对dropout进行广播（在heads维度广播）\n            # 表示 Dropout 掩码在注意力 head 维度上进行广播，也就是：\n            # ✅ 意图说明：\n            # 所有注意力头（num_heads）共享同一个 dropout 掩码，\n            deterministic=deterministic, # 是否为推理模式，推理时关闭dropout\n            dtype=self.dtype,  # 计算数据类型（float32或float16等）\n            precision=None,  # JAX的计算精度参数，None表示使用默认精度 默认值为 bfloat16（在支持的硬件上）或 float32。\n        )\n        # dtype 是数据的类型（变量的存储格式）\n        # precision 是执行乘法运算时的数值精度控制参数\n        # 计算加权的 value 向量作为多头注意力的输出，使用 einsum 实现多头维度对应乘积求和\n        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states)\n        # 将多头维度合并回 hidden_size 维度\n        attn_output = attn_output.reshape(attn_output.shape[:2] + (-1,))\n        # 通过输出映射层并应用 dropout\n        projected_attn_output = self.dense(attn_output)\n        projected_attn_output = self.dropout(projected_attn_output, deterministic=deterministic)\n        # 残差连接 + LayerNorm，保证训练稳定性和梯度流动\n        layernormed_attn_output = self.LayerNorm(projected_attn_output + hidden_states)\n        # 根据是否需要返回 attention 权重，输出不同格式\n        outputs = (layernormed_attn_output, attn_weights) if output_attentions else (layernormed_attn_output,)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:08:01.503692Z","iopub.execute_input":"2025-05-24T06:08:01.504115Z","iopub.status.idle":"2025-05-24T06:08:01.524518Z","shell.execute_reply.started":"2025-05-24T06:08:01.504086Z","shell.execute_reply":"2025-05-24T06:08:01.522450Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"config.hidden_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:10:12.978145Z","iopub.execute_input":"2025-05-24T06:10:12.978550Z","iopub.status.idle":"2025-05-24T06:10:12.987976Z","shell.execute_reply.started":"2025-05-24T06:10:12.978519Z","shell.execute_reply":"2025-05-24T06:10:12.986561Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"4096"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# 2. 构造输入\nbatch_size = 2\nseq_length = 8\nhidden_states = jnp.ones((batch_size, seq_length, config.hidden_size), dtype=jnp.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:10:20.929647Z","iopub.execute_input":"2025-05-24T06:10:20.929975Z","iopub.status.idle":"2025-05-24T06:10:20.969181Z","shell.execute_reply.started":"2025-05-24T06:10:20.929954Z","shell.execute_reply":"2025-05-24T06:10:20.967699Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"hidden_states.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:10:31.863882Z","iopub.execute_input":"2025-05-24T06:10:31.864292Z","iopub.status.idle":"2025-05-24T06:10:31.871721Z","shell.execute_reply.started":"2025-05-24T06:10:31.864205Z","shell.execute_reply":"2025-05-24T06:10:31.870435Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(2, 8, 4096)"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"attention_mask = jnp.ones((batch_size, seq_length), dtype=jnp.int32)  # 无屏蔽，全部为1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:10:44.937624Z","iopub.execute_input":"2025-05-24T06:10:44.937960Z","iopub.status.idle":"2025-05-24T06:10:44.967768Z","shell.execute_reply.started":"2025-05-24T06:10:44.937929Z","shell.execute_reply":"2025-05-24T06:10:44.966883Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# 3. 构建模块实例\nattn_module = FlaxAlbertSelfAttention(config=config, dtype=jnp.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:11:08.604562Z","iopub.execute_input":"2025-05-24T06:11:08.604983Z","iopub.status.idle":"2025-05-24T06:11:08.611401Z","shell.execute_reply.started":"2025-05-24T06:11:08.604951Z","shell.execute_reply":"2025-05-24T06:11:08.609987Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# 4. 初始化权重\nkey = jax.random.PRNGKey(0) # rng key","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:11:36.075760Z","iopub.execute_input":"2025-05-24T06:11:36.076090Z","iopub.status.idle":"2025-05-24T06:11:36.081919Z","shell.execute_reply.started":"2025-05-24T06:11:36.076068Z","shell.execute_reply":"2025-05-24T06:11:36.081081Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"variables = attn_module.init(key, hidden_states, attention_mask, deterministic=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:12:25.132794Z","iopub.execute_input":"2025-05-24T06:12:25.133820Z","iopub.status.idle":"2025-05-24T06:12:27.561535Z","shell.execute_reply.started":"2025-05-24T06:12:25.133768Z","shell.execute_reply":"2025-05-24T06:12:27.560623Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"jax.tree.map(jnp.shape, variables)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:13:16.810138Z","iopub.execute_input":"2025-05-24T06:13:16.810517Z","iopub.status.idle":"2025-05-24T06:13:16.819570Z","shell.execute_reply.started":"2025-05-24T06:13:16.810490Z","shell.execute_reply":"2025-05-24T06:13:16.818444Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'params': {'LayerNorm': {'bias': (4096,), 'scale': (4096,)},\n  'dense': {'bias': (4096,), 'kernel': (4096, 4096)},\n  'key': {'bias': (4096,), 'kernel': (4096, 4096)},\n  'query': {'bias': (4096,), 'kernel': (4096, 4096)},\n  'value': {'bias': (4096,), 'kernel': (4096, 4096)}}}"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"# 5. 前向调用\noutputs = attn_module.apply(variables, hidden_states, attention_mask, deterministic=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:14:22.909111Z","iopub.execute_input":"2025-05-24T06:14:22.909482Z","iopub.status.idle":"2025-05-24T06:14:22.982053Z","shell.execute_reply.started":"2025-05-24T06:14:22.909455Z","shell.execute_reply":"2025-05-24T06:14:22.978128Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"print(type(outputs),len(outputs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:15:05.410601Z","iopub.execute_input":"2025-05-24T06:15:05.411032Z","iopub.status.idle":"2025-05-24T06:15:05.417847Z","shell.execute_reply.started":"2025-05-24T06:15:05.411001Z","shell.execute_reply":"2025-05-24T06:15:05.416862Z"}},"outputs":[{"name":"stdout","text":"<class 'tuple'> 1\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"attention_output = outputs[0]  # shape: (batch_size, seq_length, hidden_size)\n# 打印结果形状\nprint(\"attention_output.shape:\", attention_output.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:15:16.546524Z","iopub.execute_input":"2025-05-24T06:15:16.546927Z","iopub.status.idle":"2025-05-24T06:15:16.552592Z","shell.execute_reply.started":"2025-05-24T06:15:16.546880Z","shell.execute_reply":"2025-05-24T06:15:16.551595Z"}},"outputs":[{"name":"stdout","text":"attention_output.shape: (2, 8, 4096)\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"class FlaxAlbertLayer(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32  # 模块中所有计算的数据类型（如 float32、float16 等）\n\n    def setup(self):\n        # 构建自注意力子层\n        self.attention = FlaxAlbertSelfAttention(self.config, dtype=self.dtype)\n         # 前馈网络第一层（扩大维度）\n        self.ffn = nn.Dense(\n            self.config.intermediate_size, # 通常为 hidden_size 的 4 倍\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        self.activation = ACT2FN[self.config.hidden_act] # 激活函数（如 gelu/relu/silu，取决于 config.hidden_act）\n        self.ffn_output = nn.Dense( # 前馈网络第二层（降回 hidden_size）\n            self.config.hidden_size,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        # 层归一化（用于前馈网络残差连接后）\n        self.full_layer_layer_norm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob) # Dropout用于正则化\n\n    def __call__(\n        self,\n        hidden_states, # 输入张量 shape: (batch_size, seq_len, hidden_size)\n        attention_mask,  # 注意力掩码，用于屏蔽 padding 等无效位置\n        deterministic: bool = True,  # 控制 Dropout 是否启用\n        output_attentions: bool = False, # 是否返回注意力权重\n    ):\n        # === 1. 自注意力子层 ===\n        attention_outputs = self.attention(\n            hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions\n        )\n        attention_output = attention_outputs[0]  # shape: (batch_size, seq_len, hidden_size)\n        # === 2. 前馈网络子层 ===\n        ffn_output = self.ffn(attention_output)  # shape: (batch_size, seq_len, intermediate_size)\n        ffn_output = self.activation(ffn_output) # 应用非线性激活函数\n        ffn_output = self.ffn_output(ffn_output)  # shape: (batch_size, seq_len, hidden_size)\n        ffn_output = self.dropout(ffn_output, deterministic=deterministic) # Dropout\n        # === 3. 残差连接 + 层归一化 ===\n        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output)\n\n        outputs = (hidden_states,) # 返回更新后的隐藏状态\n         # 可选：返回注意力权重\n        if output_attentions:\n            outputs += (attention_outputs[1],)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:21:07.393399Z","iopub.execute_input":"2025-05-24T06:21:07.393802Z","iopub.status.idle":"2025-05-24T06:21:07.404743Z","shell.execute_reply.started":"2025-05-24T06:21:07.393776Z","shell.execute_reply":"2025-05-24T06:21:07.403651Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# 设计结构总结（标准 Transformer Block）：\n# Self-Attention 子层（带残差连接与隐含 LayerNorm）。\n# 前馈网络子层（两层线性 + 激活 + Dropout + 残差 + LayerNorm）。\n# 保持输出维度与输入一致（以便堆叠）。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attn_layer = FlaxAlbertLayer(config=config, dtype=jnp.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:22:11.254049Z","iopub.execute_input":"2025-05-24T06:22:11.254349Z","iopub.status.idle":"2025-05-24T06:22:11.258961Z","shell.execute_reply.started":"2025-05-24T06:22:11.254327Z","shell.execute_reply":"2025-05-24T06:22:11.258070Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"variables = attn_layer.init(key, hidden_states, attention_mask, deterministic=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:25:37.525871Z","iopub.execute_input":"2025-05-24T06:25:37.526211Z","iopub.status.idle":"2025-05-24T06:25:42.777018Z","shell.execute_reply.started":"2025-05-24T06:25:37.526179Z","shell.execute_reply":"2025-05-24T06:25:42.775669Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"jax.tree.map(jnp.shape, variables)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:25:44.210164Z","iopub.execute_input":"2025-05-24T06:25:44.210527Z","iopub.status.idle":"2025-05-24T06:25:44.217579Z","shell.execute_reply.started":"2025-05-24T06:25:44.210496Z","shell.execute_reply":"2025-05-24T06:25:44.216442Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"{'params': {'attention': {'LayerNorm': {'bias': (4096,), 'scale': (4096,)},\n   'dense': {'bias': (4096,), 'kernel': (4096, 4096)},\n   'key': {'bias': (4096,), 'kernel': (4096, 4096)},\n   'query': {'bias': (4096,), 'kernel': (4096, 4096)},\n   'value': {'bias': (4096,), 'kernel': (4096, 4096)}},\n  'ffn': {'bias': (16384,), 'kernel': (4096, 16384)},\n  'ffn_output': {'bias': (4096,), 'kernel': (16384, 4096)},\n  'full_layer_layer_norm': {'bias': (4096,), 'scale': (4096,)}}}"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"outputs = attn_layer.apply(variables,\n                            hidden_states,\n                            attention_mask, \n                            deterministic=False,\n                            rngs={'dropout': jax.random.key(seed=88)}\n                            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:26:13.852734Z","iopub.execute_input":"2025-05-24T06:26:13.854512Z","iopub.status.idle":"2025-05-24T06:26:13.970438Z","shell.execute_reply.started":"2025-05-24T06:26:13.854478Z","shell.execute_reply":"2025-05-24T06:26:13.965839Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"outputs[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:26:16.491449Z","iopub.execute_input":"2025-05-24T06:26:16.491745Z","iopub.status.idle":"2025-05-24T06:26:16.498433Z","shell.execute_reply.started":"2025-05-24T06:26:16.491726Z","shell.execute_reply":"2025-05-24T06:26:16.497267Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"(2, 8, 4096)"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"config.inner_group_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:33:56.702831Z","iopub.execute_input":"2025-05-24T06:33:56.703178Z","iopub.status.idle":"2025-05-24T06:33:56.709129Z","shell.execute_reply.started":"2025-05-24T06:33:56.703155Z","shell.execute_reply":"2025-05-24T06:33:56.708117Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"class FlaxAlbertLayerCollection(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32   # 模块计算所用数据类型\n\n    def setup(self):\n        # 构建 inner_group_num 个共享层（FlaxAlbertLayer 实例） 注意：ALBERT 特点是参数共享，所以多个层可以复用同一个子层对象\n        self.layers = [\n            FlaxAlbertLayer(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.inner_group_num)\n        ]\n\n    def __call__(\n        self,\n        hidden_states,  # 输入表示（batch_size, seq_len, hidden_size）\n        attention_mask,  # 注意力掩码，用于屏蔽 padding 或未来信息\n        deterministic: bool = True, # 是否为推理模式（控制 dropout）\n        output_attentions: bool = False,   # 是否输出注意力权重\n        output_hidden_states: bool = False, # 是否输出每层的 hidden states\n    ):\n        layer_hidden_states = () # 存储中间层的 hidden_states\n        layer_attentions = () # 存储中间层的注意力权重\n\n        for layer_index, albert_layer in enumerate(self.layers):\n             # 依次执行每一层 ALBERT Layer\n            layer_output = albert_layer(\n                hidden_states,\n                attention_mask,\n                deterministic=deterministic,\n                output_attentions=output_attentions,\n            )\n            hidden_states = layer_output[0] # 更新 hidden_states，作为下一层输入\n            # 可选：记录每层的注意力权重\n            if output_attentions:\n                layer_attentions = layer_attentions + (layer_output[1],)\n            # 可选：记录每层的 hidden state\n            if output_hidden_states:\n                layer_hidden_states = layer_hidden_states + (hidden_states,)\n        # === 构建最终输出 ===\n        outputs = (hidden_states,)  # 最后一层的输出\n        if output_hidden_states:\n            outputs = outputs + (layer_hidden_states,)  # 所有中间层 hidden states\n        if output_attentions:\n            outputs = outputs + (layer_attentions,)  # 所有中间层 attention\n        return outputs  # last-layer hidden state, (layer hidden states), (layer attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:37:59.104688Z","iopub.execute_input":"2025-05-24T06:37:59.105021Z","iopub.status.idle":"2025-05-24T06:37:59.116055Z","shell.execute_reply.started":"2025-05-24T06:37:59.104998Z","shell.execute_reply":"2025-05-24T06:37:59.114536Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# 对前面FlaxAlbertLayerCollection的封装调用\nclass FlaxAlbertLayerCollections(nn.Module):\n    config: AlbertConfig  # 模型配置，包含层数、隐藏维度等超参数\n    dtype: jnp.dtype = jnp.float32    # 指定计算时使用的数据类型（如 float32 / float16）\n    layer_index: Optional[str] = None # 层编号，可用于命名或调试（目前未使用）\n\n    def setup(self):\n         # 初始化 ALBERT 层集合，一个逻辑 block，内部包含 inner_group_num 个共享的 FlaxAlbertLayer\n        self.albert_layers = FlaxAlbertLayerCollection(self.config, dtype=self.dtype)\n\n    def __call__(\n        self,\n        hidden_states,  # 输入隐状态（batch_size, seq_length, hidden_size）\n        attention_mask,  # 注意力掩码（用于屏蔽无效位置）\n        deterministic: bool = True, # 控制 dropout 是否启用（True 表示推理模式）\n        output_attentions: bool = False,  # 是否返回注意力权重\n        output_hidden_states: bool = False,   # 是否返回每层的中间隐状态\n    ):\n        outputs = self.albert_layers( # 调用 ALBERT 层集合执行前向传播\n            hidden_states,\n            attention_mask,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n        return outputs  # 返回结构为：最后一层隐状态 + 可选的中间隐状态 + 可选的注意力权重","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:02:25.185042Z","iopub.execute_input":"2025-05-24T07:02:25.185571Z","iopub.status.idle":"2025-05-24T07:02:25.195599Z","shell.execute_reply.started":"2025-05-24T07:02:25.185538Z","shell.execute_reply":"2025-05-24T07:02:25.194540Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"config.num_hidden_groups","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:55:39.223519Z","iopub.execute_input":"2025-05-24T06:55:39.223821Z","iopub.status.idle":"2025-05-24T06:55:39.230209Z","shell.execute_reply.started":"2025-05-24T06:55:39.223798Z","shell.execute_reply":"2025-05-24T06:55:39.229348Z"}},"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"class FlaxAlbertLayerGroups(nn.Module):\n    config: AlbertConfig  # 模型配置，包含总层数、组数等\n    dtype: jnp.dtype = jnp.float32  # 计算时使用的数据类型（如 float32、float16）\n\n    def setup(self):\n        # 构建多个 layer group，每个 group 是 FlaxAlbertLayerCollections 实例\n        # ALBERT 采用参数共享机制，多个 transformer 层共享 group 内的子层权重\n        self.layers = [\n            FlaxAlbertLayerCollections(self.config, \n                                       name=str(i),   # 用于模块命名\n                                       layer_index=str(i), # 可选的调试信息\n                                       dtype=self.dtype)  # 控制计算精度\n            for i in range(self.config.num_hidden_groups)\n        ]\n\n    def __call__(\n        self,\n        hidden_states,   # 输入张量，形状 [batch, seq_len, hidden_dim]\n        attention_mask,   # 注意力 mask，用于屏蔽无效位置\n        deterministic: bool = True,  # 是否关闭 dropout，推理时应为 True\n        output_attentions: bool = False,  # 是否输出每层的注意力权重\n        output_hidden_states: bool = False, # 是否输出每层的隐藏状态\n        return_dict: bool = True,    # 是否使用字典形式返回\n    ):\n        all_attentions = () if output_attentions else None  # 保存所有层的注意力输出（可选）\n        all_hidden_states = (hidden_states,) if output_hidden_states else None  # 保存每层隐藏状态（可选）\n         # 遍历所有 hidden layer，注意每 N 层复用一个 group\n        for i in range(self.config.num_hidden_layers):\n            # 计算当前层使用的共享 group 索引\n            group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n            # 调用对应 group 执行 forward（会运行 inner_group_num 次共享层）\n            layer_group_output = self.layers[group_idx](\n                hidden_states,\n                attention_mask,\n                deterministic=deterministic,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n            # 更新当前隐藏状态\n            hidden_states = layer_group_output[0]\n\n            if output_attentions:  # 收集注意力输出\n                all_attentions = all_attentions + layer_group_output[-1]\n\n            if output_hidden_states: # 收集隐藏状态输出\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict: # 返回结果\n            # 返回元组形式 (last_hidden_state, all_hidden_states, all_attentions)\n            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n        # 返回标准字典形式的结构体\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,  # 最后一层的输出\n            hidden_states=all_hidden_states,# 所有隐藏层输出（可选）\n            attentions=all_attentions   # 所有注意力输出（可选）\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:02:29.628535Z","iopub.execute_input":"2025-05-24T07:02:29.628860Z","iopub.status.idle":"2025-05-24T07:02:29.640886Z","shell.execute_reply.started":"2025-05-24T07:02:29.628826Z","shell.execute_reply":"2025-05-24T07:02:29.639683Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"layerGroups=FlaxAlbertLayerGroups(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:02:32.434015Z","iopub.execute_input":"2025-05-24T07:02:32.434425Z","iopub.status.idle":"2025-05-24T07:02:32.440537Z","shell.execute_reply.started":"2025-05-24T07:02:32.434398Z","shell.execute_reply":"2025-05-24T07:02:32.438859Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"variables = layerGroups.init(key, hidden_states, attention_mask, deterministic=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:02:32.742675Z","iopub.execute_input":"2025-05-24T07:02:32.743079Z","iopub.status.idle":"2025-05-24T07:02:38.351532Z","shell.execute_reply.started":"2025-05-24T07:02:32.743053Z","shell.execute_reply":"2025-05-24T07:02:38.348382Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"jax.tree.map(jnp.shape, variables)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:03:01.998876Z","iopub.execute_input":"2025-05-24T07:03:01.999243Z","iopub.status.idle":"2025-05-24T07:03:02.006539Z","shell.execute_reply.started":"2025-05-24T07:03:01.999215Z","shell.execute_reply":"2025-05-24T07:03:02.005538Z"}},"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"{'params': {'0': {'albert_layers': {'0': {'attention': {'LayerNorm': {'bias': (4096,),\n       'scale': (4096,)},\n      'dense': {'bias': (4096,), 'kernel': (4096, 4096)},\n      'key': {'bias': (4096,), 'kernel': (4096, 4096)},\n      'query': {'bias': (4096,), 'kernel': (4096, 4096)},\n      'value': {'bias': (4096,), 'kernel': (4096, 4096)}},\n     'ffn': {'bias': (16384,), 'kernel': (4096, 16384)},\n     'ffn_output': {'bias': (4096,), 'kernel': (16384, 4096)},\n     'full_layer_layer_norm': {'bias': (4096,), 'scale': (4096,)}}}}}}"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"outputs = layerGroups.apply(variables,\n                            hidden_states,\n                            attention_mask, \n                            deterministic=True\n                            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:04:18.999968Z","iopub.execute_input":"2025-05-24T07:04:19.000301Z","iopub.status.idle":"2025-05-24T07:04:20.466770Z","shell.execute_reply.started":"2025-05-24T07:04:19.000279Z","shell.execute_reply":"2025-05-24T07:04:20.465830Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"outputs.last_hidden_state.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:04:50.202700Z","iopub.execute_input":"2025-05-24T07:04:50.203038Z","iopub.status.idle":"2025-05-24T07:04:50.209097Z","shell.execute_reply.started":"2025-05-24T07:04:50.203014Z","shell.execute_reply":"2025-05-24T07:04:50.207934Z"}},"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"(2, 8, 4096)"},"metadata":{}}],"execution_count":103},{"cell_type":"code","source":"class FlaxAlbertEncoder(nn.Module):\n    config: AlbertConfig # 模型配置，包含hidden_size、层数、注意力头数等超参数\n    dtype: jnp.dtype = jnp.float32  # 指定计算所用的数据类型（如 float32 或 float16）\n\n    def setup(self):\n        # 将嵌入层输出的维度（embedding_size）映射到 hidden_size\n        # ALBERT 的 embedding_size 通常小于 hidden_size，通过这个全连接层做线性映射\n        self.embedding_hidden_mapping_in = nn.Dense(\n            self.config.hidden_size,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range), # 初始化方式：正态分布\n            dtype=self.dtype,\n        )\n        # 构建 ALBERT 的多个层组（LayerGroups），每组内部可能共享参数\n        # 每组由若干重复层（FlaxAlbertLayerCollections）组成\n        self.albert_layer_groups = FlaxAlbertLayerGroups(self.config, dtype=self.dtype)\n\n    def __call__(\n        self,\n        hidden_states,  # 输入 token 的嵌入表示（embedding 输出）\n        attention_mask,  # 注意力掩码，用于屏蔽 padding 等无效位置\n        deterministic: bool = True, # 是否处于推理模式（影响 dropout 等行为）\n        output_attentions: bool = False,  # 是否输出每层的注意力权重\n        output_hidden_states: bool = False,  # 是否输出每层的中间表示\n        return_dict: bool = True,  # 是否返回字典结构输出\n    ):\n        # 第一步：将 embedding 投影到 hidden_size，以供后续层使用\n        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n        # 第二步：将 hidden_states 和 attention_mask 传入层组中处理，得到最终输出\n        return self.albert_layer_groups(\n            hidden_states,\n            attention_mask,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:08:22.556691Z","iopub.execute_input":"2025-05-24T07:08:22.557091Z","iopub.status.idle":"2025-05-24T07:08:22.567152Z","shell.execute_reply.started":"2025-05-24T07:08:22.557065Z","shell.execute_reply":"2025-05-24T07:08:22.565939Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"text = \"Deep learning changes the world\"\n# 编码文本，返回 NumPy 格式（Flax 使用）\nxx = tokenizer(text, return_tensors=\"np\")\n# 构造 position_ids\nseq_length = xx[\"input_ids\"].shape[1]\nxx[\"position_ids\"] = np.arange(seq_length)[None, :] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:15:51.426680Z","iopub.execute_input":"2025-05-24T07:15:51.427010Z","iopub.status.idle":"2025-05-24T07:15:51.433093Z","shell.execute_reply.started":"2025-05-24T07:15:51.426986Z","shell.execute_reply":"2025-05-24T07:15:51.432201Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"xx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:15:53.840063Z","iopub.execute_input":"2025-05-24T07:15:53.840340Z","iopub.status.idle":"2025-05-24T07:15:53.846971Z","shell.execute_reply.started":"2025-05-24T07:15:53.840322Z","shell.execute_reply":"2025-05-24T07:15:53.846104Z"}},"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"{'input_ids': array([[   2,  855, 2477, 1693,   14,  126,    3]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]]), 'position_ids': array([[0, 1, 2, 3, 4, 5, 6]])}"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"attention_mask=xx.pop('attention_mask')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:15:56.166614Z","iopub.execute_input":"2025-05-24T07:15:56.166941Z","iopub.status.idle":"2025-05-24T07:15:56.171762Z","shell.execute_reply.started":"2025-05-24T07:15:56.166885Z","shell.execute_reply":"2025-05-24T07:15:56.170937Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"embeddings=FlaxAlbertEmbeddings(config)\nvariables = embeddings.init(jax.random.key(0),**xx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:16:08.385012Z","iopub.execute_input":"2025-05-24T07:16:08.385314Z","iopub.status.idle":"2025-05-24T07:16:08.534081Z","shell.execute_reply.started":"2025-05-24T07:16:08.385295Z","shell.execute_reply":"2025-05-24T07:16:08.533245Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"hidden_states = embeddings.apply(\n  variables, # 参数\n  **xx, # 输入数据\n  deterministic=False, \n  rngs={'dropout': jax.random.key(seed=88)}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:17:09.542229Z","iopub.execute_input":"2025-05-24T07:17:09.542561Z","iopub.status.idle":"2025-05-24T07:17:09.563061Z","shell.execute_reply.started":"2025-05-24T07:17:09.542537Z","shell.execute_reply":"2025-05-24T07:17:09.562104Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"encoder=FlaxAlbertEncoder(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:17:14.647001Z","iopub.execute_input":"2025-05-24T07:17:14.647393Z","iopub.status.idle":"2025-05-24T07:17:14.652132Z","shell.execute_reply.started":"2025-05-24T07:17:14.647367Z","shell.execute_reply":"2025-05-24T07:17:14.651114Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"variables = encoder.init(key, hidden_states, attention_mask, deterministic=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:17:27.277575Z","iopub.execute_input":"2025-05-24T07:17:27.277844Z","iopub.status.idle":"2025-05-24T07:17:33.301713Z","shell.execute_reply.started":"2025-05-24T07:17:27.277825Z","shell.execute_reply":"2025-05-24T07:17:33.298138Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"jax.tree.map(jnp.shape, variables)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:17:47.680564Z","iopub.execute_input":"2025-05-24T07:17:47.680870Z","iopub.status.idle":"2025-05-24T07:17:47.688054Z","shell.execute_reply.started":"2025-05-24T07:17:47.680847Z","shell.execute_reply":"2025-05-24T07:17:47.687166Z"}},"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"{'params': {'albert_layer_groups': {'0': {'albert_layers': {'0': {'attention': {'LayerNorm': {'bias': (4096,),\n        'scale': (4096,)},\n       'dense': {'bias': (4096,), 'kernel': (4096, 4096)},\n       'key': {'bias': (4096,), 'kernel': (4096, 4096)},\n       'query': {'bias': (4096,), 'kernel': (4096, 4096)},\n       'value': {'bias': (4096,), 'kernel': (4096, 4096)}},\n      'ffn': {'bias': (16384,), 'kernel': (4096, 16384)},\n      'ffn_output': {'bias': (4096,), 'kernel': (16384, 4096)},\n      'full_layer_layer_norm': {'bias': (4096,), 'scale': (4096,)}}}}},\n  'embedding_hidden_mapping_in': {'bias': (4096,), 'kernel': (128, 4096)}}}"},"metadata":{}}],"execution_count":127},{"cell_type":"code","source":"outputs = encoder.apply(variables,\n                            hidden_states,\n                            attention_mask, \n                            deterministic=True\n                            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:18:27.249708Z","iopub.execute_input":"2025-05-24T07:18:27.250027Z","iopub.status.idle":"2025-05-24T07:18:28.512170Z","shell.execute_reply.started":"2025-05-24T07:18:27.250003Z","shell.execute_reply":"2025-05-24T07:18:28.510936Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"print(type(outputs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:18:42.544092Z","iopub.execute_input":"2025-05-24T07:18:42.544439Z","iopub.status.idle":"2025-05-24T07:18:42.549297Z","shell.execute_reply.started":"2025-05-24T07:18:42.544415Z","shell.execute_reply":"2025-05-24T07:18:42.548397Z"}},"outputs":[{"name":"stdout","text":"<class 'transformers.modeling_flax_outputs.FlaxBaseModelOutput'>\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"outputs.last_hidden_state.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:19:06.049038Z","iopub.execute_input":"2025-05-24T07:19:06.050096Z","iopub.status.idle":"2025-05-24T07:19:06.055289Z","shell.execute_reply.started":"2025-05-24T07:19:06.050067Z","shell.execute_reply":"2025-05-24T07:19:06.054247Z"}},"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"(1, 7, 4096)"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"class FlaxAlbertOnlyMLMHead(nn.Module):\n    config: AlbertConfig # 配置对象\n    dtype: jnp.dtype = jnp.float32  # 计算所使用的数据类型\n    bias_init: Callable[..., np.ndarray] = jax.nn.initializers.zeros # 输出层 bias 的初始化方法\n\n    def setup(self):\n        # 第1层：将 hidden_size 投影到 embedding_size\n        self.dense = nn.Dense(self.config.embedding_size, dtype=self.dtype)\n        # 激活函数（如 gelu、relu 等），从配置中指定\n        self.activation = ACT2FN[self.config.hidden_act]\n          # 层归一化，保持数值稳定性\n        self.LayerNorm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n         # 解码层：将 embedding_size 映射到 vocab_size，用于生成预测分布（注意：不带 bias）\n        self.decoder = nn.Dense(self.config.vocab_size, dtype=self.dtype, use_bias=False)\n         # 显式定义 bias 参数，作为 decoder 的偏置项加到输出上\n        self.bias = self.param(\"bias\", self.bias_init, (self.config.vocab_size,))\n\n    def __call__(self, hidden_states, shared_embedding=None):\n        # 输入为 Transformer 的输出 hidden_states（shape: [batch, seq_len, hidden_size]）\n        # Step 1: 投影至 embedding_size（与原始 embedding 层一致，便于参数共享）\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.activation(hidden_states) # Step 2: 应用非线性激活函数\n        hidden_states = self.LayerNorm(hidden_states) # Step 3: 应用层归一化，提升训练稳定性\n        \n        # Step 4: 使用词向量矩阵转置作为权重共享（若提供），否则使用默认 decoder 权重\n        if shared_embedding is not None:\n            # 共享 embedding 权重：decoder 的权重 = embedding 的转置\n            hidden_states = self.decoder.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n        else: # 使用 decoder 自身的权重\n            hidden_states = self.decoder(hidden_states)\n        # Step 5: 加上 bias，得到最终对每个 token 的 logits\n        hidden_states += self.bias\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:45:54.020668Z","iopub.execute_input":"2025-05-24T08:45:54.021076Z","iopub.status.idle":"2025-05-24T08:45:54.034848Z","shell.execute_reply.started":"2025-05-24T08:45:54.021049Z","shell.execute_reply":"2025-05-24T08:45:54.033881Z"}},"outputs":[],"execution_count":178},{"cell_type":"code","source":"# 此模块对应 ALBERT 的 MLM（Masked Language Modeling）输出头。\n# 包含一个非线性前馈层 + LayerNorm + decoder 层（线性层），用于从 Transformer 的输出生成词表维度的 logits。\n# 如果提供 shared_embedding（来自嵌入层），则共享权重以节省参数（与 ALBERT/BERT 论文一致）。\n# 手动加上 bias 而非交由 nn.Dense 控制，是为了与 huggingface 兼容并支持权重共享的灵活性","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlaxAlbertSOPHead(nn.Module):\n    config: AlbertConfig # 配置对象\n    dtype: jnp.dtype = jnp.float32  # 指定计算精度（如 float32、float16）\n\n    def setup(self):\n        # Dropout 层，用于在训练时进行正则化，防止过拟合\n        self.dropout = nn.Dropout(self.config.classifier_dropout_prob)\n        # 分类器：将 pooled_output 映射为两个类别（用于句子顺序预测）\n        self.classifier = nn.Dense(2, dtype=self.dtype)\n\n    def __call__(self, pooled_output, deterministic=True):\n        # pooled_output 是从 Transformer 的 cls token 或平均池化得到的句子级表示\n        # 应用 dropout，训练时启用，推理时关闭（由 deterministic 控制）\n        pooled_output = self.dropout(pooled_output, deterministic=deterministic)\n        logits = self.classifier(pooled_output)  # 分类器生成 logits，用于二分类（句子是否是顺序）\n        return logits  # shape: [batch_size, 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:46:16.326944Z","iopub.execute_input":"2025-05-24T08:46:16.327452Z","iopub.status.idle":"2025-05-24T08:46:16.336200Z","shell.execute_reply.started":"2025-05-24T08:46:16.327413Z","shell.execute_reply":"2025-05-24T08:46:16.335111Z"}},"outputs":[],"execution_count":179},{"cell_type":"code","source":"# 模块设计意图说明：\n# 本模块用于 SOP (Sentence Order Prediction) 任务，这是 ALBERT 替代 NSP 的预训练目标。\n# 输入为整个序列的聚合表示（通常是 [CLS] token 的输出），输出为 [batch_size, 2] 的 logits。\n# 包括一个 Dropout 层用于训练时正则化，以及一个 Dense 层进行二分类预测。\n# 模块结构简洁，符合 ALBERT 论文中的 SOP head 架构设计","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids=jnp.zeros((2,8), dtype=\"i4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:44:59.320314Z","iopub.execute_input":"2025-05-24T07:44:59.320720Z","iopub.status.idle":"2025-05-24T07:44:59.326417Z","shell.execute_reply.started":"2025-05-24T07:44:59.320694Z","shell.execute_reply":"2025-05-24T07:44:59.325520Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"jnp.atleast_2d(input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:45:07.421492Z","iopub.execute_input":"2025-05-24T07:45:07.421814Z","iopub.status.idle":"2025-05-24T07:45:07.448169Z","shell.execute_reply.started":"2025-05-24T07:45:07.421792Z","shell.execute_reply":"2025-05-24T07:45:07.447205Z"}},"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"Array([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"},"metadata":{}}],"execution_count":140},{"cell_type":"code","source":"jnp.arange(jnp.atleast_2d(input_ids).shape[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:45:26.670284Z","iopub.execute_input":"2025-05-24T07:45:26.670981Z","iopub.status.idle":"2025-05-24T07:45:26.701294Z","shell.execute_reply.started":"2025-05-24T07:45:26.670952Z","shell.execute_reply":"2025-05-24T07:45:26.700353Z"}},"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"Array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32)"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]),(2,8))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:46:09.419761Z","iopub.execute_input":"2025-05-24T07:46:09.420738Z","iopub.status.idle":"2025-05-24T07:46:09.454292Z","shell.execute_reply.started":"2025-05-24T07:46:09.420699Z","shell.execute_reply":"2025-05-24T07:46:09.453459Z"}},"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"Array([[0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7]], dtype=int32)"},"metadata":{}}],"execution_count":142},{"cell_type":"code","source":"# 抽象基类：用于初始化权重、下载和加载预训练模型，提供标准接口。\n# 所有具体 ALBERT 模型都应继承自此类。\nclass FlaxAlbertPreTrainedModel(FlaxPreTrainedModel):\n    \n    config_class = AlbertConfig # 指定配置类\n    base_model_prefix = \"albert\" # 用于自动加载权重时的 key 前缀\n    module_class: nn.Module = None # 子类需指定：ALBERT 具体模型模块类\n\n    def __init__(\n        self,\n        config: AlbertConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        # 初始化模块实例（如 FlaxAlbertModule），由 module_class 提供\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        # 继承父类初始化流程，构建 self.module、self.params 等\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n        \n    # 初始化模型参数，可指定已有参数（用于缺失参数补齐）。\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # 构造输入张量用于触发模块初始化\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        token_type_ids = jnp.zeros_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        attention_mask = jnp.ones_like(input_ids)\n        # 分割随机数生成器，用于初始化参数和 dropout\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        # 调用 Flax 模块的 init 方法生成参数\n        random_params = self.module.init(\n            rngs, input_ids, attention_mask, token_type_ids, position_ids, return_dict=False\n        )[\"params\"]\n        # 若提供了部分已有参数，则合并已有参数与随机初始化结果（补齐缺失）\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params)) # 随机初始化参数\n            params = flatten_dict(unfreeze(params)) \n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n    # 模型推理或训练调用接口，统一处理各种输入参数，并调用内部 Flax module。\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        params: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        # 根据 config 或显式传参，决定是否输出中间层与注意力\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        # 若 token_type、position_ids、attention_mask 未提供，则自动补全默认值\n        if token_type_ids is None:\n            token_type_ids = jnp.zeros_like(input_ids)\n\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n\n        # 如果提供了 dropout 的 RNG，则传入 apply 函数中\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n        # 调用 Flax module 的 apply 方法执行推理（或训练），传入权重与输入\n        return self.module.apply(\n            {\"params\": params or self.params},\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(token_type_ids, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,  # deterministic = not train\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:48:54.523830Z","iopub.execute_input":"2025-05-24T07:48:54.524409Z","iopub.status.idle":"2025-05-24T07:48:54.541754Z","shell.execute_reply.started":"2025-05-24T07:48:54.524383Z","shell.execute_reply":"2025-05-24T07:48:54.540579Z"}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"class FlaxAlbertModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32  # 模型计算精度\n    add_pooling_layer: bool = True # 是否添加池化层用于句向量表示\n\n    def setup(self):\n        # 构建嵌入层：词嵌入 + 位置嵌入 + token_type 嵌入\n        self.embeddings = FlaxAlbertEmbeddings(self.config, dtype=self.dtype)\n        # 构建Transformer编码器（基于参数共享的ALBERT Encoder）\n        self.encoder = FlaxAlbertEncoder(self.config, dtype=self.dtype)\n        if self.add_pooling_layer:\n             # 构建池化层，用于提取句子级别表示（对应CLS token）\n            self.pooler = nn.Dense(\n                self.config.hidden_size,  # 输出维度仍为hidden_size\n                kernel_init=jax.nn.initializers.normal(self.config.initializer_range),  # 初始化方式\n                dtype=self.dtype,\n                name=\"pooler\",\n            )\n            self.pooler_activation = nn.tanh # 使用tanh激活，遵循BERT样式\n        else:\n            self.pooler = None\n            self.pooler_activation = None\n\n    def __call__(\n        self,\n        input_ids,  # 输入token id序列 [batch, seq]\n        attention_mask, # 注意力mask，1表示有效token，0表示padding\n        token_type_ids: Optional[np.ndarray] = None,  # 句子类型id（如句子对任务中用于区分A/B句）\n        position_ids: Optional[np.ndarray] = None,  # 位置id，若未传入则自动构造\n        deterministic: bool = True,  # 是否处于inference模式，控制dropout等行为\n        output_attentions: bool = False,  # 是否输出所有注意力权重\n        output_hidden_states: bool = False,\n        return_dict: bool = True,   # 是否以字典方式返回结果\n    ):\n         # 如果未提供 token_type_ids，默认为0（即全为第一句话）\n        if token_type_ids is None:\n            token_type_ids = jnp.zeros_like(input_ids)\n\n        # 如果未提供 position_ids，自动按序构建 [0, 1, ..., seq_len-1]\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n        # 经过 embedding 层：得到初始 hidden_states\n        hidden_states = self.embeddings(input_ids, token_type_ids, position_ids, deterministic=deterministic)\n         # 输入 encoder 进行多层Transformer处理，输出结构中hidden_states位于outputs[0]\n        outputs = self.encoder(\n            hidden_states,\n            attention_mask,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0] # 获取最后一层的输出\n        if self.add_pooling_layer:\n            # 池化层：提取CLS位置的向量作为整体句向量\n            pooled = self.pooler(hidden_states[:, 0])  # [batch, hidden_size]\n            pooled = self.pooler_activation(pooled)  # 应用 tanh 激活\n        else:\n            pooled = None\n\n        if not return_dict:\n             # 若不要求返回字典，则返回tuple格式\n            if pooled is None:\n                return (hidden_states,) + outputs[1:]\n            return (hidden_states, pooled) + outputs[1:]\n         # 返回标准结构体格式，包含所有输出项\n        return FlaxBaseModelOutputWithPooling(\n            last_hidden_state=hidden_states,\n            pooler_output=pooled,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:54:42.052296Z","iopub.execute_input":"2025-05-24T07:54:42.052620Z","iopub.status.idle":"2025-05-24T07:54:42.066613Z","shell.execute_reply.started":"2025-05-24T07:54:42.052596Z","shell.execute_reply":"2025-05-24T07:54:42.065427Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"FlaxAlbertPreTrainedModel.module_class = FlaxAlbertModule  # 设置具体的 Flax 模块类\nmodel = FlaxAlbertPreTrainedModel(config=config, input_shape=(1, 16))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:55:16.617665Z","iopub.execute_input":"2025-05-24T07:55:16.618211Z","iopub.status.idle":"2025-05-24T07:55:23.704032Z","shell.execute_reply.started":"2025-05-24T07:55:16.618187Z","shell.execute_reply":"2025-05-24T07:55:23.699820Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"# # Step 2: 初始化参数\nrng = jax.random.PRNGKey(0)\nparams = model.init_weights(rng, input_shape=(1, 16))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:11:33.866539Z","iopub.execute_input":"2025-05-24T08:11:33.867150Z","iopub.status.idle":"2025-05-24T08:11:39.550770Z","shell.execute_reply.started":"2025-05-24T08:11:33.867110Z","shell.execute_reply":"2025-05-24T08:11:39.549635Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"jax.tree.map(jnp.shape,params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:12:18.877987Z","iopub.execute_input":"2025-05-24T08:12:18.878309Z","iopub.status.idle":"2025-05-24T08:12:18.886506Z","shell.execute_reply.started":"2025-05-24T08:12:18.878286Z","shell.execute_reply":"2025-05-24T08:12:18.885673Z"}},"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"{'embeddings': {'LayerNorm': {'bias': (128,), 'scale': (128,)},\n  'position_embeddings': {'embedding': (512, 128)},\n  'token_type_embeddings': {'embedding': (2, 128)},\n  'word_embeddings': {'embedding': (30000, 128)}},\n 'encoder': {'albert_layer_groups': {'0': {'albert_layers': {'0': {'attention': {'LayerNorm': {'bias': (4096,),\n        'scale': (4096,)},\n       'dense': {'bias': (4096,), 'kernel': (4096, 4096)},\n       'key': {'bias': (4096,), 'kernel': (4096, 4096)},\n       'query': {'bias': (4096,), 'kernel': (4096, 4096)},\n       'value': {'bias': (4096,), 'kernel': (4096, 4096)}},\n      'ffn': {'bias': (16384,), 'kernel': (4096, 16384)},\n      'ffn_output': {'bias': (4096,), 'kernel': (16384, 4096)},\n      'full_layer_layer_norm': {'bias': (4096,), 'scale': (4096,)}}}}},\n  'embedding_hidden_mapping_in': {'bias': (4096,), 'kernel': (128, 4096)}},\n 'pooler': {'bias': (4096,), 'kernel': (4096, 4096)}}"},"metadata":{}}],"execution_count":151},{"cell_type":"code","source":"# # Step 3: 构造输入（示例：batch_size=1, seq_len=16）\ninput_ids = jnp.ones((1, 16), dtype=jnp.int32)\n# # Step 4: 执行前向推理\noutputs = model(\n    input_ids=input_ids,\n    params=params,\n    train=False,  # 推理模式\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:14:41.743800Z","iopub.execute_input":"2025-05-24T08:14:41.744125Z","iopub.status.idle":"2025-05-24T08:14:43.322955Z","shell.execute_reply.started":"2025-05-24T08:14:41.744105Z","shell.execute_reply":"2025-05-24T08:14:43.322007Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"# 如果 return_dict=True，则 outputs 是一个字典\nprint(outputs.last_hidden_state.shape,outputs.pooler_output.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:15:58.579968Z","iopub.execute_input":"2025-05-24T08:15:58.580291Z","iopub.status.idle":"2025-05-24T08:15:58.585311Z","shell.execute_reply.started":"2025-05-24T08:15:58.580266Z","shell.execute_reply":"2025-05-24T08:15:58.584192Z"}},"outputs":[{"name":"stdout","text":"(1, 16, 4096) (1, 4096)\n","output_type":"stream"}],"execution_count":156},{"cell_type":"code","source":"# @add_start_docstrings 它的作用是：将两个 docstring 片段拼接起来，作为 FlaxAlbertModel 类的最终文档字符串（__doc__）。\n@add_start_docstrings(\n    \"The bare Albert Model transformer outputting raw hidden-states without any specific head on top.\",\n    ALBERT_START_DOCSTRING,\n)\nclass FlaxAlbertModel(FlaxAlbertPreTrainedModel):\n    module_class = FlaxAlbertModule # 子类需指定：ALBERT 具体模型模块类","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:25:14.178833Z","iopub.execute_input":"2025-05-24T08:25:14.179166Z","iopub.status.idle":"2025-05-24T08:25:14.184038Z","shell.execute_reply.started":"2025-05-24T08:25:14.179142Z","shell.execute_reply":"2025-05-24T08:25:14.183151Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"help(FlaxAlbertModel)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 文档字符串（__doc__）在以下几种场景中非常有用：\n# 1. 使用 help() 或交互式查看 API 时\n# 在 Python shell 或 Jupyter Notebook 中输入：\n# help(FlaxAlbertModel)\n# 或\n# FlaxAlbertModel?\n# 你会看到 @add_start_docstrings 拼接后的完整文档，便于快速了解模型用途、输入输出、参数结构等。\n# ✅ 2. 自动文档生成工具\n# 如 Sphinx + autodoc 扩展会自动抓取类的 __doc__：\n# .. autoclass:: transformers.FlaxAlbertModel\n#    :members:\n# 文档网站（如 HuggingFace 文档）就是用这种方式生成的，add_start_docstrings 提高了文档一致性和可维护性。\n# ✅ 3. IDE 提示与补全\n# 在 PyCharm、VS Code 中悬浮鼠标或查看 docstring，可直接看到这段描述，快速理解该类或方法：\n# model = FlaxAlbertModel(config)\n# 悬停 FlaxAlbertModel 时会看到完整 docstring\n# ✅ 4. 开源时为他人/未来自己阅读源码服务\n# 例如你阅读 Transformers 源码，看到每个模型头（如 FlaxAlbertModel, FlaxAlbertForSequenceClassification）\n# 都有详细 docstring，可以快速了解用途，不必一行行查代码。\n# ✅ 5. 写 wrapper 或工具自动分析时\n# 有些高级工具会分析类的 __doc__ 生成参数列表、接口规范等。比如 HuggingFace CLI 工具、AutoModel 分支选择时也可能用到。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HuggingFace Transformers 框架中用于 向类的 docstring 中追加代码调用示例 的工具函数，\n# 作用是为 FlaxAlbertModel 类动态添加“使用示例代码段”。\nappend_call_sample_docstring(\n    FlaxAlbertModel, # 被修改的类 \n    _CHECKPOINT_FOR_DOC,   # 使用的预训练模型名称（作为示例）\n    FlaxBaseModelOutputWithPooling,   # 输出的数据结构类型（用于 docstring 说明）\n    _CONFIG_FOR_DOC)  # 配置类（也用于 docstring 示例）","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:33:03.383184Z","iopub.execute_input":"2025-05-24T08:33:03.383962Z","iopub.status.idle":"2025-05-24T08:33:03.388887Z","shell.execute_reply.started":"2025-05-24T08:33:03.383922Z","shell.execute_reply":"2025-05-24T08:33:03.387938Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"# 为 API 自动文档生成提供一致的调用示例\n# 用户在 help()、IDE 或网页文档中能看到该模型如何初始化、如何调用\n# 配合 add_start_docstrings 形成完整 docstring 结构：\n# 前面是模型说明（add_start_docstrings）\n# 后面是使用代码（append_call_sample_docstring）","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"help(FlaxAlbertModel)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxAlbertModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:34:45.690432Z","iopub.execute_input":"2025-05-24T08:34:45.690748Z","iopub.status.idle":"2025-05-24T08:34:45.740356Z","shell.execute_reply.started":"2025-05-24T08:34:45.690725Z","shell.execute_reply":"2025-05-24T08:34:45.739208Z"}},"outputs":[],"execution_count":165},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:34:53.471393Z","iopub.execute_input":"2025-05-24T08:34:53.471734Z","iopub.status.idle":"2025-05-24T08:34:53.955537Z","shell.execute_reply.started":"2025-05-24T08:34:53.471710Z","shell.execute_reply":"2025-05-24T08:34:53.954468Z"}},"outputs":[],"execution_count":166},{"cell_type":"code","source":"model = FlaxAlbertModel.from_pretrained(\"albert-xxlarge-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:35:02.032781Z","iopub.execute_input":"2025-05-24T08:35:02.033111Z","iopub.status.idle":"2025-05-24T08:35:18.547668Z","shell.execute_reply.started":"2025-05-24T08:35:02.033089Z","shell.execute_reply":"2025-05-24T08:35:18.546461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/893M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd9064f84538440bb1b14cee2349b9fa"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing FlaxAlbertModel: {('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'LayerNorm', 'kernel')}\n- This IS expected if you are initializing FlaxAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":167},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\") # 返回张量","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:35:26.931648Z","iopub.execute_input":"2025-05-24T08:35:26.931967Z","iopub.status.idle":"2025-05-24T08:35:26.955142Z","shell.execute_reply.started":"2025-05-24T08:35:26.931942Z","shell.execute_reply":"2025-05-24T08:35:26.953950Z"}},"outputs":[],"execution_count":168},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:36:27.679814Z","iopub.execute_input":"2025-05-24T08:36:27.680155Z","iopub.status.idle":"2025-05-24T08:36:27.687657Z","shell.execute_reply.started":"2025-05-24T08:36:27.680132Z","shell.execute_reply":"2025-05-24T08:36:27.686497Z"}},"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"{'input_ids': Array([[    2, 10975,    15,    51,  1952,    25, 10901,     3]], dtype=int32), 'token_type_ids': Array([[0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': Array([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)}"},"metadata":{}}],"execution_count":170},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:36:37.646339Z","iopub.execute_input":"2025-05-24T08:36:37.646657Z","iopub.status.idle":"2025-05-24T08:36:40.544680Z","shell.execute_reply.started":"2025-05-24T08:36:37.646633Z","shell.execute_reply":"2025-05-24T08:36:40.543062Z"}},"outputs":[],"execution_count":171},{"cell_type":"code","source":"last_hidden_states = outputs.last_hidden_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:36:50.273919Z","iopub.execute_input":"2025-05-24T08:36:50.274348Z","iopub.status.idle":"2025-05-24T08:36:50.280032Z","shell.execute_reply.started":"2025-05-24T08:36:50.274306Z","shell.execute_reply":"2025-05-24T08:36:50.278573Z"}},"outputs":[],"execution_count":172},{"cell_type":"code","source":"last_hidden_states.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:37:08.097439Z","iopub.execute_input":"2025-05-24T08:37:08.097872Z","iopub.status.idle":"2025-05-24T08:37:08.105819Z","shell.execute_reply.started":"2025-05-24T08:37:08.097843Z","shell.execute_reply":"2025-05-24T08:37:08.104775Z"}},"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"(1, 8, 4096)"},"metadata":{}}],"execution_count":174},{"cell_type":"code","source":"# 层层封装 这个封装之前的\nclass FlaxAlbertForPreTrainingModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32  # 模型内部计算使用的数值精度类型\n    def setup(self):\n        # 主体部分使用 ALBERT 编码器模块\n        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype)\n        # 用于 Masked Language Modeling（MLM）任务的预测头\n        self.predictions = FlaxAlbertOnlyMLMHead(config=self.config, dtype=self.dtype)\n        # 用于 Sentence Order Prediction（SOP）任务的分类器\n        self.sop_classifier = FlaxAlbertSOPHead(config=self.config, dtype=self.dtype)\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        deterministic: bool = True,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        # 1. 输入传入基础 ALBERT 模块，得到编码器输出（包括序列特征与池化输出）\n        outputs = self.albert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            position_ids,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        # 2. 如果使用共享词嵌入，则从 embedding 层中提取词向量权重，用于 decoder 权重共享\n        if self.config.tie_word_embeddings:\n            shared_embedding = self.albert.variables[\"params\"][\"embeddings\"][\"word_embeddings\"][\"embedding\"]\n        else:\n            shared_embedding = None\n\n        hidden_states = outputs[0]  # 序列中每个 token 的隐状态（用于 MLM）\n        pooled_output = outputs[1]  # 第一个 token 的池化表示（用于 SOP）\n        # 3. 预测 masked token 的词（MLM 输出）\n        prediction_scores = self.predictions(hidden_states, shared_embedding=shared_embedding)\n        # 4. 预测句子顺序是否合理（SOP 输出）\n        sop_scores = self.sop_classifier(pooled_output, deterministic=deterministic)\n        # 5. 返回结果，支持 tuple 或 dict 形式\n        if not return_dict:\n            return (prediction_scores, sop_scores) + outputs[2:]\n\n        return FlaxAlbertForPreTrainingOutput(\n            prediction_logits=prediction_scores, # MLM logits\n            sop_logits=sop_scores,   # SOP logits\n            hidden_states=outputs.hidden_states, # 可选中间层表示\n            attentions=outputs.attentions,   # 可选注意力权重\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:51:34.343853Z","iopub.execute_input":"2025-05-24T08:51:34.345074Z","iopub.status.idle":"2025-05-24T08:51:34.355599Z","shell.execute_reply.started":"2025-05-24T08:51:34.345041Z","shell.execute_reply":"2025-05-24T08:51:34.354629Z"}},"outputs":[],"execution_count":181},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Albert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a\n    `sentence order prediction (classification)` head.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass FlaxAlbertForPreTraining(FlaxAlbertPreTrainedModel):\n    module_class = FlaxAlbertForPreTrainingModule # 内部子模块","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:53:41.974776Z","iopub.execute_input":"2025-05-24T08:53:41.975254Z","iopub.status.idle":"2025-05-24T08:53:41.982608Z","shell.execute_reply.started":"2025-05-24T08:53:41.975215Z","shell.execute_reply":"2025-05-24T08:53:41.980873Z"}},"outputs":[],"execution_count":182},{"cell_type":"code","source":"model = FlaxAlbertForPreTraining.from_pretrained(\"albert-xxlarge-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:54:20.424496Z","iopub.execute_input":"2025-05-24T08:54:20.424892Z","iopub.status.idle":"2025-05-24T08:54:31.681859Z","shell.execute_reply.started":"2025-05-24T08:54:20.424865Z","shell.execute_reply":"2025-05-24T08:54:31.680669Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing FlaxAlbertForPreTraining: {('predictions', 'decoder', 'bias')}\n- This IS expected if you are initializing FlaxAlbertForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxAlbertForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxAlbertForPreTraining were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: {('sop_classifier', 'classifier', 'bias'), ('sop_classifier', 'classifier', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":184},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:54:35.629064Z","iopub.execute_input":"2025-05-24T08:54:35.630427Z","iopub.status.idle":"2025-05-24T08:54:38.678199Z","shell.execute_reply.started":"2025-05-24T08:54:35.630381Z","shell.execute_reply":"2025-05-24T08:54:38.677231Z"}},"outputs":[],"execution_count":185},{"cell_type":"code","source":"jax.tree.map(jnp.shape,outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:55:27.679406Z","iopub.execute_input":"2025-05-24T08:55:27.679767Z","iopub.status.idle":"2025-05-24T08:55:27.687080Z","shell.execute_reply.started":"2025-05-24T08:55:27.679745Z","shell.execute_reply":"2025-05-24T08:55:27.686070Z"}},"outputs":[{"execution_count":187,"output_type":"execute_result","data":{"text/plain":"FlaxAlbertForPreTrainingOutput(prediction_logits=(1, 8, 30000), sop_logits=(1, 2), hidden_states=None, attentions=None)"},"metadata":{}}],"execution_count":187},{"cell_type":"code","source":"FLAX_ALBERT_FOR_PRETRAINING_DOCSTRING = \"\"\"\n    Returns:\n\n    Example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, FlaxAlbertForPreTraining\n\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n    >>> model = FlaxAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n\n    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n    >>> outputs = model(**inputs)\n\n    >>> prediction_logits = outputs.prediction_logits\n    >>> seq_relationship_logits = outputs.sop_logits\n    ```\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:53:44.612966Z","iopub.execute_input":"2025-05-24T08:53:44.613364Z","iopub.status.idle":"2025-05-24T08:53:44.618814Z","shell.execute_reply.started":"2025-05-24T08:53:44.613332Z","shell.execute_reply":"2025-05-24T08:53:44.617730Z"}},"outputs":[],"execution_count":183},{"cell_type":"code","source":"print(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\") + FLAX_ALBERT_FOR_PRETRAINING_DOCSTRING)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 这行代码的作用是覆盖 FlaxAlbertForPreTraining.__call__ 方法的文档字符串（docstring），即当用户使\n# 用 help(FlaxAlbertForPreTraining) 或 IDE 查看方法时，会看到更清晰的调用说明\noverwrite_call_docstring(\n    FlaxAlbertForPreTraining,\n    ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\") + FLAX_ALBERT_FOR_PRETRAINING_DOCSTRING,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:02:01.196635Z","iopub.execute_input":"2025-05-24T09:02:01.197046Z","iopub.status.idle":"2025-05-24T09:02:01.205645Z","shell.execute_reply.started":"2025-05-24T09:02:01.197016Z","shell.execute_reply":"2025-05-24T09:02:01.204355Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"# overwrite_call_docstring 和 @add_start_docstrings 是作用于不同目标的文档字符串：\n# @add_start_docstrings 是给 类本身 添加或扩展 docstring。\n# overwrite_call_docstring 是给 类的 __call__ 方法 覆盖（替换）docstring。\n# 两者文档不会简单“前后”叠加\n# @add_start_docstrings 影响的是 FlaxAlbertForPreTraining 类的 docstring，通常你用 help(\n#     FlaxAlbertForPreTraining) 看到的是它的内容。\n# overwrite_call_docstring 影响的是 FlaxAlbertForPreTraining.__call__ 函数的 \n# docstring，help(FlaxAlbertForPreTraining.__call__) 才会看到它。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"help(FlaxAlbertForPreTraining.__call__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:03:10.196058Z","iopub.execute_input":"2025-05-24T09:03:10.196482Z","iopub.status.idle":"2025-05-24T09:03:10.202809Z","shell.execute_reply.started":"2025-05-24T09:03:10.196451Z","shell.execute_reply":"2025-05-24T09:03:10.201597Z"}},"outputs":[{"name":"stdout","text":"Help on function __call__ in module __main__:\n\n__call__(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, params: dict = None, dropout_rng: <function PRNGKey at 0x7e9af89e4ea0> = None, train: bool = False, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None)\n    The [`FlaxAlbertPreTrainedModel`] forward method, overrides the `__call__` special method.\n    \n    <Tip>\n    \n    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n    the latter silently ignores them.\n    \n    </Tip>\n    \n    Args:\n        input_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary.\n    \n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n    \n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n    \n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n    \n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n    \n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n    \n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n    \n    \n    Returns:\n    \n    Example:\n    \n    ```python\n    >>> from transformers import AutoTokenizer, FlaxAlbertForPreTraining\n    \n    >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n    >>> model = FlaxAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n    \n    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n    >>> outputs = model(**inputs)\n    \n    >>> prediction_logits = outputs.prediction_logits\n    >>> seq_relationship_logits = outputs.sop_logits\n    ```\n\n","output_type":"stream"}],"execution_count":196},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxAlbertForPreTraining","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:03:57.844170Z","iopub.execute_input":"2025-05-24T09:03:57.844632Z","iopub.status.idle":"2025-05-24T09:03:57.851675Z","shell.execute_reply.started":"2025-05-24T09:03:57.844601Z","shell.execute_reply":"2025-05-24T09:03:57.850221Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:04:05.615465Z","iopub.execute_input":"2025-05-24T09:04:05.615788Z","iopub.status.idle":"2025-05-24T09:04:08.912818Z","shell.execute_reply.started":"2025-05-24T09:04:05.615768Z","shell.execute_reply":"2025-05-24T09:04:08.911766Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708a1a5329434ea5b99996267cf5a139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b54338851d1469c8cf0e30820026719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a23124a2bc6843cca6130b3b3a6192ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a7b2c4196f4f4cb6fa3eec5fac1b80"}},"metadata":{}}],"execution_count":198},{"cell_type":"code","source":"model = FlaxAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:04:25.178051Z","iopub.execute_input":"2025-05-24T09:04:25.178845Z","iopub.status.idle":"2025-05-24T09:04:33.475585Z","shell.execute_reply.started":"2025-05-24T09:04:25.178808Z","shell.execute_reply":"2025-05-24T09:04:33.474521Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"flax_model.msgpack:   0%|          | 0.00/44.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ff7e61c7834cba9b93b68f2cbad8f4"}},"metadata":{}},{"name":"stderr","text":"Some weights of FlaxAlbertForPreTraining were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: {('sop_classifier', 'classifier', 'bias'), ('albert', 'pooler', 'kernel'), ('albert', 'pooler', 'bias'), ('sop_classifier', 'classifier', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":199},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:04:36.318887Z","iopub.execute_input":"2025-05-24T09:04:36.319311Z","iopub.status.idle":"2025-05-24T09:04:36.326472Z","shell.execute_reply.started":"2025-05-24T09:04:36.319285Z","shell.execute_reply":"2025-05-24T09:04:36.325361Z"}},"outputs":[],"execution_count":200},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:04:42.840876Z","iopub.execute_input":"2025-05-24T09:04:42.841278Z","iopub.status.idle":"2025-05-24T09:04:44.337256Z","shell.execute_reply.started":"2025-05-24T09:04:42.841250Z","shell.execute_reply":"2025-05-24T09:04:44.336299Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"prediction_logits = outputs.prediction_logits\nseq_relationship_logits = outputs.sop_logits # 序列关系","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:05:42.324102Z","iopub.execute_input":"2025-05-24T09:05:42.324499Z","iopub.status.idle":"2025-05-24T09:05:42.330163Z","shell.execute_reply.started":"2025-05-24T09:05:42.324474Z","shell.execute_reply":"2025-05-24T09:05:42.328681Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"print(prediction_logits.shape,seq_relationship_logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:06:06.782841Z","iopub.execute_input":"2025-05-24T09:06:06.783299Z","iopub.status.idle":"2025-05-24T09:06:06.788915Z","shell.execute_reply.started":"2025-05-24T09:06:06.783268Z","shell.execute_reply":"2025-05-24T09:06:06.787790Z"}},"outputs":[{"name":"stdout","text":"(1, 8, 30000) (1, 2)\n","output_type":"stream"}],"execution_count":205},{"cell_type":"code","source":"_CONFIG_FOR_DOC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:06:31.873256Z","iopub.execute_input":"2025-05-24T09:06:31.873756Z","iopub.status.idle":"2025-05-24T09:06:31.880432Z","shell.execute_reply.started":"2025-05-24T09:06:31.873726Z","shell.execute_reply":"2025-05-24T09:06:31.879395Z"}},"outputs":[{"execution_count":206,"output_type":"execute_result","data":{"text/plain":"'AlbertConfig'"},"metadata":{}}],"execution_count":206},{"cell_type":"code","source":"append_replace_return_docstrings(\n    FlaxAlbertForPreTraining, \n    output_type=FlaxAlbertForPreTrainingOutput,\n    config_class=_CONFIG_FOR_DOC\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# append_replace_return_docstrings 主要是修改类的 __call__ 方法的 docstring 中的返回值部分，\n# 具体位置是在 overwrite_call_docstring 已经设置的 docstring 中，针对返回值描述段进行追加或替换。\n# 具体位置关系\n# overwrite_call_docstring 先对 __call__ 的整体 docstring 进行覆盖（包括输入、输出等说明）\n# append_replace_return_docstrings 进一步追加或替换返回值部分的描述，细化输出信息\n# 所以它是在 overwrite_call_docstring 设置的 docstring 基础上，针对“返回值”段落做补充或覆盖\n# 什么时候能看到？\n# 执行 help(FlaxAlbertForPreTraining.__call__)（查看该类调用方法的帮助文档）\n# 或者在支持的 IDE 中查看 __call__ 方法的文档时","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlaxAlbertForMaskedLMModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # 初始化子模块\n        # 不使用池化层，符合MaskedLM任务特点（无需序列级别表示）\n        self.albert = FlaxAlbertModule(config=self.config, add_pooling_layer=False, dtype=self.dtype)\n        # 预测头，用于输出词汇表上的预测分数\n        self.predictions = FlaxAlbertOnlyMLMHead(config=self.config, dtype=self.dtype)\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        deterministic: bool = True,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        # 先通过 Albert 基础模型获取隐藏状态\n        outputs = self.albert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            position_ids,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]   # 最后一层隐藏状态\n         # 是否共享词向量权重，减少参数，提升性能\n        if self.config.tie_word_embeddings:\n            shared_embedding = self.albert.variables[\"params\"][\"embeddings\"][\"word_embeddings\"][\"embedding\"]\n        else:\n            shared_embedding = None\n\n        # 通过预测头计算每个token对应词汇表上的预测分数(logits)\n        logits = self.predictions(hidden_states, shared_embedding=shared_embedding)\n\n        if not return_dict: # 返回tuple结构，兼容不同调用方式\n            return (logits,) + outputs[1:]\n        # 返回结构化对象，包含logits、隐藏状态和注意力权重，方便后续处理和分析\n        return FlaxMaskedLMOutput(\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:15:23.521848Z","iopub.execute_input":"2025-05-24T09:15:23.522449Z","iopub.status.idle":"2025-05-24T09:15:23.539208Z","shell.execute_reply.started":"2025-05-24T09:15:23.522415Z","shell.execute_reply":"2025-05-24T09:15:23.537803Z"}},"outputs":[],"execution_count":209},{"cell_type":"code","source":"# 设计意图：\n# 该模块基于Albert基础模型，专注于Masked Language Model任务，故禁用池化层（不需要句向量）。\n# 支持共享词嵌入权重，减少参数冗余。\n# 通过返回结构化输出，方便后续调用者获取更多信息（如中间隐藏层、注意力）","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 掩码模型,继承自PreTrainedModel 这样可以用from_pretrained方法 内部包装了一个子模块\n@add_start_docstrings(\"\"\"Albert Model with a `language modeling` head on top.\"\"\", ALBERT_START_DOCSTRING)\nclass FlaxAlbertForMaskedLM(FlaxAlbertPreTrainedModel): \n    module_class = FlaxAlbertForMaskedLMModule","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:18:14.173211Z","iopub.execute_input":"2025-05-24T09:18:14.173632Z","iopub.status.idle":"2025-05-24T09:18:14.179705Z","shell.execute_reply.started":"2025-05-24T09:18:14.173606Z","shell.execute_reply":"2025-05-24T09:18:14.178610Z"}},"outputs":[],"execution_count":210},{"cell_type":"code","source":"append_call_sample_docstring(\n    FlaxAlbertForMaskedLM,    # 目标模型类\n    _CHECKPOINT_FOR_DOC,  # 用于示例的预训练模型权重标识（checkpoint）\n    FlaxMaskedLMOutput,   # 模型调用返回的输出类型，用于生成文档说明\n    _CONFIG_FOR_DOC,  # 模型配置类，用于补充配置参数说明\n    revision=\"refs/pr/11\"   # 代码版本信息，便于追踪文档对应的代码分支或PR\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:20:13.496736Z","iopub.execute_input":"2025-05-24T09:20:13.497225Z","iopub.status.idle":"2025-05-24T09:20:13.503665Z","shell.execute_reply.started":"2025-05-24T09:20:13.497197Z","shell.execute_reply":"2025-05-24T09:20:13.502612Z"}},"outputs":[],"execution_count":212},{"cell_type":"code","source":"help(FlaxAlbertForMaskedLM.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ```python\n# >>> from transformers import AutoTokenizer, FlaxAlbertForMaskedLM\n\n# >>> tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\", revision=\"refs/pr/11\")\n# >>> model = FlaxAlbertForMaskedLM.from_pretrained(\"albert-xxlarge-v2\", revision=\"refs/pr/11\")\n\n# >>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"jax\")\n\n# >>> outputs = model(**inputs)\n# >>> logits = outputs.logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlaxAlbertForSequenceClassificationModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n         # 初始化基础Albert模型\n        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype)\n        # 分类器的dropout率，优先使用专门的classifier_dropout_prob，否则用隐藏层dropout\n        classifier_dropout = (\n            self.config.classifier_dropout_prob\n            if self.config.classifier_dropout_prob is not None\n            else self.config.hidden_dropout_prob\n        )\n        # Dropout层，用于分类器输入，防止过拟合\n        self.dropout = nn.Dropout(rate=classifier_dropout)\n        self.classifier = nn.Dense( # 线性分类层，输出类别数目个logits\n            self.config.num_labels,\n            dtype=self.dtype,\n        )\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        deterministic: bool = True,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        # 通过Albert基础模型获得输出，含隐藏状态和池化后的句向量\n        outputs = self.albert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            position_ids,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1] # 获取池化输出（句子级别表示）\n        pooled_output = self.dropout(pooled_output, deterministic=deterministic)  # 对池化输出应用dropout，训练时生效，推理时关闭\n        logits = self.classifier(pooled_output) # 线性层计算分类logits\n\n        if not return_dict:  # 返回元组，兼容旧接口\n            return (logits,) + outputs[2:]\n        # 返回结构化输出，包含logits、隐藏层状态和注意力权重\n        return FlaxSequenceClassifierOutput(\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:24:22.376099Z","iopub.execute_input":"2025-05-24T09:24:22.376529Z","iopub.status.idle":"2025-05-24T09:24:22.388841Z","shell.execute_reply.started":"2025-05-24T09:24:22.376500Z","shell.execute_reply":"2025-05-24T09:24:22.387718Z"}},"outputs":[],"execution_count":214},{"cell_type":"code","source":"# 设计意图：\n# 基于Albert模型做序列分类，使用池化层输出句子表示。\n# 分类头包含Dropout和全连接层，防止过拟合并实现类别预测。\n# 支持返回dict或tuple格式的结果，兼容不同调用习惯。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 继承自FlaxAlbertPreTrainedModel 就具有下载和加载模型的方法\n# 里面包装了一个子模块,实际上是调用子模块的call方法\n@add_start_docstrings(\n    \"\"\"\n    Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass FlaxAlbertForSequenceClassification(FlaxAlbertPreTrainedModel):\n    module_class = FlaxAlbertForSequenceClassificationModule","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:28:15.860039Z","iopub.execute_input":"2025-05-24T09:28:15.860397Z","iopub.status.idle":"2025-05-24T09:28:15.865594Z","shell.execute_reply.started":"2025-05-24T09:28:15.860375Z","shell.execute_reply":"2025-05-24T09:28:15.864655Z"}},"outputs":[],"execution_count":215},{"cell_type":"code","source":"append_call_sample_docstring( # 附加调用示例文档字符串\n    FlaxAlbertForSequenceClassification, # 为哪个类添加\n    _CHECKPOINT_FOR_DOC, # 用于示例的预训练模型权重标识（checkpoint）\n    FlaxSequenceClassifierOutput, # 模型调用返回的输出类型，用于生成文档说明\n    _CONFIG_FOR_DOC, # 模型配置类，用于补充配置参数说明\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:30:11.963202Z","iopub.execute_input":"2025-05-24T09:30:11.963639Z","iopub.status.idle":"2025-05-24T09:30:11.970843Z","shell.execute_reply.started":"2025-05-24T09:30:11.963612Z","shell.execute_reply":"2025-05-24T09:30:11.969088Z"}},"outputs":[],"execution_count":216},{"cell_type":"code","source":"# help(FlaxAlbertForSequenceClassification.__call__)\n # Example:\n    \n #    ```python\n #    >>> from transformers import AutoTokenizer, FlaxAlbertForSequenceClassification\n    \n #    >>> tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")\n #    >>> model = FlaxAlbertForSequenceClassification.from_pretrained(\"albert-xxlarge-v2\")\n    \n #    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n    \n #    >>> outputs = model(**inputs)\n #    >>> logits = outputs.logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlaxAlbertForMultipleChoiceModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # 初始化ALBERT主干模型\n        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype)\n         # 多选任务中用于分类前的dropout，缓解过拟合\n        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n         # 最终分类器，输出每个选项一个logit（二分类），后续reshape得到每个样本的多个选项\n        self.classifier = nn.Dense(1, dtype=self.dtype)\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        deterministic: bool = True,  # 控制Dropout是否启用\n        output_attentions: bool = False, # 是否返回注意力信息\n        output_hidden_states: bool = False,  # 是否返回中间隐藏层\n        return_dict: bool = True,  # 是否返回结构化结果\n    ):\n        # 多选任务中input维度为(batch_size, num_choices, seq_len)，这里reshape成(batch_size*num_choices, seq_len)\n        num_choices = input_ids.shape[1]\n        input_ids = input_ids.reshape(-1, input_ids.shape[-1]) if input_ids is not None else None\n        attention_mask = attention_mask.reshape(-1, attention_mask.shape[-1]) if attention_mask is not None else None\n        token_type_ids = token_type_ids.reshape(-1, token_type_ids.shape[-1]) if token_type_ids is not None else None\n        position_ids = position_ids.reshape(-1, position_ids.shape[-1]) if position_ids is not None else None\n\n        # 通过ALBERT主干获得每个选项的输出\n        outputs = self.albert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            position_ids,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        # 取句子级表示（通常是[CLS]位输出）\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output, deterministic=deterministic)\n        logits = self.classifier(pooled_output)  # 每个选项对应一个logit (batch_size*num_choices, 1)\n        # 恢复为(batch_size, num_choices)\n        reshaped_logits = logits.reshape(-1, num_choices)\n\n        if not return_dict:\n            return (reshaped_logits,) + outputs[2:]\n        # 返回结构化输出：logits、隐藏层和注意力\n        return FlaxMultipleChoiceModelOutput(\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:35:39.507434Z","iopub.execute_input":"2025-05-24T09:35:39.507834Z","iopub.status.idle":"2025-05-24T09:35:39.521876Z","shell.execute_reply.started":"2025-05-24T09:35:39.507796Z","shell.execute_reply":"2025-05-24T09:35:39.520683Z"}},"outputs":[],"execution_count":219},{"cell_type":"code","source":"# 设计意图说明：\n# 本模块用于多选任务（例如 SWAG、RACE），每个输入样本包含多个选项，模型需输出每个选项的分数。\n# 将多选维度合并为批次维度，通过共享的 ALBERT 模型分别处理每个选项，再通过线性分类层输出 logits。\n# 最后将 logits reshape 回 (batch_size, num_choices)，用于交叉熵计算或评估。\n# 支持结构化输出，便于调试或进一步处理中间层结果","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Albert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass FlaxAlbertForMultipleChoice(FlaxAlbertPreTrainedModel):\n    module_class = FlaxAlbertForMultipleChoiceModule","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:35:42.287418Z","iopub.execute_input":"2025-05-24T09:35:42.287794Z","iopub.status.idle":"2025-05-24T09:35:42.294238Z","shell.execute_reply.started":"2025-05-24T09:35:42.287768Z","shell.execute_reply":"2025-05-24T09:35:42.292471Z"}},"outputs":[],"execution_count":220},{"cell_type":"code","source":"overwrite_call_docstring(\n    FlaxAlbertForMultipleChoice, ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\")\n)\nappend_call_sample_docstring(\n    FlaxAlbertForMultipleChoice,\n    _CHECKPOINT_FOR_DOC,\n    FlaxMultipleChoiceModelOutput,\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:35:56.093994Z","iopub.execute_input":"2025-05-24T09:35:56.094352Z","iopub.status.idle":"2025-05-24T09:35:56.102284Z","shell.execute_reply.started":"2025-05-24T09:35:56.094329Z","shell.execute_reply":"2025-05-24T09:35:56.100791Z"}},"outputs":[],"execution_count":221},{"cell_type":"code","source":"_CONFIG_FOR_DOC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:36:24.379653Z","iopub.execute_input":"2025-05-24T09:36:24.380196Z","iopub.status.idle":"2025-05-24T09:36:24.388927Z","shell.execute_reply.started":"2025-05-24T09:36:24.380161Z","shell.execute_reply":"2025-05-24T09:36:24.387359Z"}},"outputs":[{"execution_count":222,"output_type":"execute_result","data":{"text/plain":"'AlbertConfig'"},"metadata":{}}],"execution_count":222},{"cell_type":"code","source":"# help(FlaxAlbertForMultipleChoice.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxAlbertForMultipleChoice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:37:38.020521Z","iopub.execute_input":"2025-05-24T09:37:38.020949Z","iopub.status.idle":"2025-05-24T09:37:38.026971Z","shell.execute_reply.started":"2025-05-24T09:37:38.020918Z","shell.execute_reply":"2025-05-24T09:37:38.025394Z"}},"outputs":[],"execution_count":224},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert-xlarge-v2\")\nmodel = FlaxAlbertForMultipleChoice.from_pretrained(\"albert-xlarge-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:38:17.730848Z","iopub.execute_input":"2025-05-24T09:38:17.731343Z","iopub.status.idle":"2025-05-24T09:38:32.264876Z","shell.execute_reply.started":"2025-05-24T09:38:17.731311Z","shell.execute_reply":"2025-05-24T09:38:32.263755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"700f8b50a55044eeb428706a8a8f9fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4803a0bc343b4563bf161de510f0c328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2336ed7d82044d089da3b2f89f55794d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/236M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f0b7695b53c4e4ebe4ff576e93bf4eb"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing FlaxAlbertForMultipleChoice: {('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'LayerNorm', 'kernel')}\n- This IS expected if you are initializing FlaxAlbertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxAlbertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxAlbertForMultipleChoice were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":225},{"cell_type":"code","source":"# 在意大利，在餐厅等正式场合供应的披萨都是未切片的\nprompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\nchoice0 = \"It is eaten with a fork and a knife.\" # 用刀叉吃\nchoice1 = \"It is eaten while held in the hand.\" # 拿在手里就可以吃。","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:39:49.908871Z","iopub.execute_input":"2025-05-24T09:39:49.909289Z","iopub.status.idle":"2025-05-24T09:39:49.915519Z","shell.execute_reply.started":"2025-05-24T09:39:49.909260Z","shell.execute_reply":"2025-05-24T09:39:49.914221Z"}},"outputs":[],"execution_count":227},{"cell_type":"code","source":"encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"jax\", padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:40:33.275217Z","iopub.execute_input":"2025-05-24T09:40:33.275719Z","iopub.status.idle":"2025-05-24T09:40:33.307991Z","shell.execute_reply.started":"2025-05-24T09:40:33.275689Z","shell.execute_reply":"2025-05-24T09:40:33.306978Z"}},"outputs":[],"execution_count":228},{"cell_type":"code","source":"encoding['input_ids'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:41:11.169077Z","iopub.execute_input":"2025-05-24T09:41:11.169439Z","iopub.status.idle":"2025-05-24T09:41:11.176495Z","shell.execute_reply.started":"2025-05-24T09:41:11.169413Z","shell.execute_reply":"2025-05-24T09:41:11.175367Z"}},"outputs":[{"execution_count":230,"output_type":"execute_result","data":{"text/plain":"(2, 35)"},"metadata":{}}],"execution_count":230},{"cell_type":"code","source":"outputs = model(**{k: v[None, :] for k, v in encoding.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:42:54.965441Z","iopub.execute_input":"2025-05-24T09:42:54.965927Z","iopub.status.idle":"2025-05-24T09:43:01.327189Z","shell.execute_reply.started":"2025-05-24T09:42:54.965876Z","shell.execute_reply":"2025-05-24T09:43:01.326004Z"}},"outputs":[],"execution_count":233},{"cell_type":"code","source":"outputs.logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:43:59.622622Z","iopub.execute_input":"2025-05-24T09:43:59.623061Z","iopub.status.idle":"2025-05-24T09:43:59.631079Z","shell.execute_reply.started":"2025-05-24T09:43:59.623030Z","shell.execute_reply":"2025-05-24T09:43:59.629796Z"}},"outputs":[{"execution_count":235,"output_type":"execute_result","data":{"text/plain":"Array([[0.08329672, 0.0852083 ]], dtype=float32)"},"metadata":{}}],"execution_count":235},{"cell_type":"code","source":"class FlaxAlbertForTokenClassificationModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # 初始化 ALBERT 主干模型，token classification 不需要句向量，故关闭 pooling\n        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)\n        # 获取分类用的 dropout 概率，优先使用 classifier_dropout_prob\n        classifier_dropout = (\n            self.config.classifier_dropout_prob\n            if self.config.classifier_dropout_prob is not None\n            else self.config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(rate=classifier_dropout) # 定义 dropout 层\n        # 定义分类器，用于对每个 token 输出 num_labels 个类别得分\n        self.classifier = nn.Dense(self.config.num_labels, dtype=self.dtype)\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        deterministic: bool = True,  # 控制 dropout 是否启用（训练时 False，推理时 True）\n        output_attentions: bool = False, # 是否返回注意力权重\n        output_hidden_states: bool = False,  # 是否返回所有隐藏层\n        return_dict: bool = True, # 是否返回结构化结果\n    ):\n        # 调用 ALBERT 主干模型，获得所有 token 的隐藏状态（hidden_states）等信息\n        outputs = self.albert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            position_ids,\n            deterministic=deterministic,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0] # 获取最后一层的隐藏状态\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic) # 应用 dropout（训练时启用，推理时关闭）\n        # 对每个 token 输出类别 logits（shape: [batch_size, seq_len, num_labels]）\n        logits = self.classifier(hidden_states)\n\n        if not return_dict: # 返回元组形式\n            return (logits,) + outputs[1:]\n\n        return FlaxTokenClassifierOutput( # 返回结构化输出，包含 logits、隐藏状态和注意力\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:47:23.834677Z","iopub.execute_input":"2025-05-24T09:47:23.835147Z","iopub.status.idle":"2025-05-24T09:47:23.847690Z","shell.execute_reply.started":"2025-05-24T09:47:23.835113Z","shell.execute_reply":"2025-05-24T09:47:23.846524Z"}},"outputs":[],"execution_count":236},{"cell_type":"code","source":"# add_start_docstrings在调用help(FlaxAlbertForTokenClassification)时就可以显示\n# 继承自FlaxAlbertPreTrainedModel是因为，父类已经有初始化权重方法和下载，加载权重的方法\n# module_class是模型类内部封装的子模块,这样调用时因为FlaxAlbertPreTrainedModel call\n# 的是子模块的call方法,所以相当于包装器用法\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass FlaxAlbertForTokenClassification(FlaxAlbertPreTrainedModel):\n    module_class = FlaxAlbertForTokenClassificationModule","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:50:43.434976Z","iopub.execute_input":"2025-05-24T09:50:43.435417Z","iopub.status.idle":"2025-05-24T09:50:43.442045Z","shell.execute_reply.started":"2025-05-24T09:50:43.435386Z","shell.execute_reply":"2025-05-24T09:50:43.441008Z"}},"outputs":[],"execution_count":237},{"cell_type":"code","source":"_CHECKPOINT_FOR_DOC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:51:24.678757Z","iopub.execute_input":"2025-05-24T09:51:24.680251Z","iopub.status.idle":"2025-05-24T09:51:24.686762Z","shell.execute_reply.started":"2025-05-24T09:51:24.680206Z","shell.execute_reply":"2025-05-24T09:51:24.685802Z"}},"outputs":[{"execution_count":238,"output_type":"execute_result","data":{"text/plain":"'albert-xxlarge-v2'"},"metadata":{}}],"execution_count":238},{"cell_type":"code","source":"_CONFIG_FOR_DOC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:51:38.368238Z","iopub.execute_input":"2025-05-24T09:51:38.368657Z","iopub.status.idle":"2025-05-24T09:51:38.376151Z","shell.execute_reply.started":"2025-05-24T09:51:38.368625Z","shell.execute_reply":"2025-05-24T09:51:38.374828Z"}},"outputs":[{"execution_count":239,"output_type":"execute_result","data":{"text/plain":"'AlbertConfig'"},"metadata":{}}],"execution_count":239},{"cell_type":"code","source":"append_call_sample_docstring(\n    FlaxAlbertForTokenClassification, # 为什么类的call方法添加文档字符串\n    _CHECKPOINT_FOR_DOC, \n    FlaxTokenClassifierOutput, # 输出返回值类型\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:52:09.473543Z","iopub.execute_input":"2025-05-24T09:52:09.474123Z","iopub.status.idle":"2025-05-24T09:52:09.480001Z","shell.execute_reply.started":"2025-05-24T09:52:09.474088Z","shell.execute_reply":"2025-05-24T09:52:09.478831Z"}},"outputs":[],"execution_count":240},{"cell_type":"code","source":"# help(FlaxAlbertForTokenClassification.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxAlbertForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")\nmodel = FlaxAlbertForTokenClassification.from_pretrained(\"albert-xxlarge-v2\")\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:54:01.325675Z","iopub.execute_input":"2025-05-24T09:54:01.327036Z","iopub.status.idle":"2025-05-24T09:54:11.689407Z","shell.execute_reply.started":"2025-05-24T09:54:01.326990Z","shell.execute_reply":"2025-05-24T09:54:11.688356Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing FlaxAlbertForTokenClassification: {('albert', 'pooler', 'kernel'), ('albert', 'pooler', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'LayerNorm', 'kernel')}\n- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":242},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:54:32.107462Z","iopub.execute_input":"2025-05-24T09:54:32.107856Z","iopub.status.idle":"2025-05-24T09:54:32.116625Z","shell.execute_reply.started":"2025-05-24T09:54:32.107829Z","shell.execute_reply":"2025-05-24T09:54:32.115513Z"}},"outputs":[{"execution_count":243,"output_type":"execute_result","data":{"text/plain":"{'input_ids': Array([[    2, 10975,    15,    51,  1952,    25, 10901,     3]], dtype=int32), 'token_type_ids': Array([[0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': Array([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)}"},"metadata":{}}],"execution_count":243},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:54:42.157330Z","iopub.execute_input":"2025-05-24T09:54:42.157719Z","iopub.status.idle":"2025-05-24T09:54:43.661685Z","shell.execute_reply.started":"2025-05-24T09:54:42.157690Z","shell.execute_reply":"2025-05-24T09:54:43.660613Z"}},"outputs":[],"execution_count":244},{"cell_type":"code","source":"outputs.logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:55:36.039330Z","iopub.execute_input":"2025-05-24T09:55:36.039717Z","iopub.status.idle":"2025-05-24T09:55:36.046499Z","shell.execute_reply.started":"2025-05-24T09:55:36.039686Z","shell.execute_reply":"2025-05-24T09:55:36.045282Z"}},"outputs":[{"execution_count":247,"output_type":"execute_result","data":{"text/plain":"(1, 8, 2)"},"metadata":{}}],"execution_count":247},{"cell_type":"code","source":"class FlaxAlbertForQuestionAnsweringModule(nn.Module):\n    config: AlbertConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # 初始化 ALBERT 主干模型，不需要句向量，故关闭 pooling 层\n        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)\n        # 定义问答任务的输出层，输出 shape: [batch_size, seq_len, 2]（start 和 end logits）\n        self.qa_outputs = nn.Dense(self.config.num_labels, dtype=self.dtype)  # 通常 num_labels=2\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        deterministic: bool = True,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        # 调用 ALBERT 主干模型，获取 token 级别表示（hidden_states）\n        outputs = self.albert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            position_ids,\n            deterministic=deterministic, # 控制 dropout 等行为，训练=False，推理=True\n            output_attentions=output_attentions,   # 是否返回注意力权重\n            output_hidden_states=output_hidden_states,  # 是否返回隐藏层\n            return_dict=return_dict,    # 是否返回结构化结果\n        )\n\n        hidden_states = outputs[0]  # [batch_size, seq_len, hidden_size]\n         # 使用线性层计算每个 token 的起始/结束位置 logits，shape: [batch_size, seq_len, 2]\n        logits = self.qa_outputs(hidden_states)\n        # 拆分成 start 和 end logits，沿最后一个维度（2）拆分\n        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n        # 去除最后一维（即每个位置只保留一个标量）\n        start_logits = start_logits.squeeze(-1)  # shape: [batch_size, seq_len] \n        end_logits = end_logits.squeeze(-1)\n\n        if not return_dict: # 返回元组形式\n            return (start_logits, end_logits) + outputs[1:]\n\n        return FlaxQuestionAnsweringModelOutput(  # 返回结构化结果，包含 start/end logits、隐藏状态、注意力\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 设计意图说明：\n# 用于抽取式问答（extractive QA）任务，如 SQuAD。\n# 每个 token 都输出一个起始分数和一个结束分数，表示答案的区间。\n# 不需要句级特征，因此禁用了 pooling 层。\n# 整体结构为：ALBERT 编码器 + 线性输出层 + 拆分 start/end。该模式是问答任务的标准设计。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 继承自FlaxAlbertPreTrainedModel,这样就有了初始化权重,from_pretrained之类的基础方法\n# 模型类内部定义一个子模块,这样可以直接让子模块干活,自己只用做添加文档字符串的事情\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass FlaxAlbertForQuestionAnswering(FlaxAlbertPreTrainedModel):\n    module_class = FlaxAlbertForQuestionAnsweringModule","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:02:02.863640Z","iopub.execute_input":"2025-05-24T10:02:02.864044Z","iopub.status.idle":"2025-05-24T10:02:02.870159Z","shell.execute_reply.started":"2025-05-24T10:02:02.864018Z","shell.execute_reply":"2025-05-24T10:02:02.869258Z"}},"outputs":[],"execution_count":250},{"cell_type":"code","source":"append_call_sample_docstring(\n    FlaxAlbertForQuestionAnswering, # 问答任务\n    _CHECKPOINT_FOR_DOC,\n    FlaxQuestionAnsweringModelOutput, # 模型的输出类型\n    _CONFIG_FOR_DOC,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:02:31.052791Z","iopub.execute_input":"2025-05-24T10:02:31.053213Z","iopub.status.idle":"2025-05-24T10:02:31.059413Z","shell.execute_reply.started":"2025-05-24T10:02:31.053185Z","shell.execute_reply":"2025-05-24T10:02:31.057984Z"}},"outputs":[],"execution_count":251},{"cell_type":"code","source":"help(FlaxAlbertForQuestionAnswering.__call__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, FlaxAlbertForQuestionAnswering","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:03:18.703309Z","iopub.execute_input":"2025-05-24T10:03:18.703662Z","iopub.status.idle":"2025-05-24T10:03:18.708953Z","shell.execute_reply.started":"2025-05-24T10:03:18.703638Z","shell.execute_reply":"2025-05-24T10:03:18.707876Z"}},"outputs":[],"execution_count":253},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")\nmodel = FlaxAlbertForQuestionAnswering.from_pretrained(\"albert-xxlarge-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:03:30.114763Z","iopub.execute_input":"2025-05-24T10:03:30.115175Z","iopub.status.idle":"2025-05-24T10:03:37.615263Z","shell.execute_reply.started":"2025-05-24T10:03:30.115145Z","shell.execute_reply":"2025-05-24T10:03:37.614253Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing FlaxAlbertForQuestionAnswering: {('albert', 'pooler', 'kernel'), ('albert', 'pooler', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'LayerNorm', 'kernel')}\n- This IS expected if you are initializing FlaxAlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxAlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxAlbertForQuestionAnswering were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: {('qa_outputs', 'bias'), ('qa_outputs', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":254},{"cell_type":"code","source":"question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\ninputs = tokenizer(question, text, return_tensors=\"jax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:03:45.405584Z","iopub.execute_input":"2025-05-24T10:03:45.405920Z","iopub.status.idle":"2025-05-24T10:03:45.455454Z","shell.execute_reply.started":"2025-05-24T10:03:45.405878Z","shell.execute_reply":"2025-05-24T10:03:45.454600Z"}},"outputs":[],"execution_count":255},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:03:53.528161Z","iopub.execute_input":"2025-05-24T10:03:53.528685Z","iopub.status.idle":"2025-05-24T10:03:53.537069Z","shell.execute_reply.started":"2025-05-24T10:03:53.528651Z","shell.execute_reply":"2025-05-24T10:03:53.535963Z"}},"outputs":[{"execution_count":256,"output_type":"execute_result","data":{"text/plain":"{'input_ids': Array([[    2,    72,    23,  2170, 27674,    60,     3,  2170, 27674,\n           23,    21,  2210, 10956,     3]], dtype=int32), 'token_type_ids': Array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]], dtype=int32), 'attention_mask': Array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)}"},"metadata":{}}],"execution_count":256},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:04:06.136073Z","iopub.execute_input":"2025-05-24T10:04:06.136559Z","iopub.status.idle":"2025-05-24T10:04:09.509864Z","shell.execute_reply.started":"2025-05-24T10:04:06.136521Z","shell.execute_reply":"2025-05-24T10:04:09.508963Z"}},"outputs":[],"execution_count":257},{"cell_type":"code","source":"outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:04:13.509145Z","iopub.execute_input":"2025-05-24T10:04:13.509555Z","iopub.status.idle":"2025-05-24T10:04:13.517135Z","shell.execute_reply.started":"2025-05-24T10:04:13.509523Z","shell.execute_reply":"2025-05-24T10:04:13.516023Z"}},"outputs":[{"execution_count":258,"output_type":"execute_result","data":{"text/plain":"FlaxQuestionAnsweringModelOutput(start_logits=Array([[ 0.14820898,  0.16998863,  0.14983207, -0.24656601,  0.06165942,\n         0.06819007,  0.14820896, -0.23087387,  0.10601612,  0.2952439 ,\n         0.18761368, -0.05515826, -0.1330166 ,  0.14820883]],      dtype=float32), end_logits=Array([[-0.04195   ,  0.06987897,  0.00533415,  0.12311459, -0.23050275,\n        -0.04046968, -0.04195002,  0.0745064 , -0.0927614 , -0.06515643,\n        -0.19795206, -0.07101451, -0.08701305, -0.04194993]],      dtype=float32), hidden_states=None, attentions=None)"},"metadata":{}}],"execution_count":258},{"cell_type":"code","source":"start_scores = outputs.start_logits\nend_scores = outputs.end_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:04:29.468363Z","iopub.execute_input":"2025-05-24T10:04:29.468737Z","iopub.status.idle":"2025-05-24T10:04:29.475158Z","shell.execute_reply.started":"2025-05-24T10:04:29.468708Z","shell.execute_reply":"2025-05-24T10:04:29.473625Z"}},"outputs":[],"execution_count":259},{"cell_type":"code","source":"# 标记每个token是开始和结束的概率\nprint(start_scores.shape,end_scores.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:04:52.595874Z","iopub.execute_input":"2025-05-24T10:04:52.596329Z","iopub.status.idle":"2025-05-24T10:04:52.603830Z","shell.execute_reply.started":"2025-05-24T10:04:52.596296Z","shell.execute_reply":"2025-05-24T10:04:52.602359Z"}},"outputs":[{"name":"stdout","text":"(1, 14) (1, 14)\n","output_type":"stream"}],"execution_count":260},{"cell_type":"code","source":"__all__ = [\n    \"FlaxAlbertPreTrainedModel\",\n    \"FlaxAlbertModel\",\n    \"FlaxAlbertForPreTraining\",\n    \"FlaxAlbertForMaskedLM\",\n    \"FlaxAlbertForSequenceClassification\",\n    \"FlaxAlbertForMultipleChoice\",\n    \"FlaxAlbertForTokenClassification\",\n    \"FlaxAlbertForQuestionAnswering\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:05:47.788893Z","iopub.execute_input":"2025-05-24T10:05:47.789330Z","iopub.status.idle":"2025-05-24T10:05:47.794711Z","shell.execute_reply.started":"2025-05-24T10:05:47.789302Z","shell.execute_reply":"2025-05-24T10:05:47.793528Z"}},"outputs":[],"execution_count":261},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}