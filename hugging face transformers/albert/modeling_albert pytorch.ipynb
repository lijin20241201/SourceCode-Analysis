{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math  # 数学函数库，如sqrt、log等\nimport os # 操作系统接口，用于路径等操作\nfrom dataclasses import dataclass  # 简化数据类定义\nfrom typing import Dict, List, Optional, Tuple, Union,Callable # 类型注解支持\nimport torch  # PyTorch 主库\nfrom torch import nn # 神经网络模块，如Linear, Conv, etc.\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss  # 常用损失函数\nfrom transformers.activations import ACT2FN # 激活函数映射，如gelu、relu等\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa # 构建SDPA注意力mask\nfrom transformers.modeling_outputs import (  # HF定义的标准模型输出格式\n    BaseModelOutput,\n    BaseModelOutputWithPooling,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel  # HF所有模型的基类\nfrom transformers.pytorch_utils import ( # PyTorch相关实用函数\n    apply_chunking_to_forward,  # 前向分块处理\n    find_pruneable_heads_and_indices, # 找到可剪枝的头\n    is_torch_greater_or_equal_than_2_2, # 检查torch版本\n    prune_linear_layer,  # 剪枝线性层\n)\nfrom transformers.utils import ( # HF通用工具\n    ModelOutput,  # 所有模型输出的基类\n    add_code_sample_docstrings,  # 添加样例代码\n    add_start_docstrings, # 添加类或方法文档开头\n    add_start_docstrings_to_model_forward, # 给forward方法加文档注释\n    logging,  # 日志工具\n    replace_return_docstrings,  # 替换返回值文档说明\n)\nfrom transformers.models.albert.configuration_albert import AlbertConfig  # ALBERT模型的配置类","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:32.262519Z","iopub.execute_input":"2025-05-23T07:57:32.263551Z","iopub.status.idle":"2025-05-23T07:57:44.227445Z","shell.execute_reply.started":"2025-05-23T07:57:32.263511Z","shell.execute_reply":"2025-05-23T07:57:44.226151Z"}},"outputs":[{"name":"stderr","text":"2025-05-23 07:57:39.303812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747987059.335592      79 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747987059.344674      79 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"logger = logging.get_logger(__name__) # 获取日志类对象\n_CHECKPOINT_FOR_DOC = \"albert/albert-base-v2\"\n_CONFIG_FOR_DOC = \"AlbertConfig\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:44.229299Z","iopub.execute_input":"2025-05-23T07:57:44.229991Z","iopub.status.idle":"2025-05-23T07:57:44.235701Z","shell.execute_reply.started":"2025-05-23T07:57:44.229951Z","shell.execute_reply":"2025-05-23T07:57:44.234232Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 将 TensorFlow 训练的权重加载到 PyTorch 的 ALBERT 模型中\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n    ## 读取 TensorFlow checkpoint 中所有变量\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n     # 打印所有变量名（可注释掉）\n    for name, array in zip(names, arrays):\n        print(name)\n\n    for name, array in zip(names, arrays):\n        original_name = name\n\n        # 去掉 TF Hub 的前缀\n        name = name.replace(\"module/\", \"\")\n\n        # 各种命名替换以匹配 PyTorch 模型结构\n        name = name.replace(\"ffn_1\", \"ffn\")\n        name = name.replace(\"bert/\", \"albert/\")\n        name = name.replace(\"attention_1\", \"attention\")\n        name = name.replace(\"transform/\", \"\")\n        name = name.replace(\"LayerNorm_1\", \"full_layer_layer_norm\")\n        name = name.replace(\"LayerNorm\", \"attention/LayerNorm\")\n        name = name.replace(\"transformer/\", \"\")\n\n        # The feed forward layer had an 'intermediate' step which has been abstracted away\n        name = name.replace(\"intermediate/dense/\", \"\")\n        name = name.replace(\"ffn/intermediate/output/dense/\", \"ffn_output/\")\n\n        # ALBERT attention was split between self and output which have been abstracted away\n        name = name.replace(\"/output/\", \"/\")\n        name = name.replace(\"/self/\", \"/\")\n\n        # The pooler is a linear layer\n        name = name.replace(\"pooler/dense\", \"pooler\")\n\n        # The classifier was simplified to predictions from cls/predictions\n        name = name.replace(\"cls/predictions\", \"predictions\")\n        name = name.replace(\"predictions/attention\", \"predictions\")\n\n        # Naming was changed to be more explicit\n        name = name.replace(\"embeddings/attention\", \"embeddings\")\n        name = name.replace(\"inner_group_\", \"albert_layers/\")\n        name = name.replace(\"group_\", \"albert_layer_groups/\")\n\n        # 分类器权重特殊处理\n        if len(name.split(\"/\")) == 1 and (\"output_bias\" in name or \"output_weights\" in name):\n            name = \"classifier/\" + name\n\n        # No ALBERT model currently handles the next sentence prediction task\n        # SOP 任务（代替NSP）处理\n        if \"seq_relationship\" in name:\n            name = name.replace(\"seq_relationship/output_\", \"sop_classifier/classifier/\")\n            name = name.replace(\"weights\", \"weight\")\n\n        name = name.split(\"/\")\n\n        #  忽略优化器状态权重（如 Adam 的动量）\n        if (\n            \"adam_m\" in name\n            or \"adam_v\" in name\n            or \"AdamWeightDecayOptimizer\" in name\n            or \"AdamWeightDecayOptimizer_1\" in name\n            or \"global_step\" in name\n        ):\n            logger.info(f\"Skipping {'/'.join(name)}\")\n            continue\n        # 遍历模型层级，逐层定位对应的 PyTorch 参数\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name): # 形如 layer_11 的结构\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(f\"Skipping {'/'.join(name)}\")\n                    continue\n            if len(scope_names) >= 2:  # 索引到某个具体层\n                num = int(scope_names[1])\n                pointer = pointer[num]\n                \n        # 特殊处理嵌入层和 kernel 权重（转置）\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n         # 检查形状匹配\n        try:\n            if pointer.shape != array.shape:\n                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n        except ValueError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(f\"Initialize PyTorch weight {name} from {original_name}\")\n        pointer.data = torch.from_numpy(array)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:44.237131Z","iopub.execute_input":"2025-05-23T07:57:44.237501Z","iopub.status.idle":"2025-05-23T07:57:44.268295Z","shell.execute_reply.started":"2025-05-23T07:57:44.237470Z","shell.execute_reply":"2025-05-23T07:57:44.267089Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# .expand((1, -1)) 会返回一个新的 view（共享内存，不复制数据），将原始 shape (8,) 扩展为 shape (1, 8)。\n# 创建了一个新的 Tensor 对象\n# 该对象的 .data 部分指向相同的底层内存\ntorch.arange(8).expand((1, -1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:44.270196Z","iopub.execute_input":"2025-05-23T07:57:44.270510Z","iopub.status.idle":"2025-05-23T07:57:44.304262Z","shell.execute_reply.started":"2025-05-23T07:57:44.270486Z","shell.execute_reply":"2025-05-23T07:57:44.302888Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 1, 2, 3, 4, 5, 6, 7]])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"a = torch.arange(8)\nb = a.view(2, 4)  # 或 b = a.expand(1, -1)（如果合法）\nb[0][0] = 999\nprint(a)  # 原始 a 也会变，说明共享内存\nprint(b) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:47.745295Z","iopub.execute_input":"2025-05-23T07:57:47.745694Z","iopub.status.idle":"2025-05-23T07:57:47.754830Z","shell.execute_reply.started":"2025-05-23T07:57:47.745664Z","shell.execute_reply":"2025-05-23T07:57:47.753553Z"}},"outputs":[{"name":"stdout","text":"tensor([999,   1,   2,   3,   4,   5,   6,   7])\ntensor([[999,   1,   2,   3],\n        [  4,   5,   6,   7]])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"torch.zeros(8, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:50.433489Z","iopub.execute_input":"2025-05-23T07:57:50.433896Z","iopub.status.idle":"2025-05-23T07:57:50.444914Z","shell.execute_reply.started":"2025-05-23T07:57:50.433871Z","shell.execute_reply":"2025-05-23T07:57:50.443132Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([0, 0, 0, 0, 0, 0, 0, 0])"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import AlbertConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:54.299051Z","iopub.execute_input":"2025-05-23T07:57:54.299407Z","iopub.status.idle":"2025-05-23T07:57:54.305678Z","shell.execute_reply.started":"2025-05-23T07:57:54.299385Z","shell.execute_reply":"2025-05-23T07:57:54.304136Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 获取 config（自动识别为 AlbertConfig）\nconfig = AlbertConfig.from_pretrained(_CHECKPOINT_FOR_DOC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:56.519089Z","iopub.execute_input":"2025-05-23T07:57:56.519430Z","iopub.status.idle":"2025-05-23T07:57:56.772103Z","shell.execute_reply.started":"2025-05-23T07:57:56.519406Z","shell.execute_reply":"2025-05-23T07:57:56.770748Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:57:58.596064Z","iopub.execute_input":"2025-05-23T07:57:58.597187Z","iopub.status.idle":"2025-05-23T07:57:58.606333Z","shell.execute_reply.started":"2025-05-23T07:57:58.597139Z","shell.execute_reply":"2025-05-23T07:57:58.605055Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"AlbertConfig {\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 获取 model（一般是 Bert 类的基础模型结构）\n# model = AutoModel.from_pretrained(checkpoint, config=config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 构建来自词、位置、类型的嵌入层\nclass AlbertEmbeddings(nn.Module):    \n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n        # 词嵌入层（包含填充索引）\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)\n        # 位置嵌入层（绝对位置）\n        self.position_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.embedding_size)\n        # token 类型嵌入层（如区分句子对 A/B）\n        self.token_type_embeddings = nn.Embedding(\n            config.type_vocab_size, config.embedding_size)\n         # 层归一化（使用原始变量名以便兼容 TensorFlow 权重）\n        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        # dropout，用于正则化\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # # 注册绝对位置索引 buffer，不持久化保存（用于位置嵌入索引）\n        # PyTorch 中的 register_buffer 方法用于注册非参数性张量（不会作为 model.parameters() 返回，但会和模型\n        # 一起保存和加载，比如 .to(device) 时也会自动迁移）\n        # persistent=True（默认） 保存模型时也保存 buffer，比如位置编码。\n        # persistent=False 不希望保存，如：推理中可以重新生成、与权重无关的缓存型 buffer。\n        # 在 ALBERT 中position_ids 只是一个辅助张量（可在加载时重新生成），所以不需要写入模型文件。\n        self.register_buffer(\n            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n        )\n         # 设置位置嵌入方式（通常为\"absolute\"）\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        # 注册 token 类型索引 buffer，初始化为全 0\n        self.register_buffer(\n            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n        )\n\n    # 从 transformers 的 BERT 实现复制过来，用于生成词嵌入表示\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        past_key_values_length: int = 0,\n    ) -> torch.Tensor:\n        # 获取输入序列的 shape，input_ids 优先\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1] # 切片 (batch_size, seq_len)\n\n        seq_length = input_shape[1] \n        # 若未传入 position_ids，则从缓冲区中截取对应位置位置编码（支持 past_key_values_length）\n        # 为啥截取是从past_key_values开始？\n        # 这是为了支持 Transformer 解码器的增量生成（incremental generation），尤其是在 自回归推理（\n        # autoregressive inference） 中，处理 past_key_values 时的位置对齐问题。\n        # 当你进行 增量生成 时，比如已经生成了 5 个 token，接下来只要生成第 6 个，前面的 position embedding 不能\n        # 重新计算，新生成的 token 的位置应该是 5，而不是从 0 开始。\n        # 举例说明：\n        # seq_length = 1（当前只生成一个 token）\n        # past_key_values_length = 5（前面已生成了 5 个 token）\n        #  你总共的输出是 [Token_0, Token_1, ..., Token_5]，现在要生成第 6 个（即 Token_5）\n        # 那么你需要的位置编码是 position_ids = [5]，而不是从 0 开始。\n        # self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n        # 就变成：self.position_ids[:, 5:6]  # 只取第6个位置的 position id\n        # past_key_values_length 表示已经生成的 token 长度。\n        # 截取时从 past_key_values_length 开始，是为了让当前 token 对应的位置编码连续递增，与生成顺序一致。\n        # 这是为了兼容生成式模型在解码阶段逐 token 推理时的位置编码\n        if position_ids is None:\n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        # 若未传入 token_type_ids，尝试从已注册的缓冲区自动填充（通常为全0）\n        if token_type_ids is None: # 如果还没有设置token_type_ids\n            if hasattr(self, \"token_type_ids\"):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length] # 从缓冲区取前 seq_length 长度\n                # 扩展为 batch_size x seq_length\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n        # 若未传入预编码的 inputs_embeds，则从 input_ids 查表获得词向量\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        # 获取 token_type 的嵌入向量\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        # 融合词嵌入和 token_type 嵌入\n        embeddings = inputs_embeds + token_type_embeddings\n         # 如果是绝对位置编码，叠加位置嵌入\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        # 层归一化 + dropout\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:04.659791Z","iopub.execute_input":"2025-05-23T07:58:04.660270Z","iopub.status.idle":"2025-05-23T07:58:04.675314Z","shell.execute_reply.started":"2025-05-23T07:58:04.660236Z","shell.execute_reply":"2025-05-23T07:58:04.673815Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"albertEmbeddings=AlbertEmbeddings(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:09.187215Z","iopub.execute_input":"2025-05-23T07:58:09.187597Z","iopub.status.idle":"2025-05-23T07:58:09.244594Z","shell.execute_reply.started":"2025-05-23T07:58:09.187575Z","shell.execute_reply":"2025-05-23T07:58:09.243218Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"albertEmbeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:09.689867Z","iopub.execute_input":"2025-05-23T07:58:09.690315Z","iopub.status.idle":"2025-05-23T07:58:09.697157Z","shell.execute_reply.started":"2025-05-23T07:58:09.690282Z","shell.execute_reply":"2025-05-23T07:58:09.695872Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"AlbertEmbeddings(\n  (word_embeddings): Embedding(30000, 128, padding_idx=0)\n  (position_embeddings): Embedding(512, 128)\n  (token_type_embeddings): Embedding(2, 128)\n  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n  (dropout): Dropout(p=0, inplace=False)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"config.type_vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:12.612092Z","iopub.execute_input":"2025-05-23T07:58:12.612431Z","iopub.status.idle":"2025-05-23T07:58:12.619319Z","shell.execute_reply.started":"2025-05-23T07:58:12.612410Z","shell.execute_reply":"2025-05-23T07:58:12.618217Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# 词嵌入是为了让token具有数学表示,以用数学方法来建模","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dim=0是指调整layer的输出维度,dim=1是指调整layer的输入维度\ndef prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0) -> nn.Linear:\n    index = index.to(layer.weight.device)  # 将索引转移到与层权重相同的设备\n    # 选择权重矩阵的指定列或行（根据 dim 参数），并克隆得到新的权重 W\n    W = layer.weight.index_select(dim, index).clone().detach()\n    # 如果有偏置项，根据 dim 选择相应的部分并克隆\n    if layer.bias is not None:\n        if dim == 1:  # 偏置没有改变维度，只需克隆\n            b = layer.bias.clone().detach()\n        else: # 根据索引选择对应的偏置部分\n            b = layer.bias[index].clone().detach()\n    # 形容列表 [384, 512]\n    new_size = list(layer.weight.size())\n    # index是裁剪后的索引\n    # 将维度 dim 上的大小替换为被保留的索引数 len(index)，也就是一个具体的整数值\n    # 这里需要修改成剪枝后的形状,dim轴变成剪枝后的,其他轴不变\n    new_size[dim] = len(index) \n    # 构建新的线性层\n    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n    new_layer.weight.requires_grad = False # 需要修改权重,这里禁用梯度\n    new_layer.weight.copy_(W.contiguous()) # 复制表示\n    new_layer.weight.requires_grad = True # 重新启用梯度\n    if layer.bias is not None: # 如果需要偏距\n        new_layer.bias.requires_grad = False \n        new_layer.bias.copy_(b.contiguous())\n        new_layer.bias.requires_grad = True\n    return new_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:21.115942Z","iopub.execute_input":"2025-05-23T07:58:21.116315Z","iopub.status.idle":"2025-05-23T07:58:21.125281Z","shell.execute_reply.started":"2025-05-23T07:58:21.116290Z","shell.execute_reply":"2025-05-23T07:58:21.124249Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"aa=nn.Linear(512,384)\nprint(aa.weight.shape,aa.bias.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:23.593469Z","iopub.execute_input":"2025-05-23T07:58:23.593864Z","iopub.status.idle":"2025-05-23T07:58:23.603460Z","shell.execute_reply.started":"2025-05-23T07:58:23.593837Z","shell.execute_reply":"2025-05-23T07:58:23.602141Z"}},"outputs":[{"name":"stdout","text":"torch.Size([384, 512]) torch.Size([384])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"list(aa.weight.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:26.132491Z","iopub.execute_input":"2025-05-23T07:58:26.132872Z","iopub.status.idle":"2025-05-23T07:58:26.141317Z","shell.execute_reply.started":"2025-05-23T07:58:26.132847Z","shell.execute_reply":"2025-05-23T07:58:26.140219Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[384, 512]"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def find_pruneable_heads_and_indices(\n    heads: list[int], n_heads: int, head_size: int, already_pruned_heads: set[int]\n) -> tuple[set[int], torch.LongTensor]:\n   \n    mask = torch.ones(n_heads, head_size)\n    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n    for head in heads:\n        # Compute how many pruned heads are before the head and move the index accordingly\n        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n        mask[head] = 0\n    mask = mask.view(-1).contiguous().eq(1)\n    index: torch.LongTensor = torch.arange(len(mask))[mask].long()\n    return heads, index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:34.401894Z","iopub.execute_input":"2025-05-23T07:58:34.402399Z","iopub.status.idle":"2025-05-23T07:58:34.411429Z","shell.execute_reply.started":"2025-05-23T07:58:34.402366Z","shell.execute_reply":"2025-05-23T07:58:34.409780Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class AlbertAttention(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n         # 检查 hidden_size 是否能整除注意力头数，除非有 embedding_size（表明是 ALBERT 的共享结构）\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads  # 注意力头数量\n        self.hidden_size = config.hidden_size # 总的隐藏层维度\n        self.attention_head_size = config.hidden_size // config.num_attention_heads  # 每个头的维度\n        self.all_head_size = self.num_attention_heads * self.attention_head_size # 一般等于 hidden_size\n         # QKV 三个投影层\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        # attention score dropout\n        self.attention_dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        # attention输出后的dropout\n        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size) # 输出层的线性投影\n        # 残差连接后的 LayerNorm\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.pruned_heads = set() # 存储已经裁剪的头\n        # 位置信息类型（支持绝对或相对位置编码）\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        # 如果使用相对位置编码，则初始化距离嵌入\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding( # 注意：相对位置范围为 [-max+1, max-1]，共 2*max-1 个位置\n                2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n    # 将线性变换后的张量 x 转换为多头注意力所需的形状\n    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n        # 形状从 (batch, seq_len, all_head_size) → (batch, seq_len, num_heads, head_dim)\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(new_x_shape)\n        # 交换轴 → (batch, num_heads, seq_len, head_dim)，符合注意力计算要求\n        return x.permute(0, 2, 1, 3)\n        \n    # 剪枝指定的注意力头（可移除部分注意力头以加速推理）\n    def prune_heads(self, heads: List[int]) -> None:\n        if len(heads) == 0:\n            return\n        # 计算需要剪枝的头和对应的 index 索引（内部还会排除已经剪掉的头）\n        # 返回已经裁剪的头,和裁剪后的头索引\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads\n        )\n\n        # 对 QKV 线性层进行剪枝  修改权重参数等\n        self.query = prune_linear_layer(self.query, index) # 输出维度剪枝\n        self.key = prune_linear_layer(self.key, index)\n        self.value = prune_linear_layer(self.value, index)\n        self.dense = prune_linear_layer(self.dense, index, dim=1) # dense 层输入维度也要剪\n\n         # 更新头的数量和总维度\n        self.num_attention_heads = self.num_attention_heads - len(heads)\n        # 所有头的总表示\n        self.all_head_size = self.attention_head_size * self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)   # 记录已剪掉的头\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n        # 线性变换生成 QKV\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n        # 形状调整为 (batch, heads, seq_len, head_dim)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # 原始注意力打分 Q*K^T / sqrt(d)\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        \n        if attention_mask is not None:\n            # 应用注意力掩码（在 BertModel forward() 函数中为所有层预先计算\n            attention_scores = attention_scores + attention_mask\n        # 相对位置编码增强\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1] # 序列长度\n            # 构造位置索引矩阵，左边为行，右边为列，用于计算任意两个token间的相对位置偏移\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            # 得到相对距离矩阵（左 token 相对于右 token 的偏移量）\n            distance = position_ids_l - position_ids_r # shape: [seq_len, seq_len]\n            # 映射到 embedding 空间，distance_embedding 是一个类似 nn.Embedding 的查表操作\n            # 注意相对位置范围是 [-max_pos+1, max_pos-1]，需要平移为正整数索引\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            # 保持数据类型一致（避免 fp16 模式下精度冲突）\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 兼容性\n            # 本质上是 query 和“模拟 key 的相对位置信息”之间的点积，从作用上是在增强 key 的相对位置编码，因此得名 relative_key\n            if self.position_embedding_type == \"relative_key\":\n                # 仅 key 编码相对位置信息，类比于 Transformer-XL 的相对位置策略\n                # query 与相对位置向量点乘，表示当前 query 对各个距离的偏好\n                # einsum: 对于每个 head，query: [B, H, L, D], pos: [L, L, D] -> [B, H, L, L]\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                # 同时对 query 和 key 引入相对位置信息，是更对称和完整的相对位置编码方式\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                # 两部分分数加在一起形成对称的相对位置信息增强\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        # Softmax 得到注意力概率\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n        # 这实际上是丢弃需要处理的整个 token，这可能看起来有点不寻常，但它取自原始的 Transformer 论文\n        # Dropout 处理\n        attention_probs = self.attention_dropout(attention_probs)\n\n        # 如果我们愿意的话,Mask heads\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n        # 加权求和得到上下文表示\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.transpose(2, 1).flatten(2)  # 恢复为原始维度\n        # 线性映射 + Dropout + 残差连接 + LayerNorm\n        projected_context_layer = self.dense(context_layer)\n        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n        return (layernormed_context_layer, attention_probs) if output_attentions else (layernormed_context_layer,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:38.573522Z","iopub.execute_input":"2025-05-23T07:58:38.573971Z","iopub.status.idle":"2025-05-23T07:58:38.599325Z","shell.execute_reply.started":"2025-05-23T07:58:38.573920Z","shell.execute_reply":"2025-05-23T07:58:38.597837Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"position_ids_l =torch.arange(8, dtype=torch.long).view(-1, 1) # (8,1) 8:seq_length\nposition_ids_r = torch.arange(8, dtype=torch.long).view(1, -1) # (1,8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:44.680207Z","iopub.execute_input":"2025-05-23T07:58:44.680839Z","iopub.status.idle":"2025-05-23T07:58:44.688394Z","shell.execute_reply.started":"2025-05-23T07:58:44.680801Z","shell.execute_reply":"2025-05-23T07:58:44.686878Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"position_ids_r","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:45.207699Z","iopub.execute_input":"2025-05-23T07:58:45.208025Z","iopub.status.idle":"2025-05-23T07:58:45.216325Z","shell.execute_reply.started":"2025-05-23T07:58:45.208004Z","shell.execute_reply":"2025-05-23T07:58:45.215047Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 1, 2, 3, 4, 5, 6, 7]])"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"distance = position_ids_l - position_ids_r","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:58:56.002194Z","iopub.execute_input":"2025-05-23T07:58:56.002558Z","iopub.status.idle":"2025-05-23T07:58:56.008738Z","shell.execute_reply.started":"2025-05-23T07:58:56.002534Z","shell.execute_reply":"2025-05-23T07:58:56.007212Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"attention=AlbertAttention(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:11.419580Z","iopub.execute_input":"2025-05-23T07:59:11.419978Z","iopub.status.idle":"2025-05-23T07:59:11.455130Z","shell.execute_reply.started":"2025-05-23T07:59:11.419955Z","shell.execute_reply":"2025-05-23T07:59:11.454176Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"distance\n# 每一行 i 表示：第 i 个 token 和所有其他 token 的相对距离。\n# distance[2][0] = 2 - 0 = 2，表示第3个 token 距离第1个 token 向右偏移2个位置\n# distance[2][3] = 2 - 3 = -1，表示第3个 token 距离第4个 token 向左偏移1个位置\n# 这段代码是为了计算序列中所有 token 对所有其他 token 的相对位置，也就是一个完整的 [L, L] 相对位置矩阵，\n# 用于构建 attention 中的相对位置打分。\n# 不是仅限相邻 token，恰恰相反，是对所有 token 两两之间都建模。","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:22.864612Z","iopub.execute_input":"2025-05-23T07:59:22.865587Z","iopub.status.idle":"2025-05-23T07:59:22.873171Z","shell.execute_reply.started":"2025-05-23T07:59:22.865557Z","shell.execute_reply":"2025-05-23T07:59:22.872210Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0, -1, -2, -3, -4, -5, -6, -7],\n        [ 1,  0, -1, -2, -3, -4, -5, -6],\n        [ 2,  1,  0, -1, -2, -3, -4, -5],\n        [ 3,  2,  1,  0, -1, -2, -3, -4],\n        [ 4,  3,  2,  1,  0, -1, -2, -3],\n        [ 5,  4,  3,  2,  1,  0, -1, -2],\n        [ 6,  5,  4,  3,  2,  1,  0, -1],\n        [ 7,  6,  5,  4,  3,  2,  1,  0]])"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"distance.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:26.534861Z","iopub.execute_input":"2025-05-23T07:59:26.535859Z","iopub.status.idle":"2025-05-23T07:59:26.541885Z","shell.execute_reply.started":"2025-05-23T07:59:26.535823Z","shell.execute_reply":"2025-05-23T07:59:26.540750Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"torch.Size([8, 8])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"config.position_embedding_type =\"relative_key\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:28.736762Z","iopub.execute_input":"2025-05-23T07:59:28.737116Z","iopub.status.idle":"2025-05-23T07:59:28.743122Z","shell.execute_reply.started":"2025-05-23T07:59:28.737095Z","shell.execute_reply":"2025-05-23T07:59:28.741253Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"attention=AlbertAttention(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:36.031774Z","iopub.execute_input":"2025-05-23T07:59:36.032210Z","iopub.status.idle":"2025-05-23T07:59:36.070452Z","shell.execute_reply.started":"2025-05-23T07:59:36.032179Z","shell.execute_reply":"2025-05-23T07:59:36.068913Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"attention.max_position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:40.290425Z","iopub.execute_input":"2025-05-23T07:59:40.290857Z","iopub.status.idle":"2025-05-23T07:59:40.299202Z","shell.execute_reply.started":"2025-05-23T07:59:40.290829Z","shell.execute_reply":"2025-05-23T07:59:40.297834Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"distance + attention.max_position_embeddings - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:42.393609Z","iopub.execute_input":"2025-05-23T07:59:42.394003Z","iopub.status.idle":"2025-05-23T07:59:42.401942Z","shell.execute_reply.started":"2025-05-23T07:59:42.393969Z","shell.execute_reply":"2025-05-23T07:59:42.401190Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor([[511, 510, 509, 508, 507, 506, 505, 504],\n        [512, 511, 510, 509, 508, 507, 506, 505],\n        [513, 512, 511, 510, 509, 508, 507, 506],\n        [514, 513, 512, 511, 510, 509, 508, 507],\n        [515, 514, 513, 512, 511, 510, 509, 508],\n        [516, 515, 514, 513, 512, 511, 510, 509],\n        [517, 516, 515, 514, 513, 512, 511, 510],\n        [518, 517, 516, 515, 514, 513, 512, 511]])"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"attention.distance_embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:45.751341Z","iopub.execute_input":"2025-05-23T07:59:45.751708Z","iopub.status.idle":"2025-05-23T07:59:45.759026Z","shell.execute_reply.started":"2025-05-23T07:59:45.751683Z","shell.execute_reply":"2025-05-23T07:59:45.757853Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Embedding(1023, 64)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"positional_embedding = attention.distance_embedding(distance + attention.max_position_embeddings - 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:52.214002Z","iopub.execute_input":"2025-05-23T07:59:52.215338Z","iopub.status.idle":"2025-05-23T07:59:52.221515Z","shell.execute_reply.started":"2025-05-23T07:59:52.215292Z","shell.execute_reply":"2025-05-23T07:59:52.220025Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"positional_embedding.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:55.109262Z","iopub.execute_input":"2025-05-23T07:59:55.109680Z","iopub.status.idle":"2025-05-23T07:59:55.119257Z","shell.execute_reply.started":"2025-05-23T07:59:55.109615Z","shell.execute_reply":"2025-05-23T07:59:55.117518Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"torch.Size([8, 8, 64])"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"heads=[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T07:59:57.366546Z","iopub.execute_input":"2025-05-23T07:59:57.367124Z","iopub.status.idle":"2025-05-23T07:59:57.373071Z","shell.execute_reply.started":"2025-05-23T07:59:57.367087Z","shell.execute_reply":"2025-05-23T07:59:57.371552Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"heads, index=find_pruneable_heads_and_indices(heads,attention.num_attention_heads,attention.attention_head_size,attention.pruned_heads)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:03.524880Z","iopub.execute_input":"2025-05-23T08:00:03.525988Z","iopub.status.idle":"2025-05-23T08:00:03.531718Z","shell.execute_reply.started":"2025-05-23T08:00:03.525946Z","shell.execute_reply":"2025-05-23T08:00:03.530605Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"query = prune_linear_layer(attention.query, index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:11.582256Z","iopub.execute_input":"2025-05-23T08:00:11.582568Z","iopub.status.idle":"2025-05-23T08:00:11.594616Z","shell.execute_reply.started":"2025-05-23T08:00:11.582548Z","shell.execute_reply":"2025-05-23T08:00:11.593700Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"query","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:14.066829Z","iopub.execute_input":"2025-05-23T08:00:14.067302Z","iopub.status.idle":"2025-05-23T08:00:14.075749Z","shell.execute_reply.started":"2025-05-23T08:00:14.067271Z","shell.execute_reply":"2025-05-23T08:00:14.074321Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Linear(in_features=768, out_features=704, bias=True)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"is_torch_greater_or_equal_than_2_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:17.303495Z","iopub.execute_input":"2025-05-23T08:00:17.304377Z","iopub.status.idle":"2025-05-23T08:00:17.311295Z","shell.execute_reply.started":"2025-05-23T08:00:17.304335Z","shell.execute_reply":"2025-05-23T08:00:17.310027Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"torch.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:20.320838Z","iopub.execute_input":"2025-05-23T08:00:20.321349Z","iopub.status.idle":"2025-05-23T08:00:20.330145Z","shell.execute_reply.started":"2025-05-23T08:00:20.321315Z","shell.execute_reply":"2025-05-23T08:00:20.328733Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'2.6.0+cu124'"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"class AlbertSdpaAttention(AlbertAttention):\n    def __init__(self, config):\n        super().__init__(config)\n        self.dropout_prob = config.attention_probs_dropout_prob\n        # 在 torch < 2.2 的环境中，SDPA 对非连续 QKV 输入+mask 有 bug，需要特殊处理\n        self.require_contiguous_qkv = not is_torch_greater_or_equal_than_2_2\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n         # 如果存在非绝对位置编码、要求输出注意力、或应用 head mask，则 fallback 到原始（eager）实现\n        # torch.nn.functional.scaled_dot_product_attention（SDPA）是 PyTorch >= 2.0 引入的内建高性能 \n        # attention 实现，但它不支持相对位置编码（如 Transformer-XL 或 T5 的策略）、head mask、或返回 attention weights。\n        # 因此这段逻辑检测是否能使用 SDPA，否则 fallback 到手动实现。\n        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n            logger.warning(\n                \"AlbertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n                \"the eager attention implementation, but specifying the eager implementation will be required from \"\n                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n                '`attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(hidden_states, attention_mask, head_mask, output_attentions)\n\n        batch_size, seq_len, _ = hidden_states.size()\n        query_layer = self.transpose_for_scores(self.query(hidden_states)) # shape: [B, H, L, D]\n        key_layer = self.transpose_for_scores(self.key(hidden_states))\n        value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n        # Reference: https://github.com/pytorch/pytorch/issues/112577\n        # 在旧版本 PyTorch（2.1.2 及以下）上，GPU 上 SDPA + mask + 非contiguous tensor 会导致错误或性能退化，因\n        # 此做了兼容性判断，显式 .contiguous()。\n        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n            query_layer = query_layer.contiguous()\n            key_layer = key_layer.contiguous()\n            value_layer = value_layer.contiguous()\n        # 使用 PyTorch 内置的高效 SDPA 算子进行注意力计算：\n        # 自动做 qk^T / sqrt(d)、softmax、dropout、softmax @ v，更高效\n        # 由于 SDPA 内部是 fused kernel，可以显著减少内存访问开销和中间态保存\n        attention_output = torch.nn.functional.scaled_dot_product_attention(\n            query=query_layer,\n            key=key_layer,\n            value=value_layer,\n            attn_mask=attention_mask, # 已处理好的 mask，一般 shape: [B, 1, 1, L]\n            dropout_p=self.dropout_prob if self.training else 0.0,\n            is_causal=False,  # 非自回归\n        )\n\n        attention_output = attention_output.transpose(1, 2) # 还原维度顺序：[B, L, H, D]\n        attention_output = attention_output.reshape(batch_size, seq_len, self.all_head_size) # 合并头\n        # Transformer 模块尾部标准操作：dropout →残差连接 →  LayerNorm。\n        projected_context_layer = self.dense(attention_output)\n        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n        return (layernormed_context_layer,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:25.052216Z","iopub.execute_input":"2025-05-23T08:00:25.052606Z","iopub.status.idle":"2025-05-23T08:00:25.066920Z","shell.execute_reply.started":"2025-05-23T08:00:25.052579Z","shell.execute_reply":"2025-05-23T08:00:25.065265Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# AlbertSdpaAttention 是为了在支持条件下替换掉原本的手写 attention 逻辑，使用 PyTorch 官方 fused 实现 \n# scaled_dot_product_attention，带来性能优势，同时保留对旧特性的 fallback 兼容处理。\n# 是否采用 SDPA 主要受以下三者限制：\n# 必须是 absolute 位置编码\n# 不要求 output_attentions\n# 不应用 head_mask\n# 这是 HuggingFace 正在逐步推动的优化方向，未来（如 v5.0.0）将需要显式指定是否使用 eager 实现。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# is_causal 参数控制 注意力是否为因果注意力（causal attention），即是否阻止当前 token 看到未来 token。\n# is_causal = True 的情况（自回归模式）：\n# 用于 语言模型推理、生成任务、训练 autoregressive 模型，如：\n# GPT 系列（GPT-2, GPT-3, GPT-4 等）\n# Transformer Decoder\n# LLM 推理或训练阶段\n# 目的是确保：\n# 第 i 个 token 只能看到前 i 个位置，不能“偷看”未来 token。\n# 🔁 等效于注意力 mask 为下三角矩阵。\n# ✅ is_causal = False 的情况（非自回归，双向注意力）：\n# 用于 编码器模型或非生成任务，如：\n# BERT、RoBERTa、ALBERT\n# 任何 encoder-only 架构\n# classification / embedding / span prediction 等任务\n# 这种情况可以自由看到整个输入序列，不加因果遮挡。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ALBERT_ATTENTION_CLASSES = {\n    \"eager\": AlbertAttention,\n    \"sdpa\": AlbertSdpaAttention,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:28.700590Z","iopub.execute_input":"2025-05-23T08:00:28.701014Z","iopub.status.idle":"2025-05-23T08:00:28.706482Z","shell.execute_reply.started":"2025-05-23T08:00:28.700988Z","shell.execute_reply":"2025-05-23T08:00:28.705319Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"config._attn_implementation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:30.660232Z","iopub.execute_input":"2025-05-23T08:00:30.660672Z","iopub.status.idle":"2025-05-23T08:00:30.669143Z","shell.execute_reply.started":"2025-05-23T08:00:30.660617Z","shell.execute_reply":"2025-05-23T08:00:30.667502Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'eager'"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"config.hidden_act","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:32.755582Z","iopub.execute_input":"2025-05-23T08:00:32.756010Z","iopub.status.idle":"2025-05-23T08:00:32.763009Z","shell.execute_reply.started":"2025-05-23T08:00:32.755984Z","shell.execute_reply":"2025-05-23T08:00:32.761984Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'gelu_new'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"import inspect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:00:34.881020Z","iopub.execute_input":"2025-05-23T08:00:34.881440Z","iopub.status.idle":"2025-05-23T08:00:34.887315Z","shell.execute_reply.started":"2025-05-23T08:00:34.881411Z","shell.execute_reply":"2025-05-23T08:00:34.885966Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# 该函数用于将输入张量按指定维度 `chunk_dim` 分块，每块大小为 `chunk_size`，对每块分别调用 forward_fn，最后拼接结果。\n# 目的是减少单次计算的内存占用，特别适用于 FFN 这类对每个 token 独立处理的函数（位置无关）。\n# 设计意图：\n# - 避免显存溢出：尤其是在处理长序列或大模型时，显存成为瓶颈。\n# - 不改变计算结果：在 `forward_fn` 不依赖序列上下文的前提下，chunk 后结果与原计算一致。\ndef apply_chunking_to_forward(\n    forward_fn: Callable[..., torch.Tensor],\n    chunk_size: int,\n    chunk_dim: int,\n    *input_tensors,\n) -> torch.Tensor:\n    \"\"\"\n    Examples:\n\n    ```python\n    # rename the usual forward() fn to forward_chunk()\n    def forward_chunk(self, hidden_states):\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\n\n    # implement a chunked forward function\n    def forward(self, hidden_states):\n        return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n    ```\"\"\"\n    # input_tensors必须是一个元组或列表的张量\n    assert len(input_tensors) > 0, f\"{input_tensors} has to be a tuple/list of tensors\"\n    # inspect.signature 自 python 3.5 以来就存在，并且是一个 python 方法 -> 向后兼容没有问题\n     # 校验 forward_fn 的参数数量必须与输入张量数量一致，否则报错\n    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n    # print(num_args_in_forward_chunk_fn,len(input_tensors)) 1 1\n    if num_args_in_forward_chunk_fn != len(input_tensors):\n        raise ValueError(\n            f\"forward_chunk_fn expects {num_args_in_forward_chunk_fn} arguments, but only {len(input_tensors)} input \"\n            \"tensors are given\"\n        )\n    # print(chunk_size,chunk_dim) 0 1 # 在序列上分块\n    if chunk_size > 0:\n         # 获取要分块维度的长度,就是序列维度的时间步长度\n        tensor_shape = input_tensors[0].shape[chunk_dim]\n         # 遍历批次内的每个输入tensor\n        for input_tensor in input_tensors:\n            # 确保整个批次中每个样本的序列长度一致\n            if input_tensor.shape[chunk_dim] != tensor_shape:\n                raise ValueError(\n                    f\"All input tenors have to be of the same shape: {tensor_shape}, \"\n                    f\"found shape {input_tensor.shape[chunk_dim]}\"\n                )\n        # 只有批次内样本的序列长度是单位块的整数倍才能分块\n        if input_tensors[0].shape[chunk_dim] % chunk_size != 0:\n            raise ValueError(\n                f\"The dimension to be chunked {input_tensors[0].shape[chunk_dim]} has to be a multiple of the chunk \"\n                f\"size {chunk_size}\"\n            )\n        # 按块大小来对序列长度维度分块\n        num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n        \n         # 对批次内每个样本在序列轴分块 \n        input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n        # zip 每组 chunk（同一位置），逐组调用 forward_fn，生成对应的输出块\n        output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n        # print(type(output_chunks)) <class 'tuple'>\n        # 5 torch.Size([2, 2, 768])\n        # print(len(output_chunks),output_chunks[0].shape)\n        # 最终在相同维度拼接所有输出块，还原成完整输出张量\n        return torch.cat(output_chunks, dim=chunk_dim)\n    # 如果 chunk_size 不合法，直接正常调用 forward_fn，不分块\n    return forward_fn(*input_tensors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:19:46.955006Z","iopub.execute_input":"2025-05-23T11:19:46.955429Z","iopub.status.idle":"2025-05-23T11:19:46.968173Z","shell.execute_reply.started":"2025-05-23T11:19:46.955402Z","shell.execute_reply":"2025-05-23T11:19:46.966547Z"}},"outputs":[],"execution_count":225},{"cell_type":"code","source":"# 该类是 ALBERT Transformer block 的核心组成：Attention + FeedForward + Residual + LayerNorm。与 BERT 的结\n# 构一致，只是参数共享（在后续模块体现）\nclass AlbertLayer(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n        self.config = config # 保存配置，用于后续模块构建或forward中使用\n        # Feed-Forward模块支持chunk方式处理长序列以节省显存\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1  # 表示序列维度的索引（B, L, D中的L）\n        # 层输出的最终 LayerNorm（Residual + FFN/Attention 后\n        self.full_layer_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         # 注意力子层（标准或SDPA），从实现类字典中按实现方式选择（如标准/eager/sdpa等）\n        self.attention = ALBERT_ATTENTION_CLASSES[config._attn_implementation](config)\n        # 前馈网络第一层（输入 -> 中间层），线性变换后再激活\n        self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)\n        # 前馈网络第二层（中间层 -> 输出），将维度映射回 hidden_size\n        self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.activation = ACT2FN[config.hidden_act] # 激活函数（如 gelu、relu 等），根据配置设定\n        self.dropout = nn.Dropout(config.hidden_dropout_prob) # dropout，用于前馈和残差路径防止过拟合\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         # 调用注意力模块，返回 attention_output 和（可选）注意力分数\n        # attention_output 是 residual add 之前的结果，维度 [B, L, D]\n        attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n        # 对 FFN 层使用 chunk 机制处理长序列，节省显存\n        # 实际执行的是 self.ff_chunk(attention_output[0])，其中 attention_output[0] 是上下文表示\n        ffn_output = apply_chunking_to_forward(\n            self.ff_chunk,\n            self.chunk_size_feed_forward,\n            # 2,\n            self.seq_len_dim,\n            attention_output[0],\n        )\n        # print(type(ffn_output))\n        # print(ffn_output.shape) torch.Size([2, 10, 768])\n        # 残差连接 + LayerNorm，形成该层最终输出,attention_output[0]是hidden_states\n        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])\n        # 返回当前层输出 + 可选的注意力权重（注意这里的 attention_output 是元组）\n        return (hidden_states,) + attention_output[1:]  # add attentions if we output them\n\n    def ff_chunk(self, attention_output: torch.Tensor) -> torch.Tensor:\n        ffn_output = self.ffn(attention_output)  # 前馈网络第一层：线性变换提升维度\n        ffn_output = self.activation(ffn_output) # 激活函数（如 gelu）引入非线性\n        ffn_output = self.ffn_output(ffn_output)  # 前馈网络第二层：映射回原始 hidden_size 维度\n        return ffn_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:19:57.212363Z","iopub.execute_input":"2025-05-23T11:19:57.212790Z","iopub.status.idle":"2025-05-23T11:19:57.225058Z","shell.execute_reply.started":"2025-05-23T11:19:57.212763Z","shell.execute_reply":"2025-05-23T11:19:57.223925Z"}},"outputs":[],"execution_count":227},{"cell_type":"code","source":"albertLayer=AlbertLayer(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:09.293038Z","iopub.execute_input":"2025-05-23T08:37:09.293398Z","iopub.status.idle":"2025-05-23T08:37:09.387161Z","shell.execute_reply.started":"2025-05-23T08:37:09.293376Z","shell.execute_reply":"2025-05-23T08:37:09.386077Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"albertLayer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:11.470749Z","iopub.execute_input":"2025-05-23T08:37:11.471148Z","iopub.status.idle":"2025-05-23T08:37:11.479860Z","shell.execute_reply.started":"2025-05-23T08:37:11.471125Z","shell.execute_reply":"2025-05-23T08:37:11.478470Z"}},"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"AlbertLayer(\n  (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (attention): AlbertAttention(\n    (query): Linear(in_features=768, out_features=768, bias=True)\n    (key): Linear(in_features=768, out_features=768, bias=True)\n    (value): Linear(in_features=768, out_features=768, bias=True)\n    (attention_dropout): Dropout(p=0, inplace=False)\n    (output_dropout): Dropout(p=0, inplace=False)\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (distance_embedding): Embedding(1023, 64)\n  )\n  (ffn): Linear(in_features=768, out_features=3072, bias=True)\n  (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n  (activation): NewGELUActivation()\n  (dropout): Dropout(p=0, inplace=False)\n)"},"metadata":{}}],"execution_count":154},{"cell_type":"code","source":"albertLayer.chunk_size_feed_forward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:14.259135Z","iopub.execute_input":"2025-05-23T08:37:14.259594Z","iopub.status.idle":"2025-05-23T08:37:14.267928Z","shell.execute_reply.started":"2025-05-23T08:37:14.259566Z","shell.execute_reply":"2025-05-23T08:37:14.266477Z"}},"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":155},{"cell_type":"code","source":"aa=torch.randn((2,10,768))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:16.460876Z","iopub.execute_input":"2025-05-23T08:37:16.461252Z","iopub.status.idle":"2025-05-23T08:37:16.467728Z","shell.execute_reply.started":"2025-05-23T08:37:16.461228Z","shell.execute_reply":"2025-05-23T08:37:16.466473Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"aa[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:18.615902Z","iopub.execute_input":"2025-05-23T08:37:18.617069Z","iopub.status.idle":"2025-05-23T08:37:18.624216Z","shell.execute_reply.started":"2025-05-23T08:37:18.617031Z","shell.execute_reply":"2025-05-23T08:37:18.623069Z"}},"outputs":[{"execution_count":157,"output_type":"execute_result","data":{"text/plain":"torch.Size([10, 768])"},"metadata":{}}],"execution_count":157},{"cell_type":"code","source":"aav=tuple(cc.chunk(2, dim=0) for cc in aa)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:20.912356Z","iopub.execute_input":"2025-05-23T08:37:20.912802Z","iopub.status.idle":"2025-05-23T08:37:20.920797Z","shell.execute_reply.started":"2025-05-23T08:37:20.912773Z","shell.execute_reply":"2025-05-23T08:37:20.919360Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"print(type(albertLayer(aa)),len(albertLayer(aa)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:37:23.128688Z","iopub.execute_input":"2025-05-23T08:37:23.129116Z","iopub.status.idle":"2025-05-23T08:37:23.173433Z","shell.execute_reply.started":"2025-05-23T08:37:23.129090Z","shell.execute_reply":"2025-05-23T08:37:23.172308Z"}},"outputs":[{"name":"stdout","text":"5 torch.Size([2, 2, 768])\n5 torch.Size([2, 2, 768])\n<class 'tuple'> 1\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"aav[0][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:39:21.740538Z","iopub.execute_input":"2025-05-23T08:39:21.740973Z","iopub.status.idle":"2025-05-23T08:39:21.748688Z","shell.execute_reply.started":"2025-05-23T08:39:21.740947Z","shell.execute_reply":"2025-05-23T08:39:21.747567Z"}},"outputs":[{"execution_count":162,"output_type":"execute_result","data":{"text/plain":"torch.Size([5, 768])"},"metadata":{}}],"execution_count":162},{"cell_type":"code","source":"print(len(aav),type(aav[0]),len(aav[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:39:24.227445Z","iopub.execute_input":"2025-05-23T08:39:24.227944Z","iopub.status.idle":"2025-05-23T08:39:24.238769Z","shell.execute_reply.started":"2025-05-23T08:39:24.227899Z","shell.execute_reply":"2025-05-23T08:39:24.236763Z"}},"outputs":[{"name":"stdout","text":"2 <class 'tuple'> 2\n","output_type":"stream"}],"execution_count":163},{"cell_type":"code","source":"albertLayer(aa)[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:39:29.474137Z","iopub.execute_input":"2025-05-23T08:39:29.474527Z","iopub.status.idle":"2025-05-23T08:39:29.505082Z","shell.execute_reply.started":"2025-05-23T08:39:29.474503Z","shell.execute_reply":"2025-05-23T08:39:29.504036Z"}},"outputs":[{"name":"stdout","text":"5 torch.Size([2, 2, 768])\n","output_type":"stream"},{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 10, 768])"},"metadata":{}}],"execution_count":164},{"cell_type":"code","source":"albertLayer(aa)[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:40:06.537688Z","iopub.execute_input":"2025-05-23T08:40:06.538082Z","iopub.status.idle":"2025-05-23T08:40:06.581517Z","shell.execute_reply.started":"2025-05-23T08:40:06.538058Z","shell.execute_reply":"2025-05-23T08:40:06.580261Z"}},"outputs":[{"name":"stdout","text":"5 torch.Size([2, 2, 768])\n","output_type":"stream"},{"execution_count":166,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 10, 768])"},"metadata":{}}],"execution_count":166},{"cell_type":"code","source":"[_ for _ in range(config.inner_group_num)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:01:14.607280Z","iopub.execute_input":"2025-05-23T09:01:14.607690Z","iopub.status.idle":"2025-05-23T09:01:14.615241Z","shell.execute_reply.started":"2025-05-23T09:01:14.607665Z","shell.execute_reply":"2025-05-23T09:01:14.614147Z"}},"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"[0]"},"metadata":{}}],"execution_count":172},{"cell_type":"code","source":"nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:01:18.494107Z","iopub.execute_input":"2025-05-23T09:01:18.494488Z","iopub.status.idle":"2025-05-23T09:01:18.597550Z","shell.execute_reply.started":"2025-05-23T09:01:18.494462Z","shell.execute_reply":"2025-05-23T09:01:18.596371Z"}},"outputs":[{"execution_count":173,"output_type":"execute_result","data":{"text/plain":"ModuleList(\n  (0): AlbertLayer(\n    (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (attention): AlbertAttention(\n      (query): Linear(in_features=768, out_features=768, bias=True)\n      (key): Linear(in_features=768, out_features=768, bias=True)\n      (value): Linear(in_features=768, out_features=768, bias=True)\n      (attention_dropout): Dropout(p=0, inplace=False)\n      (output_dropout): Dropout(p=0, inplace=False)\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (distance_embedding): Embedding(1023, 64)\n    )\n    (ffn): Linear(in_features=768, out_features=3072, bias=True)\n    (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n    (activation): NewGELUActivation()\n    (dropout): Dropout(p=0, inplace=False)\n  )\n)"},"metadata":{}}],"execution_count":173},{"cell_type":"code","source":"class AlbertLayerGroup(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n        # 创建一个子层列表，包含 inner_group_num 个 AlbertLayer 层\n        # ALBERT 的参数共享特性体现在多个组之间共享该组的层结构\n        self.albert_layers = nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        layer_hidden_states = ()  # 用于收集每一层的隐藏状态（如果需要）\n        layer_attentions = ()  # 用于收集每一层的注意力权重（如果需要）\n        # 遍历本组内的每一层（通常1层，多组共享）\n        for layer_index, albert_layer in enumerate(self.albert_layers):\n             # 调用当前层的前向传播\n            layer_output = albert_layer(\n                hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n            hidden_states = layer_output[0] # 在迭代内更新 hidden_states 以供下一层使用\n            # 收集注意力权重\n            if output_attentions:\n                layer_attentions = layer_attentions + (layer_output[1],)\n             # 收集中间隐藏状态\n            if output_hidden_states:\n                layer_hidden_states = layer_hidden_states + (hidden_states,)\n        # 最终输出构成：当前组最后一层的输出 + 可选的中间隐藏状态 + 可选的注意力权重\n        outputs = (hidden_states,)\n        if output_hidden_states:\n            outputs = outputs + (layer_hidden_states,)\n        if output_attentions:\n            outputs = outputs + (layer_attentions,)\n        return outputs  # last-layer hidden state, (layer hidden states), (layer attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:05:39.234206Z","iopub.execute_input":"2025-05-23T09:05:39.234666Z","iopub.status.idle":"2025-05-23T09:05:39.249863Z","shell.execute_reply.started":"2025-05-23T09:05:39.234607Z","shell.execute_reply":"2025-05-23T09:05:39.248882Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"layerGroup=AlbertLayerGroup(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:37.345688Z","iopub.execute_input":"2025-05-23T09:07:37.346064Z","iopub.status.idle":"2025-05-23T09:07:37.438553Z","shell.execute_reply.started":"2025-05-23T09:07:37.346031Z","shell.execute_reply":"2025-05-23T09:07:37.437388Z"}},"outputs":[],"execution_count":175},{"cell_type":"code","source":"layerGroup.albert_layers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:08:16.196012Z","iopub.execute_input":"2025-05-23T09:08:16.196337Z","iopub.status.idle":"2025-05-23T09:08:16.204833Z","shell.execute_reply.started":"2025-05-23T09:08:16.196316Z","shell.execute_reply":"2025-05-23T09:08:16.203727Z"}},"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"AlbertLayer(\n  (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (attention): AlbertAttention(\n    (query): Linear(in_features=768, out_features=768, bias=True)\n    (key): Linear(in_features=768, out_features=768, bias=True)\n    (value): Linear(in_features=768, out_features=768, bias=True)\n    (attention_dropout): Dropout(p=0, inplace=False)\n    (output_dropout): Dropout(p=0, inplace=False)\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (distance_embedding): Embedding(1023, 64)\n  )\n  (ffn): Linear(in_features=768, out_features=3072, bias=True)\n  (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n  (activation): NewGELUActivation()\n  (dropout): Dropout(p=0, inplace=False)\n)"},"metadata":{}}],"execution_count":178},{"cell_type":"code","source":"# ALBERT 的参数共享结构：通过多个 AlbertLayerGroup 实现跨层共享。每组内部层数由 inner_group_num 控制。\n# 模块化设计：AlbertLayerGroup 封装了一个参数共享单元，为整体模型提供层级复用结构。\n# 可选调试功能：支持按需输出注意力权重和中间层隐藏状态，便于调试和分析模型行为。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(config.embedding_size, config.hidden_size,config.num_hidden_groups)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:13:42.138881Z","iopub.execute_input":"2025-05-23T09:13:42.139213Z","iopub.status.idle":"2025-05-23T09:13:42.144762Z","shell.execute_reply.started":"2025-05-23T09:13:42.139191Z","shell.execute_reply":"2025-05-23T09:13:42.143700Z"}},"outputs":[{"name":"stdout","text":"128 768 1\n","output_type":"stream"}],"execution_count":180},{"cell_type":"code","source":"[None] *config.num_hidden_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:15:41.508713Z","iopub.execute_input":"2025-05-23T09:15:41.509146Z","iopub.status.idle":"2025-05-23T09:15:41.515910Z","shell.execute_reply.started":"2025-05-23T09:15:41.509118Z","shell.execute_reply":"2025-05-23T09:15:41.514570Z"}},"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"[None, None, None, None, None, None, None, None, None, None, None, None]"},"metadata":{}}],"execution_count":182},{"cell_type":"code","source":"class AlbertTransformer(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n\n        self.config = config\n        # 将 embedding 层输出的低维表示（embedding_size）映射到 Transformer 所需的高维表示（hidden_size）\n        self.embedding_hidden_mapping_in = nn.Linear(config.embedding_size, config.hidden_size)\n        # 构建多个参数共享的 LayerGroup（组），每组内层结构共享\n        self.albert_layer_groups = nn.ModuleList(\n            [AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -> Union[BaseModelOutput, Tuple]:\n         # 将 embedding 结果映射到 transformer 的 hidden_size 维度\n        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n         # 如果需要记录中间层输出，初始化为 tuple，记录每层输出\n        all_hidden_states = (hidden_states,) if output_hidden_states else None\n        # 如果需要记录注意力权重，初始化为 tuple\n        all_attentions = () if output_attentions else None\n        # 如果没有提供 head_mask，默认设置为 None 列表\n        head_mask = [None] * self.config.num_hidden_layers if head_mask is None else head_mask\n         # 遍历每一层（实际层数由 config.num_hidden_layers 决定）\n        for i in range(self.config.num_hidden_layers):\n            # 每组包含的层数\n            layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n\n            # 当前层属于哪一组（组索引）\n            group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n             # 传入组索引获取当前的组,并前向传播\n            layer_group_output = self.albert_layer_groups[group_idx](\n                hidden_states,\n                attention_mask,\n                # 为该组选择对应的 head mask 子集\n                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n                output_attentions,\n                output_hidden_states,\n            )\n            # 更新 hidden_states 以供下一层使用\n            hidden_states = layer_group_output[0]\n             # 累积 attentions（如果需要）\n            if output_attentions:\n                all_attentions = all_attentions + layer_group_output[-1]\n             # 累积中间隐藏状态（如果需要）\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n        # 根据是否使用字典返回结构，组织输出\n        if not return_dict:\n            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states,   # 最后一层的输出\n            hidden_states=all_hidden_states,   # 所有层的隐藏状态（可选）\n            attentions=all_attentions   # 所有层的注意力权重（可选）\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:21:10.959015Z","iopub.execute_input":"2025-05-23T09:21:10.959373Z","iopub.status.idle":"2025-05-23T09:21:10.973186Z","shell.execute_reply.started":"2025-05-23T09:21:10.959348Z","shell.execute_reply":"2025-05-23T09:21:10.971455Z"}},"outputs":[],"execution_count":184},{"cell_type":"code","source":"#  设计意图说明：\n# 参数共享：ALBERT 的核心思想是通过分组 LayerGroup 来共享参数，减少参数规模，提高效率。\n# 层数控制：num_hidden_layers 决定总层数，num_hidden_groups 控制参数共享的粒度。\n# 调试支持：通过 output_attentions 与 output_hidden_states，可以在训练或分析过程中提取中间结果。\n# 兼容性输出：支持 return_dict=False 以返回 tuple，兼容老式调用方式。\n# 这是 ALBERT 模型结构中的 Transformer 主体模块，核心是对 embedding 后的表示进行多层共享 Transformer 编码。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型版本\tEmbedding Size (embedding_size)\tHidden Size (hidden_size)\tLayers (num_hidden_layers)\tParameters (M)\n# ALBERT-base\t128\t768\t12\t~12M\n# ALBERT-large\t128\t1024\t24\t~18M\n# ALBERT-xlarge\t128\t2048\t24\t~60M\n# ALBERT-xxlarge\t128\t4096\t12\t~235M","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config.initializer_range","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:34:42.440996Z","iopub.execute_input":"2025-05-23T09:34:42.441330Z","iopub.status.idle":"2025-05-23T09:34:42.449210Z","shell.execute_reply.started":"2025-05-23T09:34:42.441308Z","shell.execute_reply":"2025-05-23T09:34:42.447606Z"}},"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"0.02"},"metadata":{}}],"execution_count":185},{"cell_type":"code","source":"from ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ALBERT 的预训练模型基类，用于提供权重初始化逻辑，以及下载和加载预训练模型的接口封装。\n# 所有 ALBERT 模型应继承自该类，以保持统一的结构和权重处理方式。\nclass AlbertPreTrainedModel(PreTrainedModel):\n\n    config_class = AlbertConfig # 指定使用的配置类\n    load_tf_weights = load_tf_weights_in_albert  # 支持从 TensorFlow 权重加载\n    base_model_prefix = \"albert\"  # 模型名称前缀，用于加载权重时识别\n    _supports_sdpa = True  # 是否支持 SDPA（缩放点积注意力优化）\n    # 为模型各个子模块初始化权重，初始化策略如下\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # Linear 层权重采用正态分布初始化，均值 0，标准差来自 config，偏置初始化为 0\n            # 与 TensorFlow 的 truncated_normal 不同\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            # Embedding 层也使用正态分布初始化\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            # 对于 padding_idx，置为 0，避免梯度更新\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            # LayerNorm 初始化：偏置为 0，权重为 1\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        elif isinstance(module, AlbertMLMHead):\n            # ALBERT 的 masked language model 输出头的 bias 初始化为 0\n            module.bias.data.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:41:56.458980Z","iopub.execute_input":"2025-05-23T09:41:56.459442Z","iopub.status.idle":"2025-05-23T09:41:56.467929Z","shell.execute_reply.started":"2025-05-23T09:41:56.459416Z","shell.execute_reply":"2025-05-23T09:41:56.466740Z"}},"outputs":[],"execution_count":186},{"cell_type":"code","source":"# 用于表示 AlbertForPreTraining 模型的输出结构，继承自 HuggingFace 的 ModelOutput，\n# 支持通过属性名访问字段，同时保持向后兼容的字典形式。\n# 设计目的是将预训练模型所有可能的输出信息（如损失、预测 logits、SOP 分类结果、\n# 隐藏状态、注意力权重）整合成一个统一的数据结构，便于用户访问与调试。\n@dataclass\nclass AlbertForPreTrainingOutput(ModelOutput):\n    # 总损失：包含掩码语言模型损失（MLM loss）和下一句预测任务（SOP loss）\n    loss: Optional[torch.FloatTensor] = None\n    # MLM 任务的输出 logits，shape 为 (batch_size, sequence_length, vocab_size)\n    # 表示每个位置上对词表中每个词的预测分数（未 softmax）\n    prediction_logits: Optional[torch.FloatTensor] = None\n    # SOP（Sentence Order Prediction）任务的分类输出 logits，shape 为 (batch_size, 2)\n    # 表示两个标签（句子顺序正确 / 错误）的预测分数（未 softmax）\n    sop_logits: Optional[torch.FloatTensor] = None\n    # 模型在每层输出的隐藏状态序列（包括 embedding 层输出）\n    # 每个元素 shape 为 (batch_size, sequence_length, hidden_size)\n    # 当设置 output_hidden_states=True 时返回，用于调试或分析内部表示\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    # 每层的注意力权重，shape 为 (batch_size, num_heads, seq_len, seq_len)\n    # 当设置 output_attentions=True 时返回，用于可视化注意力分布\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n# 该结构为预训练任务（MLM + SOP）提供了完整的输出接口设计，便于灵活访问每部分结果，同时能与 \n# transformers 的其他模型输出格式保持一致。","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:48:02.714917Z","iopub.execute_input":"2025-05-23T09:48:02.715413Z","iopub.status.idle":"2025-05-23T09:48:02.725111Z","shell.execute_reply.started":"2025-05-23T09:48:02.715383Z","shell.execute_reply":"2025-05-23T09:48:02.723724Z"}},"outputs":[],"execution_count":188},{"cell_type":"code","source":"ALBERT_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Args:\n        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nALBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `({0})`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n            [`PreTrainedTokenizer.encode`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n\n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n\n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:49:36.038509Z","iopub.execute_input":"2025-05-23T09:49:36.038909Z","iopub.status.idle":"2025-05-23T09:49:36.047373Z","shell.execute_reply.started":"2025-05-23T09:49:36.038886Z","shell.execute_reply":"2025-05-23T09:49:36.046371Z"}},"outputs":[],"execution_count":189},{"cell_type":"code","source":"config.inner_group_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:05:50.138078Z","iopub.execute_input":"2025-05-23T10:05:50.138549Z","iopub.status.idle":"2025-05-23T10:05:50.148606Z","shell.execute_reply.started":"2025-05-23T10:05:50.138521Z","shell.execute_reply":"2025-05-23T10:05:50.145791Z"}},"outputs":[{"execution_count":191,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":191},{"cell_type":"code","source":"@add_start_docstrings(\n    \"The bare ALBERT Model transformer outputting raw hidden-states without any specific head on top.\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertModel(AlbertPreTrainedModel):\n    # ALBERT 模型的主干类，继承自预训练模型基类，支持权重加载与初始化等通用功能。\n    config_class = AlbertConfig\n    base_model_prefix = \"albert\"\n\n    def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.config = config\n        # 嵌入层，包括词嵌入、位置嵌入、类型嵌入（即 segment embedding）\n        self.embeddings = AlbertEmbeddings(config)\n        # Transformer 编码器部分，负责处理嵌入后的输入序列\n        self.encoder = AlbertTransformer(config)\n        # 池化层（可选）：用于提取句子级别的表示（如 [CLS] token）\n        if add_pooling_layer:\n            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n            self.pooler_activation = nn.Tanh()  # 池化后通常接 Tanh 激活作为句子向量\n        else:\n            self.pooler = None\n            self.pooler_activation = None\n         # 注意力机制实现方式（用于兼容 SDPA 等）\n        self.attn_implementation = config._attn_implementation\n        # 位置嵌入类型（例如绝对或相对位置编码）\n        self.position_embedding_type = config.position_embedding_type\n        # 执行权重初始化和其他后处理操作\n        self.post_init()\n\n    def get_input_embeddings(self) -> nn.Embedding:\n         # 获取当前模型使用的词嵌入层，常用于模型嵌入替换\n        return self.embeddings.word_embeddings\n    # 设置词嵌入层，使用户可以自定义嵌入矩阵（如共享或重新训练）\n    def set_input_embeddings(self, value: nn.Embedding) -> None:\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n        \"\"\"\n        裁剪注意力头（prune attention heads），用于模型压缩和加速推理。    \n        参数 heads_to_prune: dict，格式为 {层索引: 要裁剪的头部索引列表}\n        ALBERT 的结构与传统 Transformer 不同，它采用了参数共享的机制：\n        - 例如 12 层 Transformer 可以被划分为若干“组”（hidden groups）；\n        - 每组内部又有多个“子层”（inner groups）；\n        - 实际层数 = num_hidden_groups * inner_group_num。\n        为了方便裁剪，所有子层被“平铺”成一个线性索引：\n        - 假设 inner_group_num=2，那么索引 0,1 对应 group 0 的两个子层，\n          索引 2,3 对应 group 1 的两个子层。    \n        任何不在有效范围的索引（如非 0,1,2,3）都会报错。\n        实际裁剪调用 attention 层的 prune_heads 方法，具体实现在 attention 层中。\n        \"\"\"\n        #  假设layer=2,inner_group_num=3 这个是每组的层数,group_idx=0这时是当前层对应的组索引\n        for layer, heads in heads_to_prune.items(): # 对应层索引-->每层要微调的头的列表\n            # 通过线性索引反推：属于第几个组（group_idx）每层对应的组\n            group_idx = int(layer / self.config.inner_group_num) \n            # 当前层在当前所在组的层列表中的索引\n            inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n            # self.encoder.albert_layer_groups[group_idx] 获取指定的组\n            # .albert_layers[inner_group_idx] 获取指定层\n            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPooling,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[BaseModelOutputWithPooling, Tuple]:\n        # 如果未显式传入，则使用 config 中的默认参数\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # 不允许同时传入 input_ids 和 inputs_embeds，两者只能选其一\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n             # 提示用户未传 attention_mask 时可能会导致 pad 被误处理为有效 token\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size() # (b,s)\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n         # 若未显式传入 attention_mask，则默认全为 1（表示所有位置都参与 attention\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        # 若未显式传入 token_type_ids（即 segment embedding），则自动构造\n        if token_type_ids is None:\n            if hasattr(self.embeddings, \"token_type_ids\"):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n        # 计算嵌入表示，支持 input_ids 或 inputs_embeds 两种输入方式\n        embedding_output = self.embeddings(\n            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n        )\n        # 判断是否使用 SDPA（高效 attention 实现），需满足多个条件\n        use_sdpa_attention_mask = (\n            self.attn_implementation == \"sdpa\"\n            and self.position_embedding_type == \"absolute\"\n            and head_mask is None\n            and not output_attentions\n        )\n        # 构造 SDPA 所需的 4D attention mask\n        if use_sdpa_attention_mask:\n            extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                attention_mask, embedding_output.dtype, tgt_len=seq_length\n            )\n        else:\n            # 普通 attention mask 的构造逻辑\n            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n            extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)   # 为了 fp16 兼容\n            extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min  # 将 pad 位置设为 -inf\n        # 获取注意力头 mask，支持按需裁剪注意力头\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n        # 调用 Transformer 编码器进行上下文建模\n        encoder_outputs = self.encoder(\n            embedding_output,\n            extended_attention_mask,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n         # 经过tranformer编码后的输出 (b,s,d)\n        sequence_output = encoder_outputs[0]\n        # 若设置了 pooler，则对 [CLS] 位置做一个全连接 + tanh，作为 pooled_output（句向量）\n        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, 0])) if self.pooler is not None else None\n        # 返回结构化输出或元组，依据 return_dict 参数决定\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=sequence_output,  # 所有 token 的最后一层输出\n            pooler_output=pooled_output,    # 句子级向量（来自 [CLS]）\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:22:19.411931Z","iopub.execute_input":"2025-05-23T10:22:19.413160Z","iopub.status.idle":"2025-05-23T10:22:19.442587Z","shell.execute_reply.started":"2025-05-23T10:22:19.413117Z","shell.execute_reply":"2025-05-23T10:22:19.441329Z"}},"outputs":[],"execution_count":192},{"cell_type":"code","source":"from transformers.models.albert.modeling_albert import AlbertMLMHead","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:26:00.396744Z","iopub.execute_input":"2025-05-23T10:26:00.397141Z","iopub.status.idle":"2025-05-23T10:26:00.448882Z","shell.execute_reply.started":"2025-05-23T10:26:00.397117Z","shell.execute_reply":"2025-05-23T10:26:00.446700Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"albertModel=AlbertModel(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:26:02.431745Z","iopub.execute_input":"2025-05-23T10:26:02.432550Z","iopub.status.idle":"2025-05-23T10:26:02.707730Z","shell.execute_reply.started":"2025-05-23T10:26:02.432496Z","shell.execute_reply":"2025-05-23T10:26:02.706125Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"# 设计意图总结：\n# 兼容多种输入形式（id 或嵌入）；\n# 自动补齐缺省字段（如 attention_mask）；\n# 支持高效 attention 实现（SDPA）；\n# 统一返回结构支持后续任务（如分类、QA）；\n# 灵活开启中间层输出与注意力权重输出。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AlbertMLMHead(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n        # LayerNorm 作用于 embedding_size，统一输出分布，增强训练稳定性\n        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        # decoder 的 bias 参数，独立管理以便于 weight tying（共享 embedding 和 decoder 权重）\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        # 将 transformer 输出从 hidden_size 映射到 embedding_size，便于对 decoder 权重共享\n        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n        # 最终用于生成词表维度输出的线性层（可与 embedding 权重共享）\n        self.decoder = nn.Linear(config.embedding_size, config.vocab_size)\n        self.activation = ACT2FN[config.hidden_act]  # 激活函数，一般为 GELU 或其他配置指定的非线性函数\n        # 显式绑定 decoder.bias 到独立 bias 参数，实现 bias 权重共享\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # 将隐藏状态映射回 embedding 空间\n        hidden_states = self.dense(hidden_states) \n        hidden_states = self.activation(hidden_states) # 应用非线性变换\n        hidden_states = self.LayerNorm(hidden_states)  # 正则化处理\n        hidden_states = self.decoder(hidden_states)  # 投影回词表空间，得到每个 token 的 logits\n\n        prediction_scores = hidden_states\n         # 返回预测每个 token 的 logits\n        return prediction_scores\n\n    def _tie_weights(self) -> None:\n        # 用于确保 decoder.bias 与 self.bias 始终共享（在部分设备或加载过程中可能失效）\n        if self.decoder.bias.device.type == \"meta\":\n            # \"meta\" 类型仅用于初始化，不应实际训练，用于兼容加速框架\n            self.decoder.bias = self.bias\n        else:\n            # 如果这两个权重断开连接（在 TPU 上或调整偏距大小时），则将它们绑定在一起\n            self.bias = self.decoder.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:50:33.220669Z","iopub.execute_input":"2025-05-23T10:50:33.221159Z","iopub.status.idle":"2025-05-23T10:50:33.232045Z","shell.execute_reply.started":"2025-05-23T10:50:33.221133Z","shell.execute_reply":"2025-05-23T10:50:33.230390Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"# SOP（Sentence Order Prediction）是 ALBERT 特有的句子顺序预测任务\nclass AlbertSOPHead(nn.Module): \n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n         # dropout 用于减少过拟合，提高泛化能力\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n         # 分类器，输入为 pooled_output（句子级表示），输出为两个类别的 logits（顺序正确 or 错误）\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n        # 对 pooled_output 应用 dropout，防止过拟合\n        dropout_pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(dropout_pooled_output) # 使用线性分类器得到 SOP 任务的 logits\n        return logits # 返回表示两个类别的 logits（用于顺序预测）","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:53:37.296924Z","iopub.execute_input":"2025-05-23T10:53:37.297299Z","iopub.status.idle":"2025-05-23T10:53:37.304849Z","shell.execute_reply.started":"2025-05-23T10:53:37.297276Z","shell.execute_reply":"2025-05-23T10:53:37.303580Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"# @add_start_docstrings(\n#     \"\"\"\n#     Albert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a\n#     `sentence order prediction (classification)` head.\n#     \"\"\",\n#     ALBERT_START_DOCSTRING,\n# )\nclass AlbertForPreTraining(AlbertPreTrainedModel):\n    # 指定权重共享的关键路径，用于参数绑定（tie weights），确保 decoder 权重和 embedding 权重一致\n    _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        # 主体部分为基础的 AlbertModel，用于提取通用表示\n        self.albert = AlbertModel(config)\n        # MLM 头，用于预测被 mask 掉的 token\n        self.predictions = AlbertMLMHead(config)\n         # Sentence Order Prediction 头，用于判断两句是否是上下文连接（是否交换）\n        self.sop_classifier = AlbertSOPHead(config) \n\n        # 权重初始化及最终处理（继承自 PreTrainedModel）\n        self.post_init()\n\n    def get_output_embeddings(self) -> nn.Linear:\n        # 返回 decoder 权重，用于 tie weights\n        return self.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n        # 设置新的 decoder 权重，用于 tie weights 或模型迁移\n        self.predictions.decoder = new_embeddings\n     # 返回嵌入层权重，用于 tie weights 或自定义初始化\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.albert.embeddings.word_embeddings\n\n    # @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    # @replace_return_docstrings(output_type=AlbertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        sentence_order_label: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        sentence_order_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see `input_ids` docstring) Indices should be in `[0, 1]`. `0` indicates original order (sequence A, then\n            sequence B), `1` indicates switched order (sequence B, then sequence A).\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, AlbertForPreTraining\n        >>> import torch\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n        >>> model = AlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)\n        >>> # Batch size 1\n        >>> outputs = model(input_ids)\n\n        >>> prediction_logits = outputs.prediction_logits\n        >>> sop_logits = outputs.sop_logits\n        ```\"\"\"\n        # 如果未显式设置 return_dict，则使用 config 中的默认设置\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # 调用主干 AlbertModel，获得 sequence_output 和 pooled_output\n       # sequence_output: 所有 token 的表示，用于 MLM\n       # pooled_output: 句子级表示（通常为第一个 token 的表示），用于 SOP\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n         # MLM 预测：对每个 token 进行词预测\n        prediction_scores = self.predictions(sequence_output)\n       # SOP 分类：判断句子顺序是否被打乱\n        sop_scores = self.sop_classifier(pooled_output)\n        total_loss = None\n        # 如果提供了标签，则计算 MLM loss 和 SOP loss\n        if labels is not None and sentence_order_label is not None:\n            loss_fct = CrossEntropyLoss()\n            # 计算 masked language model 的交叉熵损失\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            # 计算 sentence order prediction 的交叉熵损失\n            sentence_order_loss = loss_fct(sop_scores.view(-1, 2), sentence_order_label.view(-1))\n            total_loss = masked_lm_loss + sentence_order_loss  # 总损失为两者之和\n            \n        # 若不使用字典形式返回结果，则使用元组方式返回\n        if not return_dict:\n            output = (prediction_scores, sop_scores) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n        # 使用结构化字典返回结果，方便下游使用\n        return AlbertForPreTrainingOutput(\n            loss=total_loss,  # 总损失（可为 None）\n            prediction_logits=prediction_scores,  # MLM 输出 logits\n            sop_logits=sop_scores,   # SOP 输出 logits\n            hidden_states=outputs.hidden_states,  # 隐藏层状态（可选）\n            attentions=outputs.attentions,    # 注意力权重（可选）\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:01:19.450749Z","iopub.execute_input":"2025-05-23T11:01:19.451181Z","iopub.status.idle":"2025-05-23T11:01:19.470312Z","shell.execute_reply.started":"2025-05-23T11:01:19.451152Z","shell.execute_reply":"2025-05-23T11:01:19.468720Z"}},"outputs":[],"execution_count":200},{"cell_type":"code","source":"from transformers import AutoTokenizer, AlbertForPreTraining","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:02:08.231396Z","iopub.execute_input":"2025-05-23T11:02:08.231831Z","iopub.status.idle":"2025-05-23T11:02:08.287349Z","shell.execute_reply.started":"2025-05-23T11:02:08.231805Z","shell.execute_reply":"2025-05-23T11:02:08.285607Z"}},"outputs":[],"execution_count":202},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:02:10.516119Z","iopub.execute_input":"2025-05-23T11:02:10.517360Z","iopub.status.idle":"2025-05-23T11:02:14.369868Z","shell.execute_reply.started":"2025-05-23T11:02:10.517318Z","shell.execute_reply":"2025-05-23T11:02:14.368555Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e60ff63abe846d3a09e0abbe9e75c18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53386b758ccd425aa05e8ee1bd91fafc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ad4ae342f7497c86d2eb4c3792d83d"}},"metadata":{}}],"execution_count":203},{"cell_type":"code","source":"model = AlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:02:28.287947Z","iopub.execute_input":"2025-05-23T11:02:28.288340Z","iopub.status.idle":"2025-05-23T11:02:31.661697Z","shell.execute_reply.started":"2025-05-23T11:02:28.288314Z","shell.execute_reply":"2025-05-23T11:02:31.660592Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17416ce15d6947a49718d2c5d935c196"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForPreTraining were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['sop_classifier.classifier.bias', 'sop_classifier.classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":204},{"cell_type":"code","source":"tokenizer.encode(\"Hello, my dog is cute\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:03:19.835353Z","iopub.execute_input":"2025-05-23T11:03:19.835792Z","iopub.status.idle":"2025-05-23T11:03:19.843521Z","shell.execute_reply.started":"2025-05-23T11:03:19.835763Z","shell.execute_reply":"2025-05-23T11:03:19.842373Z"}},"outputs":[{"execution_count":206,"output_type":"execute_result","data":{"text/plain":"[2, 10975, 15, 51, 1952, 25, 10901, 3]"},"metadata":{}}],"execution_count":206},{"cell_type":"code","source":"input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:03:46.816947Z","iopub.execute_input":"2025-05-23T11:03:46.817375Z","iopub.status.idle":"2025-05-23T11:03:46.825472Z","shell.execute_reply.started":"2025-05-23T11:03:46.817347Z","shell.execute_reply":"2025-05-23T11:03:46.824039Z"}},"outputs":[],"execution_count":207},{"cell_type":"code","source":"input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:03:54.423433Z","iopub.execute_input":"2025-05-23T11:03:54.423872Z","iopub.status.idle":"2025-05-23T11:03:54.434214Z","shell.execute_reply.started":"2025-05-23T11:03:54.423844Z","shell.execute_reply":"2025-05-23T11:03:54.432564Z"}},"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"tensor([[    2, 10975,    15,    51,  1952,    25, 10901,     3]])"},"metadata":{}}],"execution_count":208},{"cell_type":"code","source":"outputs = model(input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:04:05.567585Z","iopub.execute_input":"2025-05-23T11:04:05.568657Z","iopub.status.idle":"2025-05-23T11:04:05.697780Z","shell.execute_reply.started":"2025-05-23T11:04:05.568591Z","shell.execute_reply":"2025-05-23T11:04:05.696595Z"}},"outputs":[],"execution_count":209},{"cell_type":"code","source":"prediction_logits = outputs.prediction_logits\nprediction_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:05:23.299289Z","iopub.execute_input":"2025-05-23T11:05:23.299719Z","iopub.status.idle":"2025-05-23T11:05:23.308575Z","shell.execute_reply.started":"2025-05-23T11:05:23.299693Z","shell.execute_reply":"2025-05-23T11:05:23.306470Z"}},"outputs":[{"execution_count":212,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 30000])"},"metadata":{}}],"execution_count":212},{"cell_type":"code","source":"sop_logits = outputs.sop_logits\nsop_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:05:40.323150Z","iopub.execute_input":"2025-05-23T11:05:40.323540Z","iopub.status.idle":"2025-05-23T11:05:40.334399Z","shell.execute_reply.started":"2025-05-23T11:05:40.323515Z","shell.execute_reply":"2025-05-23T11:05:40.332439Z"}},"outputs":[{"execution_count":213,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2])"},"metadata":{}}],"execution_count":213},{"cell_type":"code","source":"# @add_start_docstrings(\n#     \"Albert Model with a `language modeling` head on top.\",\n#     ALBERT_START_DOCSTRING,\n# )\nclass AlbertForMaskedLM(AlbertPreTrainedModel):\n    # 指定需要共享权重的参数键（通常用于权重绑定 tied weights）\n    _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n         # 主体 encoder，不包含池化层，因为 masked LM 不需要句子级表示\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.predictions = AlbertMLMHead(config) # MLM 头部，用于预测 masked 的词\n\n         # 初始化权重等操作\n        self.post_init()\n     # 获取输出层权重（用于 tied weights 或导出）\n    def get_output_embeddings(self) -> nn.Linear:\n        return self.predictions.decoder\n     # 设置输出层权重，同时绑定 bias\n    def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n        self.predictions.decoder = new_embeddings\n        self.predictions.bias = new_embeddings.bias\n     # 获取输入词嵌入层（用于 tied weights）\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.albert.embeddings.word_embeddings\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[MaskedLMOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoTokenizer, AlbertForMaskedLM\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n        >>> model = AlbertForMaskedLM.from_pretrained(\"albert/albert-base-v2\")\n\n        >>> # add mask_token\n        >>> inputs = tokenizer(\"The capital of [MASK] is Paris.\", return_tensors=\"pt\")\n        >>> with torch.no_grad():\n        ...     logits = model(**inputs).logits\n\n        >>> # retrieve index of [MASK]\n        >>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n        >>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n        >>> tokenizer.decode(predicted_token_id)\n        'france'\n        ```\n\n        ```python\n        >>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n        >>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n        >>> outputs = model(**inputs, labels=labels)\n        >>> round(outputs.loss.item(), 2)\n        0.81\n        ```\n        \"\"\"\n        # 设置是否使用字典方式返回结果（兼容旧接口）\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         # 编码器前向传播，返回 sequence_output 和其它信息\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_outputs = outputs[0] # 获取 token 级别表示（B, L, H）\n        # MLM 预测 logits (B, L, Vocab)\n        prediction_scores = self.predictions(sequence_outputs)\n\n        masked_lm_loss = None\n        if labels is not None:\n             # 有监督训练时计算交叉熵损失（忽略 label=-100 的位置）\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n             # 返回 tuple 格式结果\n            output = (prediction_scores,) + outputs[2:] # outputs[2:] 包含 hidden_states 和 attentions（如果开启）\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n        # 返回字典格式结果\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:20:31.588009Z","iopub.execute_input":"2025-05-23T11:20:31.588420Z","iopub.status.idle":"2025-05-23T11:20:31.606791Z","shell.execute_reply.started":"2025-05-23T11:20:31.588394Z","shell.execute_reply":"2025-05-23T11:20:31.605487Z"}},"outputs":[],"execution_count":230},{"cell_type":"code","source":"model = AlbertForMaskedLM.from_pretrained(\"albert/albert-base-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:20:38.616318Z","iopub.execute_input":"2025-05-23T11:20:38.616744Z","iopub.status.idle":"2025-05-23T11:20:39.138935Z","shell.execute_reply.started":"2025-05-23T11:20:38.616716Z","shell.execute_reply":"2025-05-23T11:20:39.137252Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":231},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:17:06.555896Z","iopub.execute_input":"2025-05-23T11:17:06.557098Z","iopub.status.idle":"2025-05-23T11:17:06.565561Z","shell.execute_reply.started":"2025-05-23T11:17:06.557051Z","shell.execute_reply":"2025-05-23T11:17:06.564155Z"}},"outputs":[{"execution_count":222,"output_type":"execute_result","data":{"text/plain":"AlbertForMaskedLM(\n  (albert): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertSdpaAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): NewGELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n  )\n  (predictions): AlbertMLMHead(\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dense): Linear(in_features=768, out_features=128, bias=True)\n    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n    (activation): NewGELUActivation()\n  )\n)"},"metadata":{}}],"execution_count":222},{"cell_type":"code","source":"inputs = tokenizer(\"The capital of [MASK] is Paris.\", return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:20:43.407969Z","iopub.execute_input":"2025-05-23T11:20:43.408456Z","iopub.status.idle":"2025-05-23T11:20:43.414924Z","shell.execute_reply.started":"2025-05-23T11:20:43.408424Z","shell.execute_reply":"2025-05-23T11:20:43.413620Z"}},"outputs":[],"execution_count":232},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:20:45.938617Z","iopub.execute_input":"2025-05-23T11:20:45.939039Z","iopub.status.idle":"2025-05-23T11:20:46.024974Z","shell.execute_reply.started":"2025-05-23T11:20:45.939012Z","shell.execute_reply":"2025-05-23T11:20:46.023928Z"}},"outputs":[],"execution_count":233},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:21:01.728055Z","iopub.execute_input":"2025-05-23T11:21:01.728470Z","iopub.status.idle":"2025-05-23T11:21:01.736480Z","shell.execute_reply.started":"2025-05-23T11:21:01.728441Z","shell.execute_reply":"2025-05-23T11:21:01.734971Z"}},"outputs":[{"execution_count":235,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 9, 30000])"},"metadata":{}}],"execution_count":235},{"cell_type":"code","source":"mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:21:49.286672Z","iopub.execute_input":"2025-05-23T11:21:49.287054Z","iopub.status.idle":"2025-05-23T11:21:49.294974Z","shell.execute_reply.started":"2025-05-23T11:21:49.287025Z","shell.execute_reply":"2025-05-23T11:21:49.293514Z"}},"outputs":[],"execution_count":236},{"cell_type":"code","source":"mask_token_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:21:59.545976Z","iopub.execute_input":"2025-05-23T11:21:59.546368Z","iopub.status.idle":"2025-05-23T11:21:59.556115Z","shell.execute_reply.started":"2025-05-23T11:21:59.546343Z","shell.execute_reply":"2025-05-23T11:21:59.554886Z"}},"outputs":[{"execution_count":237,"output_type":"execute_result","data":{"text/plain":"tensor([4])"},"metadata":{}}],"execution_count":237},{"cell_type":"code","source":"predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:22:19.126729Z","iopub.execute_input":"2025-05-23T11:22:19.128285Z","iopub.status.idle":"2025-05-23T11:22:19.136174Z","shell.execute_reply.started":"2025-05-23T11:22:19.128226Z","shell.execute_reply":"2025-05-23T11:22:19.134593Z"}},"outputs":[],"execution_count":238},{"cell_type":"code","source":"tokenizer.decode(predicted_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:22:29.471576Z","iopub.execute_input":"2025-05-23T11:22:29.471994Z","iopub.status.idle":"2025-05-23T11:22:29.481143Z","shell.execute_reply.started":"2025-05-23T11:22:29.471966Z","shell.execute_reply":"2025-05-23T11:22:29.479517Z"}},"outputs":[{"execution_count":239,"output_type":"execute_result","data":{"text/plain":"'france'"},"metadata":{}}],"execution_count":239},{"cell_type":"code","source":"labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:22:41.434130Z","iopub.execute_input":"2025-05-23T11:22:41.434584Z","iopub.status.idle":"2025-05-23T11:22:41.442012Z","shell.execute_reply.started":"2025-05-23T11:22:41.434555Z","shell.execute_reply":"2025-05-23T11:22:41.440865Z"}},"outputs":[],"execution_count":240},{"cell_type":"code","source":"labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:22:58.915694Z","iopub.execute_input":"2025-05-23T11:22:58.916708Z","iopub.status.idle":"2025-05-23T11:22:58.925997Z","shell.execute_reply.started":"2025-05-23T11:22:58.916611Z","shell.execute_reply":"2025-05-23T11:22:58.924299Z"}},"outputs":[{"execution_count":241,"output_type":"execute_result","data":{"text/plain":"tensor([[   2,   14, 1057,   16,  714,   25, 1162,    9,    3]])"},"metadata":{}}],"execution_count":241},{"cell_type":"code","source":"labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:23:06.719281Z","iopub.execute_input":"2025-05-23T11:23:06.719832Z","iopub.status.idle":"2025-05-23T11:23:06.729359Z","shell.execute_reply.started":"2025-05-23T11:23:06.719761Z","shell.execute_reply":"2025-05-23T11:23:06.727654Z"}},"outputs":[],"execution_count":242},{"cell_type":"code","source":"labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:23:16.891146Z","iopub.execute_input":"2025-05-23T11:23:16.891588Z","iopub.status.idle":"2025-05-23T11:23:16.902269Z","shell.execute_reply.started":"2025-05-23T11:23:16.891559Z","shell.execute_reply":"2025-05-23T11:23:16.900391Z"}},"outputs":[{"execution_count":243,"output_type":"execute_result","data":{"text/plain":"tensor([[-100, -100, -100, -100,  714, -100, -100, -100, -100]])"},"metadata":{}}],"execution_count":243},{"cell_type":"code","source":"outputs = model(**inputs, labels=labels)\nprint(outputs.logits.shape,outputs.loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:27:01.205330Z","iopub.execute_input":"2025-05-23T11:27:01.205738Z","iopub.status.idle":"2025-05-23T11:27:01.299598Z","shell.execute_reply.started":"2025-05-23T11:27:01.205714Z","shell.execute_reply":"2025-05-23T11:27:01.298563Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 9, 30000]) tensor(0.8129, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":248},{"cell_type":"code","source":"round(outputs.loss.item(), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:27:07.251527Z","iopub.execute_input":"2025-05-23T11:27:07.251930Z","iopub.status.idle":"2025-05-23T11:27:07.260147Z","shell.execute_reply.started":"2025-05-23T11:27:07.251906Z","shell.execute_reply":"2025-05-23T11:27:07.258730Z"}},"outputs":[{"execution_count":249,"output_type":"execute_result","data":{"text/plain":"0.81"},"metadata":{}}],"execution_count":249},{"cell_type":"code","source":"config.num_labels ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:28:53.450868Z","iopub.execute_input":"2025-05-23T11:28:53.451401Z","iopub.status.idle":"2025-05-23T11:28:53.460830Z","shell.execute_reply.started":"2025-05-23T11:28:53.451372Z","shell.execute_reply":"2025-05-23T11:28:53.458531Z"}},"outputs":[{"execution_count":250,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":250},{"cell_type":"code","source":"print(config.problem_type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:32:48.387053Z","iopub.execute_input":"2025-05-23T11:32:48.387493Z","iopub.status.idle":"2025-05-23T11:32:48.393826Z","shell.execute_reply.started":"2025-05-23T11:32:48.387465Z","shell.execute_reply":"2025-05-23T11:32:48.392581Z"}},"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":252},{"cell_type":"code","source":"@add_start_docstrings(\n    \"\"\"\n    Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels  # 类别数量（用于分类或回归任务）\n        self.config = config  # 保存配置以供后续使用\n\n        self.albert = AlbertModel(config)   # 主体模型，输出hidden_states和pooled_output\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)  # 防止过拟合的Dropout层\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)  # 分类器，根据pooled_output输出最终logits\n\n        # Initialize weights and apply final processing\n        self.post_init()  # 初始化权重和其它后处理\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=\"textattack/albert-base-v2-imdb\",\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"'LABEL_1'\",\n        expected_loss=0.12,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[SequenceClassifierOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            标签，用于计算损失函数。如果为1类问题，使用回归损失；否则使用交叉熵或BCE。\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # 调用Albert模型获得encoder的输出\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]  # 获取句子的整体表示（通常是[CLS]向量）\n\n        pooled_output = self.dropout(pooled_output)  # 加Dropout增强泛化能力\n        logits = self.classifier(pooled_output) # 分类器输出预测logits\n\n        loss = None\n        if labels is not None:\n              # 动态确定任务类型（支持回归、单标签、多标签分类）\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n             # 根据任务类型选择合适的损失函数\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n         # 返回格式支持字典或元组两种风格\n        if not return_dict:\n            output = (logits,) + outputs[2:] # outputs[2:] = (hidden_states, attentions)\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 简要设计意图总结：\n# 本模型基于 AlbertModel，并在 pooled_output 后加一层线性变换，实现分类或回归。\n# 通过 problem_type 自动适应不同类型的下游任务。\n# 支持多种输入格式和输出格式，兼容 Huggingface 的训练与推理流程。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        classifier_dropout_prob = (\n            config.classifier_dropout_prob\n            if config.classifier_dropout_prob is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[TokenClassifierOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=\"twmkn9/albert-base-v2-squad2\",\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        qa_target_start_index=12,\n        qa_target_end_index=13,\n        expected_output=\"'a nice puppet'\",\n        expected_loss=7.36,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits: torch.Tensor = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n            num_choices-1]` where *num_choices* is the size of the second dimension of the input tensors. (see\n            *input_ids* above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits: torch.Tensor = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"__all__ = [\n    \"load_tf_weights_in_albert\",\n    \"AlbertPreTrainedModel\",\n    \"AlbertModel\",\n    \"AlbertForPreTraining\",\n    \"AlbertForMaskedLM\",\n    \"AlbertForSequenceClassification\",\n    \"AlbertForTokenClassification\",\n    \"AlbertForQuestionAnswering\",\n    \"AlbertForMultipleChoice\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:43:24.244432Z","iopub.execute_input":"2025-05-23T09:43:24.245788Z","iopub.status.idle":"2025-05-23T09:43:24.251950Z","shell.execute_reply.started":"2025-05-23T09:43:24.245738Z","shell.execute_reply":"2025-05-23T09:43:24.250330Z"}},"outputs":[],"execution_count":187},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}